<?xml version="1.0" encoding="utf-8"?>
<xliff version="1.2" xmlns="urn:oasis:names:tc:xliff:document:1.2" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="urn:oasis:names:tc:xliff:document:1.2 xliff-core-1.2-transitional.xsd">
  <file datatype="xml" source-language="en-us" target-language="zh-cn" original="2/18/2016 5:38:21 PM" tool-id="MarkdownTransformer" product-name="N/A" product-version="N/A" build-num="1">
    <header>
      <xliffext:olfilehash xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">029297ec15cfd3e3d4d0e2f165bd7e035e1fa4b5</xliffext:olfilehash>
      <tool tool-id="MarkdownTransformer" tool-name="MarkdownToXliff" tool-version="1.0" tool-company="Microsoft" />
    </header>
    <body>
      <group extype="content">
        <group id="101">
          <trans-unit id="101" xml:space="preserve">
            <source>Compression support</source>
            <target state="new">Compression support</target>
          </trans-unit>
          <trans-unit id="102" xml:space="preserve">
            <source>Processing large data sets can cause I/O and network bottlenecks.</source>
            <target state="new">Processing large data sets can cause I/O and network bottlenecks.</target>
          </trans-unit>
          <trans-unit id="103" xml:space="preserve">
            <source>Therefore, compressed data in stores can not only speed up data transfer across the network and save disk space, but also bring significant performance improvements in processing big data.</source>
            <target state="new">Therefore, compressed data in stores can not only speed up data transfer across the network and save disk space, but also bring significant performance improvements in processing big data.</target>
          </trans-unit>
          <trans-unit id="104" xml:space="preserve">
            <source>At this time, compression is supported for file-based data stores such as Azure Blob or On-premises File System.</source>
            <target state="new">At this time, compression is supported for file-based data stores such as Azure Blob or On-premises File System.</target>
          </trans-unit>
          <trans-unit id="105" xml:space="preserve">
            <source>To specify compression for a dataset, use the <bpt id="2">&lt;strong&gt;</bpt>compression<ept id="2">&lt;/strong&gt;</ept> property in the dataset JSON as in the following example:</source>
            <target state="new">To specify compression for a dataset, use the <bpt id="2">&lt;strong&gt;</bpt>compression<ept id="2">&lt;/strong&gt;</ept> property in the dataset JSON as in the following example:</target>
          </trans-unit>
          <trans-unit id="106" xml:space="preserve">
            <source>Note that the <bpt id="2">&lt;strong&gt;</bpt>compression<ept id="2">&lt;/strong&gt;</ept> section has two properties:</source>
            <target state="new">Note that the <bpt id="2">&lt;strong&gt;</bpt>compression<ept id="2">&lt;/strong&gt;</ept> section has two properties:</target>
          </trans-unit>
          <trans-unit id="107" xml:space="preserve">
            <source><bpt id="1">&lt;strong&gt;</bpt>Type:<ept id="1">&lt;/strong&gt;</ept> the compression codec, which can be <bpt id="3">&lt;strong&gt;</bpt>GZIP<ept id="3">&lt;/strong&gt;</ept>, <bpt id="5">&lt;strong&gt;</bpt>Deflate<ept id="5">&lt;/strong&gt;</ept> or <bpt id="7">&lt;strong&gt;</bpt>BZIP2<ept id="7">&lt;/strong&gt;</ept>.</source>
            <target state="new"><bpt id="1">&lt;strong&gt;</bpt>Type:<ept id="1">&lt;/strong&gt;</ept> the compression codec, which can be <bpt id="3">&lt;strong&gt;</bpt>GZIP<ept id="3">&lt;/strong&gt;</ept>, <bpt id="5">&lt;strong&gt;</bpt>Deflate<ept id="5">&lt;/strong&gt;</ept> or <bpt id="7">&lt;strong&gt;</bpt>BZIP2<ept id="7">&lt;/strong&gt;</ept>.</target>
          </trans-unit>
          <trans-unit id="108" xml:space="preserve">
            <source><bpt id="1">&lt;strong&gt;</bpt>Level:<ept id="1">&lt;/strong&gt;</ept> the compression ratio, which can be <bpt id="3">&lt;strong&gt;</bpt>Optimal<ept id="3">&lt;/strong&gt;</ept> or <bpt id="5">&lt;strong&gt;</bpt>Fastest<ept id="5">&lt;/strong&gt;</ept>.</source>
            <target state="new"><bpt id="1">&lt;strong&gt;</bpt>Level:<ept id="1">&lt;/strong&gt;</ept> the compression ratio, which can be <bpt id="3">&lt;strong&gt;</bpt>Optimal<ept id="3">&lt;/strong&gt;</ept> or <bpt id="5">&lt;strong&gt;</bpt>Fastest<ept id="5">&lt;/strong&gt;</ept>.</target>
          </trans-unit>
          <trans-unit id="109" xml:space="preserve">
            <source><bpt id="1">&lt;strong&gt;</bpt>Fastest:<ept id="1">&lt;/strong&gt;</ept> The compression operation should complete as quickly as possible, even if the resulting file is not optimally compressed.</source>
            <target state="new"><bpt id="1">&lt;strong&gt;</bpt>Fastest:<ept id="1">&lt;/strong&gt;</ept> The compression operation should complete as quickly as possible, even if the resulting file is not optimally compressed.</target>
          </trans-unit>
          <trans-unit id="110" xml:space="preserve">
            <source><bpt id="1">&lt;strong&gt;</bpt>Optimal<ept id="1">&lt;/strong&gt;</ept>: The compression operation should be optimally compressed, even if the operation takes a longer time to complete.</source>
            <target state="new"><bpt id="1">&lt;strong&gt;</bpt>Optimal<ept id="1">&lt;/strong&gt;</ept>: The compression operation should be optimally compressed, even if the operation takes a longer time to complete.</target>
          </trans-unit>
          <trans-unit id="111" xml:space="preserve">
            <source>See <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>Compression Level<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept> topic for more information.</source>
            <target state="new">See <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>Compression Level<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept> topic for more information.</target>
          </trans-unit>
          <trans-unit id="112" xml:space="preserve">
            <source>Suppose the above sample dataset is used as the output of a copy activity, the copy activity will compresses the output data with GZIP codec using optimal ratio and then write the compressed data into a file named pagecounts.csv.gz in the Azure Blob Storage.</source>
            <target state="new">Suppose the above sample dataset is used as the output of a copy activity, the copy activity will compresses the output data with GZIP codec using optimal ratio and then write the compressed data into a file named pagecounts.csv.gz in the Azure Blob Storage.</target>
          </trans-unit>
          <trans-unit id="113" xml:space="preserve">
            <source>When you specify compression property in an input dataset JSON, the pipeline can read compressed data from the source and when you specify the property in an output dataset JSON, the copy activity can write compressed data to the destination.</source>
            <target state="new">When you specify compression property in an input dataset JSON, the pipeline can read compressed data from the source and when you specify the property in an output dataset JSON, the copy activity can write compressed data to the destination.</target>
          </trans-unit>
          <trans-unit id="114" xml:space="preserve">
            <source>Here are a few sample scenarios:</source>
            <target state="new">Here are a few sample scenarios:</target>
          </trans-unit>
          <trans-unit id="115" xml:space="preserve">
            <source>Read GZIP compressed data from an Azure blob, decompress it, and write result data to an Azure SQL database.</source>
            <target state="new">Read GZIP compressed data from an Azure blob, decompress it, and write result data to an Azure SQL database.</target>
          </trans-unit>
          <trans-unit id="116" xml:space="preserve">
            <source>You define the input Azure Blob dataset with the compression JSON property in this case.</source>
            <target state="new">You define the input Azure Blob dataset with the compression JSON property in this case.</target>
          </trans-unit>
          <trans-unit id="117" xml:space="preserve">
            <source>Read data from a plain-text file from on-premises File System, compress it using GZip format, and write the compressed data to an Azure blob.</source>
            <target state="new">Read data from a plain-text file from on-premises File System, compress it using GZip format, and write the compressed data to an Azure blob.</target>
          </trans-unit>
          <trans-unit id="118" xml:space="preserve">
            <source>You define an output Azure Blob dataset with the compression JSON property in this case.</source>
            <target state="new">You define an output Azure Blob dataset with the compression JSON property in this case.</target>
          </trans-unit>
          <trans-unit id="119" xml:space="preserve">
            <source>Read a GZIP-compressed data from an Azure blob, decompress it, compress it using BZIP2, and write result data to an Azure blob.</source>
            <target state="new">Read a GZIP-compressed data from an Azure blob, decompress it, compress it using BZIP2, and write result data to an Azure blob.</target>
          </trans-unit>
          <trans-unit id="120" xml:space="preserve">
            <source>You define the input Azure Blob dataset with compression type set to GZIP and the output dataset with compression type set to BZIP2 in this case.</source>
            <target state="new">You define the input Azure Blob dataset with compression type set to GZIP and the output dataset with compression type set to BZIP2 in this case.</target>
          </trans-unit>
        </group>
      </group>
    </body>
  </file>
</xliff>