<properties
   pageTitle="Data partitioning guidance | Microsoft Azure"
   description="Guidance upon how to separate partitions to be managed and accessed separately."
   services=""
   documentationCenter="na"
   authors="dragon119"
   manager="masimms"
   editor=""
   tags=""/>

<tags
   ms.service="best-practice"
   ms.devlang="na"
   ms.topic="article"
   ms.tgt_pltfrm="na"
   ms.workload="na"
   ms.date="04/28/2015"
   ms.author="masashin"/>

# Conseils partitionnement de données

![](media/best-practices-data-partitioning/pnp-logo.png)

## Vue d'ensemble

Dans de nombreuses solutions à grande échelle, les données sont divisées en partitions distinctes qui peuvent être gérées et accessibles séparément. La stratégie de partitionnement doit être choisie avec soin afin de maximiser les avantages tout en minimisant les effets indésirables. Partitionnement peut aider à améliorer l'évolutivité, réduire les conflits et optimiser les performances. Un sous-produit de partitionnement est qui peut également fournir un mécanisme de répartition des données par le modèle d'utilisation ; vous pourriez archiver des données plus anciennes, plus inactifs (froides) moins cher de stockage de données.

## Pourquoi partitionner les données ?

La plupart des applications de nuage et services stockent et récupérer des données dans le cadre de leurs opérations. La conception des magasins de données qu'une application utilise peut avoir une incidence significative sur les performances, le débit et évolutivité d'un système. Une technique qui est couramment appliquée dans les systèmes à grande échelle consiste à diviser les données en partitions séparées.

> Le terme _partitionnement_ utilisée dans cette orientation se réfère au processus de diviser physiquement les données en stockages de données distincts. Ce n'est pas le même que le partitionnement de Table SQL Server, qui est un concept différent.

Partitionnement des données peut offrir un certain nombre d'avantages. Par exemple, il peut être appliqué afin de :

- **Améliorer l'évolutivité**. Mise à l'échelle d'un système de base de données unique finira par atteindre une limite matérielle physique. Diviser des données à travers plusieurs partitions, chacune d'entre elles est hébergé sur un serveur distinct, permet au système d'évoluer presque indéfiniment.
- **Améliorer les performances**. Opérations d'accès aux données sur chaque partition d'avoir lieu sur un plus petit volume de données. Sous réserve que les données sont partitionnées de manière appropriée, c'est beaucoup plus efficace. Opérations qui affectent plus d'une partition peuvent exécuter en parallèle. Chaque partition peut être située près de l'application qui l'utilise pour réduire la latence du réseau.
- **Améliorer la disponibilité**. Séparation des données sur plusieurs serveurs permet d'éviter un point de défaillance unique. Si un serveur tombe en panne, ou fait l'objet d'une maintenance planifiée, seules les données que partition n'est pas disponible. Opérations sur d'autres partitions peuvent continuer. Augmenter le nombre de partitions réduit l'incidence relative d'une panne de serveur unique en réduisant le pourcentage des données qui ne seront pas disponibles. Réplication de chaque partition peut réduire davantage le risque d'un échec de partition unique qui affectent les opérations. Il permet également la séparation des données essentielles qui doivent être continuellement et haute disponibilité des données de faible valeur (tels que les fichiers journaux) qui a des exigences de disponibilité inférieurs.
- **Améliorer la sécurité**. Selon la nature des données et comment il est partitionné, il peut être possible de séparer les données sensibles et non sensibles dans des partitions différentes et donc différents magasins de données ou serveurs. Sécurité peut ensuite être spécifiquement optimisée pour les données sensibles.
- **Fournir la souplesse opérationnelle**. Partitionnement offre de nombreuses possibilités pour fine tuning opérations, maximisant l'efficacité administrative et minimiser les coûts. Quelques exemples sont définissant des stratégies différentes pour la gestion, de surveillance, de sauvegarde et de restauration, et autres tâches d'administration basée sur l'importance des données dans chaque partition.
- **Correspondre à la Banque de données pour le modèle d'utilisation**. Partitionnement permet à chaque partition à être déployée sur un autre type de magasin de données, basé sur le coût et les fonctionnalités intégrées qu'offres du magasin de données. Par exemple, les données binaires volumineuses pourraient être stockées dans un magasin de données blob, tandis que se tiendrait plus les données structurées dans une base de données documentaire. Pour plus d'informations, consultez [Création d'une Solution polyglotte](https://msdn.microsoft.com/library/dn313279.aspx) dans les modèles & guide des pratiques [Accès aux données pour des Solutions hautement évolutive: à l'aide de SQL, NoSQL et persistance polyglotte](https://msdn.microsoft.com/library/dn271399.aspx) sur le site Web de Microsoft.

Certains systèmes n'implémentent pas la partition car il est considéré comme un passage supérieur plutôt qu'un avantage. Raisons courantes pour ce raisonnement incluent :

- De nombreux systèmes de stockage de données ne supportent pas les jointures dans partitions, et il peut être difficile de maintenir l'intégrité référentielle dans un système partitionné. Il est souvent nécessaire de mettre en œuvre les jointures et les vérifications d'intégrité dans le code de l'application (dans la couche de partitionnement), qui peut entraîner des e/s supplémentaires et de la complexité de l'application.
- Maintien des partitions n'est pas toujours une tâche triviale. Dans un système où les données sont volatiles, vous devrez peut-être rééquilibrer les partitions périodiquement pour réduire la contention et points chauds.
- Certains outils ordinaires ne fonctionnent pas naturellement avec les données partitionnées.

## Conception des partitions

Données peuvent être partitionnées de différentes manières : horizontalement, verticalement ou fonctionnellement. La stratégie que vous choisissez dépend du motif pour partitionner les données et les exigences des applications et des services qui utiliseront les données.

> [AZURE. REMARQUE] Les schémas de partitionnement décrits dans ce guide sont expliqués d'une manière indépendante de la technologie de stockage de données sous-jacent. Elles peuvent être appliquées à de nombreux types de magasins de données, y compris relationnelles et les bases de données NoSQL.

### Stratégies de partitionnement

Les trois stratégies typiques pour le partitionnement des données sont :

- **Partitionnement horizontal** (souvent appelée _sharding_). Dans cette stratégie, chaque partition est un magasin de données à part entière, mais toutes les partitions d'ont le même schéma. Chaque partition est connue comme un _tesson_ et est titulaire d'un sous-ensemble spécifique des données, telles que toutes les commandes pour un ensemble spécifique de clients dans une application de commerce électronique.
- **Le partitionnement vertical**. Dans cette stratégie chaque partition contient un sous-ensemble des champs pour les éléments dans le magasin de données. Les champs sont divisés selon leur mode d'emploi, par exemple en plaçant les champs accédés fréquemment dans une partition verticale et les champs moins fréquemment consultés dans un autre.
- **Le partitionnement fonctionnel**. Dans cette stratégie données sont regroupées selon comment il est utilisé par chaque contexte délimité dans le système. Par exemple, un système de commerce électronique qu'implements séparent les fonctions de facturation et gestion inventaire de produits peut stocker les données de facturation dans une partition et produit des données l'inventaire dans un autre.

Il est important de noter que les trois stratégies décrites ici peuvent être combinés.  Ils ne sont pas mutuellement exclusives, et vous devez envisager tous les lorsque vous concevez un schéma de partitionnement. Par exemple, vous pourriez diviser les données en éclats et ensuite utiliser le partitionnement vertical de subdiviser davantage les données dans chaque éclat. De même, les données dans une partition fonctionnelle peuvent être divisées en éclats (qui peuvent également être partitionnés verticalement).

Toutefois, les exigences différentes de chaque stratégie peuvent soulever un certain nombre de questions conflictuelles que vous devez évaluer et équilibre lors de la conception d'un schéma de partitionnement qui respecte les objectifs globaux de performance de traitement de données pour votre système. Les sections suivantes explorent chacune des stratégies plus en détail.

### Horizontal de partitionnement (sharding)

La figure 1 représente une vue générale de partitionnement horizontal ou sharding. Dans cet exemple, les données de l'inventaire produit sont divisées en éclats basés sur la clé de produit. Chaque fragment contient les données pour une rangée de touches éclat (A-G et H-Z), organisé par ordre alphabétique.

![](media/best-practices-data-partitioning/DataPartitioning01.png)

_La figure 1. -Horizontalement le partitionnement des données (sharding) basées sur une clé de partition_

Sharding permet de répartir la charge sur les ordinateurs plus ; réduisant la contention et amélioration des performances. Vous pouvez faire évoluer le système en ajoutant davantage de tessons s'exécutant sur des serveurs supplémentaires.

The most important factor when implementing this partitioning strategy is the choice of sharding key. It can be difficult to change the key after the system is in operation. The key must ensure that data is partitioned so that the workload is as even as possible across the shards. Note that different shards do not have to contain similar volumes of data, rather the important consideration is to balance the number of requests; some shards may be very large but each item is the subject of a low number of access operations, while other shards may be smaller but each item is accessed much more frequently. It is also important to ensure that a single shard does not exceed the scale limits (in terms of capacity and processing resources) of the data store being used to host that shard.

Le régime sharding devrait également éviter de créer des points chauds (ou partitions chaudes) qui peuvent affecter les performances et la disponibilité. Par exemple, en utilisant un hachage d'un identificateur de client au lieu de la première lettre du nom d'un client empêche la répartition déséquilibrée qui résulterait de lettres initiales courantes et moins courantes. Il s'agit d'une technique typique qui contribue à diffuser les données plus uniformément dans l'ensemble des partitions.

The sharding key you choose should minimize any future requirements to split large shards into smaller pieces, coalesce small shards into larger partitions, or change the schema that describes the data stored in a set of partitions. These operations can be very time consuming, and may require taking one or more shards offline while they are performed. If shards are replicated, it may be possible to keep some of the replicas online while others are split, merged, or reconfigured, but the system may need to limit the operations that can be performed on the data in these shards while the reconfiguration is taking place. For example, the data in the replicas could be marked as read-only to limit the scope of any inconsistences that could otherwise occur while shards are being restructured.

> Pour plus amples renseignements et des conseils sur un grand nombre de ces considérations, et techniques de bonnes pratiques pour la conception de données stocke ce partitionnement horizontal de mettre en œuvre, consultez le [Sharding Pattern](http://aka.ms/Sharding-Pattern)

### Le partitionnement vertical

L'utilisation la plus courante pour le partitionnement vertical est de réduire les e/s et des coûts de performance associés à aller chercher les éléments qui sont accessibles plus fréquemment. La figure 2 montre une vue d'ensemble d'un exemple de partitionnement vertical, où se déroulent des propriétés différentes pour chaque élément de données dans différentes partitions ; le nom, description et informations sur les prix des produits sont accessibles plus fréquemment que le volume est en stock ou la dernière date ordonnée.

![](media/best-practices-data-partitioning/DataPartitioning02.png)

_La figure 2. -Verticalement le partitionnement des données par son mode d'emploi_

Dans cet exemple, l'application interroge régulièrement le nom du produit, description et prix ensemble lors de l'affichage des détails des produits aux clients. Le niveau des stocks et la date où le produit a été modifié commandé chez le fabricant ont lieu dans une partition séparée, car ces deux éléments sont couramment utilisées conjointement. Ce schéma de partitionnement a l'avantage supplémentaire que les données relativement lent (nom du produit, description et prix) sont séparées de la plus dynamique de données (niveau de stock et dernière date ordonnée). Une application peut s'avérer bénéfique pour mettre en cache les données lent en mémoire si elle est fréquemment consultée.

Un autre scénario typique pour cette stratégie de partitionnement est de maximiser la sécurité des données sensibles. Par exemple, en stockant les numéros de carte de crédit et les numéros de vérification de sécurité carte correspondante en partitions séparées.

Le partitionnement vertical peut également réduire la quantité d'accès simultané pour les données.

> Le partitionnement vertical fonctionne au niveau de l'entité au sein d'un magasin de données, partiellement normaliser une entité pour le décomposer d'un _large_ élément d'un ensemble de _rétrécir_ éléments. Il est idéalement adapté pour des magasins de données orientée colonne comme HBase et Cassandra. Si les données d'une collection de colonnes soient peu susceptibles de changer, vous pouvez également envisager d'utiliser colonne magasins dans SQL Server.

### Le partitionnement fonctionnel

Pour les systèmes où il est possible d'identifier un contexte délimité pour chaque secteur d'activité distinct ou le service dans l'application, la séparation fonctionnelle fournit une technique pour améliorer les performances d'accès d'isolement et de données. Une autre utilisation courante de partitionnement fonctionnel consiste à séparer les données en lecture-écriture de données en lecture seule utilisées pour générer des rapports. La figure 3 montre un aperçu de la répartition fonctionnelle où les données d'inventaire sont séparées de données clients.

![](media/best-practices-data-partitioning/DataPartitioning03.png)

_La figure 3. -Fonctionnellement partitionner des données de contexte délimité ou un sous-domaine_

Cette stratégie de partitionnement peut aider à réduire les conflits d'accès données dans les différentes parties du système.

## Conception de partitions pour l'évolutivité

Il est essentiel de considérer la taille et la charge de travail pour chaque partition et les équilibrer afin que les données sont réparties pour atteindre une extensibilité maximale. Toutefois, vous devez également partitionner les données afin qu'elle ne dépasse pas les limites d'échelle d'un magasin de partition unique.

Suivez ces étapes Lorsque vous créez les partitions pour l'évolutivité :

1. Analyser la demande pour comprendre les modèles d'accès aux données, tels que la taille du jeu de résultats retourné par chaque requête, la fréquence d'accès, le temps de latence inhérents et du côté serveur calculer les exigences de traitement. Dans bien des cas, quelques grandes entités exigeront la plupart des ressources de traitement.
2. Basée sur l'analyse, déterminer l'évolutivité actuels et futurs des cibles telles que la taille des données et charge de travail et répartir les données sur les partitions pour atteindre l'objectif de l'évolutivité. Dans la stratégie de partitionnement horizontale, choisir la touche Eclat approprié est important pour s'assurer que la distribution est la même. Pour plus d'informations, voir la [Modèle sharding](http://aka.ms/Sharding-Pattern).
3. Make sure that the resources available to each partition are sufficient to handle the scalability requirements in terms of data size and throughput. For example, the node hosting a partition might impose a hard limit on the amount of storage space, processing power, or network bandwidth that it provides. If the data storage and processing requirements are likely to exceed these limits it may be necessary to refine your partitioning strategy or split data out further. For example, one scalability approach might be to separate logging data from the core application features by using separate data stores to prevent the total data storage requirements exceeding the scaling limit of the node. If the total number of data stores exceeds the node limit, it may be necessary to use separate storage nodes.
4. Surveiller le système sous utilisation afin de vérifier que les données sont distribuées comme prévu et que les partitions peuvent gérer la charge imposée à eux. Il pourrait être possible que l'utilisation ne correspond pas à celui prévu par l'analyse, il est possible de rééquilibrer les partitions. A défaut, il peut être nécessaire de revoir certaines parties du système pour obtenir l'équilibre nécessaire.

Notez que certains environnements de cloud allouer des ressources en ce qui concerne les limites de l'infrastructure, et vous devez vous assurer que les limites de votre limite sélectionnée fournissent assez d'espace pour toute croissance prévue du volume des données, en termes de bande passante, puissance de traitement et stockage des données. Par exemple, si vous utilisez le stockage de table Azure, un tesson occupé peut nécessiter plus de ressources que n'existe pas à une partition unique pour traiter les demandes (il y a une limite au volume de requêtes qui peuvent être gérés par une seule partition dans un laps de temps, consultez la page [Azure Storage évolutivité et cibles de rendement](https://msdn.microsoft.com/library/azure/dn249410.aspx) sur le site Web de Microsoft pour plus de détails). Dans ce cas, l'éclat devrez peut-être être repartitionnées pour répartir la charge. Si la taille totale ou le débit de ces tables dépasse la capacité d'un compte de stockage, il peut être nécessaire de créer des comptes de stockage supplémentaires et les tableaux à travers ces comptes. Si le nombre de comptes de stockage dépasse le nombre de comptes qui sont disponibles pour un abonnement, alors il peut être nécessaire d'utiliser des abonnements multiples.

## Conception de partitions pour les performances des requêtes

Les performances des requêtes peuvent souvent être réactivée à l'aide de petits ensembles de données et l'exécution de requêtes parallèles. Chaque partition doit contenir une faible proportion de l'ensemble des données, et cette réduction de volume peut améliorer les performances des requêtes. Toutefois, la partitionnement n'est pas une alternative pour la conception et la configuration d'une base de données appropriée. Par exemple, assurez-vous que vous avez les index nécessaires en place si vous utilisez une base de données relationnelle.

Suivez ces étapes Lorsque vous créez les partitions pour les performances des requêtes :

1. Examiner les exigences de l'application et la performance :
	- Utilisez les exigences commerciales pour déterminer des requêtes critiques qui doivent toujours effectuer rapidement.
	- Surveiller le système pour identifier les requêtes qui effectuent lentement.
	- Mettre en place dont les requêtes sont exécutées plus fréquemment. Une seule instance de chaque requête pourrait avoir un coût minime, mais la consommation cumulée de ressources pourrait être importante. Il peut être utile de séparer les données récupérées par ces requêtes dans une partition distincte, ou même un cache.
2. Partitionner les données qui sont l'origine de ralentissement des performances. Veiller à ce que vous avez :
	- Limiter la taille de chaque partition pour que le temps de réponse de requête est au sein de la cible.
	- Concevoir la touche éclat de manière que l'application peut facilement trouver la partition si vous implémentez un partitionnement horizontal. Cela empêche la requête devoir parcourir chaque partition.
	- Tenir compte de l'emplacement de la cloison sur les performances des requêtes. Si possible, essayez de conserver les données dans les partitions qui sont géographiquement proches les applications et les utilisateurs qui accèdent à ce.
3. Si une entité a des exigences de performance de débit et de la requête, utiliser le partitionnement fonctionnel basé sur cette entité. Si c'est toujours pas en mesure de satisfaire aux exigences, appliquer le partitionnement horizontal aussi bien. Dans la plupart des cas, une seule stratégie de partitionnement suffiront, mais dans certains cas, il est plus efficace de combiner ces deux stratégies.
4. Envisagez d'utiliser des requêtes asynchrones qui s'exécutent en parallèle sur les partitions pour améliorer les performances.

## Conception de partitions pour voir la disponibilité

Partitionnement des données peut améliorer la disponibilité des applications en faisant en sorte que le dataset entier ne constitue pas un point de défaillance unique et que les différents sous-ensembles de l'objet dataset peuvent être gérés indépendamment. Réplication des partitions contenant des données critiques peut également améliorer la disponibilité.

Conception et mise en œuvre de cloisons, tenir compte des facteurs suivants qui affectent la disponibilité :

- Comment critique les données sont d'opérations commerciales. Certaines données peuvent comporter des informations critiques telles que les détails de la facture ou des transactions bancaires. Autres données peuvent être tout simplement des données opérationnelles moins critiques, tels que les fichiers journaux, traces de performance et ainsi de suite. Après l'identification de chaque type de données, tenez compte :
	- Stocker des données critiques dans des partitions hautement disponible avec un plan de sauvegarde appropriée.
	- Établir une gestion séparée et la surveillance des mécanismes ou procédures pour les criticalities différentes de chaque jeu de données. Les données de lieu qui a le même niveau de criticité dans la même partition afin qu'il puisse sauvegardées ensemble à une fréquence appropriée. Par exemple, détenant des données pour les transactions bancaires des partitions peuvent doivent être sauvegardés plus fréquemment que les partitions tenant la journalisation ou des informations de traçage.
- Comment les partitions individuelles peuvent être gérées. Conception de partitions pour soutenir la maintenance et la gestion indépendante présente plusieurs avantages. Par exemple :
	- Si une partition échoue, il peut être récupéré indépendamment sans affecter les instances des applications qui accèdent aux données dans d'autres partitions.
	- Partitionnement des données par zone géographique peut permettre à des tâches de maintenance planifiées se produisent à des heures creuses pour chaque emplacement. S'assurer que les partitions ne sont pas trop grandes pour éviter toute maintenance planifiée en cours d'achèvement au cours de cette période.
- S'il faut répliquer des données critiques sur les partitions. Cette stratégie peut améliorer la disponibilité et les performances, même si elle peut également introduire des problèmes de cohérence. Il faut du temps pour que les modifications apportées aux données dans une partition à synchroniser avec chaque réplique, et durant cette période diverses partitions contient des valeurs de données différentes.

## Questions et considérations

En utilisant le partitionnement ajoute la complexité pour la conception et le développement du système. Il est important de considérer le partitionnement comme un élément fondamental de la conception du système, même si le système contient uniquement une partition unique au départ. Adressage de partitionnement qu'après coup quand le système commence à souffrir des performances et des problèmes d'évolutivité augmente seulement la complexité comme vous l'avez probablement ont maintenant un système live de maintenir. Mise à jour du système pour intégrer le partitionnement dans cet environnement nécessite non seulement modifier la logique d'accès aux données, il peut également impliquer la migration de grandes quantités de données existantes de le distribuer dans partitions, souvent alors que les utilisateurs attendent de pouvoir continuer à utiliser le système.

Dans certains cas, partitionnement ne constitue pas important parce que le jeu de données initial est petit et peut être facilement manipulé par un seul serveur. Cela peut être vrai dans un système qui ne devrait pas faire évoluer au-delà de sa taille initiale, mais beaucoup de systèmes commerciaux doivent être en mesure d'élargir le nombre d'utilisateurs augmente. Cette expansion est généralement accompagnée d'un accroissement du volume de données. Vous devez aussi comprendre que partitionnement n'est pas toujours une fonction de magasins de données volumineuses. Par exemple, un magasin de données de petite taille pourrait être fortement accessible par des centaines de clients simultanés. Partitionnement des données dans cette situation peut aider à réduire les conflits et améliorer le débit.

Vous devez considérer les points suivants lorsque vous concevez un schéma de partitionnement de données :

- Si possible, conserver les données pour les opérations de base de données les plus courantes dans chaque partition pour réduire au minimum les opérations cross-partition data access. Interrogation sur les partitions peut être plus longue que l'interrogation que dans une seule partition, mais optimisant les partitions d'un ensemble de requêtes peut nuire autres ensembles de requêtes. Pour réduire le temps de requête dans partitions où cela ne peut pas être évité, exécuter des requêtes parallèles sur les partitions et agréger les résultats dans l'application. Toutefois, cette approche ne serait pas possible dans certains cas, notamment quand il est nécessaire d'obtenir un résultat d'une requête et l'utiliser dans la requête suivante.
- Si les requêtes faire usage des données de référence relativement statique, tels que les tables de codes postaux ou des listes de produits, envisager de reproduire ces données dans toutes les partitions afin de réduire l'exigence d'une opération de recherche distinct dans une partition différente. Cette approche peut également réduire le risque de devenir un groupe de données « à chaud » qui est soumis à un trafic intense de dans l'ensemble du système, bien qu'il y a un coût supplémentaire associé à la synchronisation de tout changement qui pourrait survenir à ces données de référence les données de référence.
- Si possible, minimiser les exigences d'intégrité référentielle dans partitions verticales et fonctionnelles. Dans ces régimes, l'application elle-même est responsable du maintien de l'intégrité référentielle dans partitions lorsque les données sont mises à jour et consommées. D'exécuter des requêtes qui doivent unir leurs données à travers de multiples partitions plus lentement que les requêtes qui joignent les données uniquement au sein de la même partition parce que l'application devra généralement d'effectuer des requêtes consécutives basés sur une clé, puis sur une clé étrangère. Au lieu de cela, envisager de reproduire ou de normaliser les données pertinentes. Pour réduire le temps de requête lorsque la Croix-partition jointures sont nécessaires, exécuter des requêtes parallèles sur les partitions et joindre les données dans l'application.
- Considérer l'effet que le schéma de partitionnement peut avoir sur la cohérence des données dans l'ensemble des partitions. Vous devez évaluer si forte cohérence est en fait une exigence. Au lieu de cela, une approche commune dans le nuage est à mettre en œuvre la cohérence éventuelle. Les données de chaque partition sont mis à jour séparément, et la logique d'application peut assumer la responsabilité de veiller à ce que les mises à jour tous les terminer avec succès — ainsi que les incohérences qui peuvent dériver d'interrogation de données pendant l'exécution d'une opération finalement cohérente de manutention. Pour plus d'informations sur l'implémentation de cohérence éventuelle, reportez-vous à l'aide de cohérence. (#insertlink #)
- Examiner comment les requêtes permettra de repérer la bonne partition. Si une requête doit analyser toutes les partitions pour localiser les données requises il sera un impact significatif sur les performances, même lorsque vous utilisez plusieurs requêtes parallèles. Requêtes utilisées avec la verticale et des stratégies de partitionnement fonctionnels peuvent spécifier naturellement les partitions. Toutefois, lorsque vous utilisez le partitionnement horizontal (sharding), localiser un document peut être difficile car chaque éclat possède le même schéma. Une solution typique pour sharding est de maintenir une carte qui peut être utilisée pour Rechercher l'emplacement de l'éclat à des données précises. Cette carte peut être mis en oeuvre dans la logique de l'application de sharding ou maintenue par le magasin de données qui supporte les sharding transparent.
- Lorsque vous utilisez une stratégie de partitionnement horizontale, examiner périodiquement rééquilibrage les éclats pour répartir les données selon la taille et de la charge de travail pour minimiser les points chauds, optimiser les performances des requêtes et contourner les limites de stockage physique. Cependant, c'est une tâche complexe qui exige souvent l'utilisation d'un outil personnalisé ou un processus.
- Réplication de chaque partition offre une protection supplémentaire contre les pannes. Si un réplica unique échoue, les requêtes peuvent être dirigées vers une copie de travail.
- Si vous atteignez les limites physiques d'une stratégie de partitionnement, vous devrez peut-être prolonger l'évolutivité à un niveau différent. Par exemple, si le partitionnement est au niveau de la base de données il peut signifier localiser ou partitions dans plusieurs bases de données de réplication. Si le partitionnement est déjà au niveau base de données, et des limitations physiques sont un problème, cela peut signifier localiser ou réplication des partitions de plusieurs comptes d'hébergement.
- Évitez les opérations qui accèdent aux données de plusieurs partitions. Quelques données magasins de mettre en œuvre la cohérence transactionnelle et l'intégrité des opérations qui modifient les données, mais seulement lorsqu'il est situé dans une seule partition. Si vous avez besoin de prise en charge transactionnelle à travers plusieurs partitions, vous aurez probablement besoin mettre en œuvre dans le cadre de votre logique d'application, parce que les systèmes plus partitionnement ne fournissent pas de prise en charge native.

Tous les magasins de données nécessitent une gestion opérationnelle et contrôle de l'activité. Les tâches peuvent varier de chargement des données, sauvegarde et restauration des données, réorganiser les données et veiller à ce que le système fonctionne correctement et efficacement.

Tenir compte des facteurs suivants qui affectent la gestion opérationnelle :

- Considérez comment vous allez implémenter une gestion appropriée et tâches opérationnelles lorsque les données sont partitionnées, telles que la sauvegarde et la restauration, archivage des données, contrôler le système et autres tâches administratives. Par exemple, le maintien de cohérence logique pendant les opérations de sauvegarde et de restauration peut être un défi.
- Comment les données peuvent être chargées en plusieurs partitions et comment les nouvelles données venant d'autres sources pourraient être ajoutées. Certains outils et utilitaires peuvent ne pas supporter les opérations de données sharded comme le chargement de données dans la partition correcte, et donc cela peut nécessiter la création ou l'obtention de nouveaux outils et utilitaires.
- Comment les données seront archivées et supprimées sur une base régulière (peut-être tous les mois) afin d'empêcher une croissance excessive des partitions. Il peut être nécessaire transformer les données pour correspondre à un schéma d'archives différents.
- Envisager l'exécution d'un processus périodique pour localiser les problèmes d'intégrité des données telles que les données dans une partition qui référence des informations dans un autre, mais cette information est manquante. Le processus pourrait soit essayer de résoudre ces problèmes automatiquement ou de déclencher une alerte à un opérateur pour corriger les problèmes manuellement. Par exemple, dans une application de commerce électronique, informations de commande pourraient se tenir dans une seule partition, mais les lignes de facturation qui constituent chaque ordre pourrait avoir lieu dans un autre. Le processus de passer une commande a besoin d'ajouter des données pour les partitions de la peine. Si cette procédure échoue, il pourrait être ligne éléments stockés pour qui il n'y a aucune commande correspondante.

Technologies de stockage de données différents fournissent généralement leurs propres fonctionnalités pour prendre en charge le partitionnement. Les sections suivantes récapitulent les options mis en place par les banques de données couramment utilisés par les applications Azure et exposent les considérations de conception d'applications qui peuvent tirer le meilleur parti de ces fonctionnalités.

## Stratégies de partitionnement de base de données de SQL Azure

Base de données SQL Azure est un relationnel de base de données-as-a-service qui s'exécute dans le nuage. Il est basé sur Microsoft SQL Server. Une base de données relationnelle divise les informations dans des tables, et chaque table contient des informations sur les entités sous forme d'une série de lignes. Chaque ligne contient des colonnes qui contiennent les données pour les champs individuels d'une entité. Le [Base de données SQL Azure](https://msdn.microsoft.com/library/azure/ee336279.aspx) page sur le site Web de Microsoft fournit une documentation détaillée sur la création et l'utilisation de bases de données SQL.

## Le partitionnement horizontal avec échelle élastique

Une seule base de données SQL a une limite au volume de données qu'il peut contenir, et le débit est limité par des facteurs architectes et le nombre de connexions simultanées qu'il soutient. Base de données SQL Azure fournit une échelle élastique pour prendre en charge la mise à l'échelle horizontale pour une base de données SQL. En utilisant une échelle élastique, vous pouvez partitionner vos données réparties sur plusieurs bases de données SQL des éclats, et vous pouvez ajouter ou enlever des éclats que le volume de données que vous devez gérer se développe et se rétrécit. En utilisant une échelle élastique peut également aider à réduire la contention en distribuant la charge sur bases de données.

> [AZURE. REMARQUE] Balance élastique est actuellement dans l'aperçu à partir de janvier 2015. Il s'agit d'un remplacement pour les fédérations de base de données de SQL Azure qui seront retirés. Les installations existantes de Fédération de base de données SQL Azure peuvent être migrées vers l'échelle élastique à l'aide de la [Utilitaire de Migration des fédérations](https://code.msdn.microsoft.com/vstudio/Federations-Migration-ce61e9c1). Alternativement, vous pouvez implémenter votre propre mécanisme sharding si votre scénario ne se prête pas naturellement aux fonctionnalités fournies par échelle élastique.

Chaque fragment est implémenté comme une base de données SQL. Un éclat peut contenir plus d'un groupe de données (appelée un _shardlet_), et chaque base de données maintient les métadonnées qui décrivent le shardlets qu'il contient. Un shardlet peut être un élément de données unique, ou il peut être un groupe d'éléments qui partagent la même clé de shardlet. Par exemple, si vous êtes données sharding dans une application multi-locataires, la clé de shardlet pourrait être l'ID du locataire et toutes les données pour un locataire donné seraient tiendrait dans le cadre de la même shardlet. Données pour les autres locataires seraient tiendrait à différents shardlets.

Il incombe à l'interface de programmation d'associer un dataset avec une clé de shardlet. Une base de données SQL distincte agit comme un gestionnaire de tesson-carte globale qui contient une liste des bases de données (éclats) qui composent l'ensemble du système ainsi que des informations sur les shardlets dans chaque base de données. Tout d'abord, une application cliente qui accède aux données se connecte à la base de données du gestionnaire de tesson-carte du monde pour obtenir une copie de l'éclat-carte (tessons de listage et shardlets) dont il met en cache localement. L'application puis utilise ces informations pour router les requêtes de données vers le serveur approprié. Cette fonctionnalité est cachée derrière une série d'API contenues dans Azure SQL Database élastique échelle Client Library, disponible comme paquet NuGet. La page [Vue d'ensemble SQL Azure Database échelle élastique](sql-database-elastic-scale-introduction.md) sur Microsoft site Web fournit une introduction plus complète à l'échelle élastique.

> [AZURE. REMARQUE] Vous pouvez répliquer la base de données du gestionnaire de tesson-carte mondiale pour réduire la latence et améliorer la disponibilité. Si vous implémentez la base de données en utilisant l'un de la prime de niveaux de prix vous pouvez configurer geo-réplication active pour copier des données en continu aux bases de données dans différentes régions. Créer une copie de la base de données dans chaque région où reposent utilisateurs et configurer votre application pour se connecter à cette copie pour obtenir le plan de l'éclat.

> Une autre approche consiste à utiliser la synchronisation de données SQL Azure ou un pipeline Azure Data Factory pour répliquer la base de données du gestionnaire de tesson-carte dans toutes les régions. Cette forme de réplication s'exécute périodiquement et est plus appropriée si la carte de l'éclat change rarement. En outre, la base de données du gestionnaire de tesson-carte n'a pas à être créé à l'aide d'une prime de niveau de prix.

Balance élastique fournit deux schémas de mappage des données de shardlets et en les enregistrant en éclats :

- Une carte de Shard liste décrit une association entre une clé unique et un shardlet. Par exemple, dans un système de plusieurs locataire, les données pour chaque locataire pourraient être associées à une clé unique et stockées dans son propre shardlet. Afin de garantir la confidentialité et l'isolement (pour empêcher un locataire d'épuiser les ressources de stockage de données disponibles pour les autres), chaque shardlet pourrait avoir lieu au sein de son propre éclat.

![](media/best-practices-data-partitioning/PointShardlet.png)

_La figure 4. -En utilisant une carte de fragment de liste pour stocker les données de locataire en éclats séparés_

- Une gamme éclat carte décrit une association entre un ensemble de valeurs clés contiguës et un shardlet. Dans l'exemple de plusieurs locataire décrit plus haut, comme une alternative à l'implémentation dédié shardlets, vous pouvez regrouper les données pour un ensemble de locataires (chacune avec sa propre clé) au sein de la même shardlet. Ce schéma est moins cher que le premier (locataires partagent des ressources de stockage de données), mais au risque de réduire la protection des données et l'isolement.

![](media/best-practices-data-partitioning/RangeShardlet.png)

_La figure 5. -En utilisant une carte de fragment de gamme pour stocker des données pour une gamme des locataires dans un tesson_

Notez qu'un éclat unique peut contenir les données pour plusieurs shardlets. Par exemple, vous pouvez utiliser la liste shardlets pour stocker des données pour les différents locataires non contigus dans le même éclat. Vous pouvez également mélanger gamme shardlets et shardlets de la liste dans le même éclat, même si elles seront traitées par des cartes différentes dans la base de données de gestionnaire tesson-carte globale (la base de données du gestionnaire de global Eclat-carte peut contenir plusieurs mappages d'éclat). La figure 6 illustre cette approche.

![](media/best-practices-data-partitioning/MultipleShardMaps.png)

_La figure 6. -Mise en œuvre de multiples éclat cartes_

Le schéma de partitionnement que vous implémentez peut ont une incidence significative sur les performances de votre système et également influer sur le taux au cours de laquelle éclats doivent être ajoutées ou supprimées ou les données repartitionnées travers éclats. Vous devez envisager les points suivants lorsque vous utilisez échelle élastique pour partitionner les données :

- Group data that is used together into the same shard and avoid operations that need to access data held in multiple shards. Bear in mind that with Elastic Scale a shard is a SQL database in its own right, and Azure SQL Database does not support cross-database joins; these operations have to be performed on the client-side. Also remember that with Azure SQL Database referential integrity constraints, triggers, and stored procedures in one database cannot reference objects in another, so don't design a system that has dependencies between shards. However, a SQL database can contain tables holding copies of reference data frequently used by queries and other operations, and these tables do not have to belong to any specific shardlet. Replicating this data across shards can help to remove the need to join data that spans databases. Ideally, such data should be static or slow-moving to minimize the replication effort and reduce the chances of it becoming stale.

	> [AZURE. REMARQUE] Bien que la base de données de SQL Azure ne supporte pas les jointures inter-bases de données, l'API échelle élastique vous permet d'effectuer des requêtes cross-éclat qui peuvent parcourir en toute transparence les données contenues dans tous les shardlets référencés par une carte de l'éclat. La Croix-éclat de pauses élastique échelle API interroge vers le bas en une série de requêtes individuelles (une pour chaque base de données) et puis fusionne les résultats ensemble. Pour plus d'informations, consultez le [Interrogation de multi-Eclat](sql-database-elastic-scale-multishard-querying.md) page sur le site Web de Microsoft.

- Les données stockées dans shardlets qui appartiennent à la même carte de shard doivent avoir le même schéma. Par exemple, ne créez pas une carte de fragment de liste qui pointe vers un shardlets contenant des données de locataire et autres shardlets contenant des informations de produit. Cette règle n'est pas appliquée par échelle élastique, mais la gestion des données et l'interrogation devient très complexe si chaque shardlet possède un schéma différent. Dans l'exemple que je viens de citer, vous devez créer deux cartes de fragment de liste ; un référencement données de locataire et l'autre point d'information des produits. N'oubliez pas que les données appartenant à des shardlets différents peuvent être stockées dans le même éclat.

	> [AZURE. REMARQUE] La fonctionnalité de requête cross-éclat de l'API d'échelle élastique dépend de chaque shardlet dans la carte de fragment contenant le même schéma.
- Opérations transactionnelles sont pris en charge uniquement pour les données qui s'est tenues au sein du même éclat et non ceux d'éclats. Les transactions peuvent s'étendre à la shardlets tant qu'ils s'inscrivent dans le même éclat. Par conséquent, si votre logique métier a besoin d'effectuer des transactions, stocker les données concernées dans le même éclat ou mettre en œuvre la cohérence éventuelle. Pour plus d'informations, reportez-vous à l'aide de cohérence des données.
- Placer des éclats près les utilisateurs qui accèdent à des données dans ces éclats (géo-localiser éclats). Cette stratégie contribuera à réduire les temps de latence.
- Éviter d'avoir un mélange de très actif (hotspots) et éclats relativement inactifs. Essayez et répartis la charge uniformément sur éclats. Il faudra peut-être les shardlet clés de hachage.
- Si vous êtes tessons de géo-localisation, assurez-vous que les clés de hachage mappent vers shardlets qui s'est tenue en éclats stockées à proximité des utilisateurs qui accèdent à ces données.
- Actuellement, seulement un jeu limité de données SQL types sont pris en charge en tant que clés de shardlet ; _int, bigint, varbinary,_ et _uniqueidentifier_. Le SQL _int_ et _bigint_ les types correspondent à la _int_ et _long_ types de données en langage c#, et avoir les mêmes échelles. Le SQL _varbinary_ type peut être géré en utilisant un _Octet_ tableau en c# et SQL _uniqueidentier_ le type correspond à la _Guid_ classe dans le .NET Framework.

Comme son nom l'indique, échelle élastique permet un système ajouter et supprimer des éclats que le volume de données se rétrécit et se développe. L'API dans la bibliothèque Azure SQL Database élastique échelle Client permettent à une application créer et supprimer dynamiquement les éclats (et transparente mise à jour le gestionnaire de tesson-carte), mais enlever un tesson est une opération destructrice qui nécessite également la suppression de toutes les données de cet éclat. Si une application doit diviser un tesson en deux fragments séparés ou de combiner ensemble les tessons, échelle élastique assure un service de Split/Merge distinct. Ce service s'exécute dans un service hébergé par cloud (le développeur doit créer ce service hébergé sur le nuage) et s'occupe de migration des données entre les éclats en toute sécurité. Pour plus d'informations, consultez la rubrique [Fractionnement et fusion avec échelle élastique](sql-database-elastic-scale-overview-split-and-merge.md) sur le site Web de Microsoft.

## Stratégies de partitionnement pour Azure Storage

Le stockage d'Azur propose trois abstractions pour gérer les données :

- Stockage de table, qui implémente le stockage structure évolutive. Une table contient une collection d'entités, dont chacune est constituée par un ensemble de propriétés et de valeurs.
- Stockage BLOB, qui fournit le stockage pour les fichiers et les objets de grande taille.
- Stockage des files d'attente, qui soutiennent la messagerie asynchrone fiable entre les applications.

Rangement de table et de stockage Blob sont essentiellement clé-valeur magasins optimisés à recevoir respectivement les données structurées et non structurées. Stockage des files d'attente fournissent un mécanisme pour créer des applications faiblement couplées et évolutives. Table de stockage, stockage Blob et stockage des files d'attente sont créés dans le contexte d'un compte de stockage Azure. Comptes de stockage Azur prennent en charge les trois formes de redondance :

- Stockage sur place redondant, qui conserve des trois copies de données au sein d'un centre de données unique. Cette forme de redondance protège contre une défaillance matérielle, mais pas contre une catastrophe qui englobe le centre de données entière.
- Stockage redondant zone qui maintient trois copies de données répartis sur différents centres de données dans la même région (ou entre deux régions géographiquement proches). Cette forme de redondance peut protéger contre les catastrophes qui se produisent au sein d'un centre de données unique, mais ne peut pas protéger contre le réseau à grande échelle qui affectent toute une région se déconnecte. Notez le stockage redondant zone seulement actuellement n'est disponible que pour les blobs de bloc.
- Stockage géo-redondantes, ce qui maintient six copies de données ; dans une région (votre région) en trois exemplaires et un autre à trois exemplaires dans une région reculée. Cette forme de redondance fournit le meilleur niveau de protection contre les catastrophes.

Microsoft a publié des cibles d'évolutivité pour les comptes de stockage d'Azur ; Voir la page [Azure Storage évolutivité et cibles de rendement](https://msdn.microsoft.com/library/azure/dn249410.aspx) sur le site Web de Microsoft. Actuellement, la capacité totale de stockage compte (la taille des données dans le stockage de la table, stockage blob et tenues exceptionnelles messages dans la file d'attente de stockage) ne doit pas dépasser 500 to. Le taux de demande maximale (en supposant une entité 1KO blob et la taille des messages) est de 20K par seconde. Si votre système est susceptible de dépasser ces limites, puis examiner la charge de partitionnement à travers de multiples comptes de stockage ; un seul abonnement Azur peut créer jusqu'à 100 comptes de stockage. Toutefois, Notez que ces limites peuvent changer au fil du temps.

## Partitionnement de table bleu azur

Stockage de table Azure est une clé/valeur stockée conçu autour de partitionnement. Toutes les entités sont stockées dans une partition, et les partitions sont gérées en interne par le stockage de table Azure. Chaque entité stockée dans une table doit fournir une clé composée de deux parties :

- La clé de partition. Il s'agit de des valeurs de chaîne qui détermine dans quelle partition de table Azure placera l'entité. Toutes les entités possédant la même clé de partition seront stockées dans la même partition.
- La clé de la ligne. Il s'agit d'une autre valeur de chaîne qui identifie l'entité au sein de la partition. Toutes les entités au sein d'une partition sont lexicalement, triées par ordre croissant, par cette clé. La combinaison de touches touche/ligne partition doit être unique pour chaque entité et ne doit pas dépasser 1KB dans la longueur.

Le reste des données d'une entité se compose de champs définis par l'application. Aucun schéma particulier n'est appliquées, et chaque ligne peut contenir un ensemble différent de champs définis par l'application. La seule limitation est que la taille maximale d'une entité (y compris les clés de partition et ligne) est actuellement de 1 Mo. La taille maximale d'une table est TB 200, bien que ces chiffres peuvent changer dans le futur (vérification de la page [Azure Storage évolutivité et cibles de rendement](https://msdn.microsoft.com/library/azure/dn249410.aspx) sur le site Web de Microsoft pour les informations les plus récentes sur ces limites. Si vous tentez de stocker des entités qui dépassent cette capacité, alors envisager de les scinder en plusieurs tables ; utiliser le partitionnement vertical et diviser les champs en les groupes les plus susceptibles d'être accessibles à l'ensemble.

La figure 7 montre la structure logique d'un compte de stockage de l'exemple (données Contoso) pour une application de commerce électronique fictif. Les comptes de stockage contient trois tables (informations client, informations sur le produit et ordre Info), et chaque table possède plusieurs partitions. Dans le tableau d'informations client, les données sont partitionnées selon la ville dans laquelle se trouve le client, et la touche de ligne contient l'ID de client. Dans le tableau d'informations sur le produit, les produits sont partitionnées par catégorie de produit et la clé de ligne contient le numéro du produit. Dans le tableau de l'ordre d'informations les commandes sont partitionnés de la date à laquelle elles ont été placées et la clé de la ligne indiquée l'heure de que réception de l'ordre. Notez que toutes les données sont classées par la clé de la ligne de chaque partition.

![](media/best-practices-data-partitioning/TableStorage.png)

_La figure 7. -Les tables et les partitions dans un compte de stockage exemple_

> [AZURE. REMARQUE] Stockage de table Azur ajoute également un champ timestamp à chaque entité. Le champ timestamp est maintenu par le stockage de la table et est mis à jour chaque fois que l'entité est modifiée et réécrites sur une partition. Le service de stockage de table utilise ce champ pour implémenter l'accès concurrentiel optimiste (chaque fois qu'une application réécrit une entité de stockage de la table, le service de stockage de table compare la valeur de l'horodatage dans l'entité en cours d'écriture avec la valeur contenue dans la table de stockage, et si elles sont différentes une autre application doit avoir modifié l'entité puisqu'il a été récupéré et l'opération d'écriture échoue). Vous ne devez pas modifier ce champ dans votre propre code, et ni vous devez spécifier une valeur pour ce champ lorsque vous créez une nouvelle entité.

Stockage de table Azure utilise la clé de partition pour déterminer la façon de stocker les données. Si une entité est ajoutée à une table avec une clé de partition inutilisés, stockage de table Azure va créer une nouvelle partition pour cette entité. Autres entités possédant la même clé de partition seront stockées dans la même partition. Ce mécanisme implémente efficacement une stratégie de déploiement automatique. Chaque partition sera stockée sur un seul serveur dans un datacenter Azur (pour aider à faire en sorte que d'exécuter rapidement des requêtes qui extraient des données d'une partition unique), mais les différentes partitions peuvent être distribuées sur plusieurs serveurs. En outre, un seul serveur peut héberger plusieurs partitions si ces partitions sont limitées en taille.

Vous devez considérer les points suivants lorsque vous concevez vos entités pour le stockage de table Azure :

- La sélection de valeurs de clé clé et ligne de partition devrait être mû par la voie dans laquelle les données sont accessibles. Vous devez choisir une combinaison de touches de clé/ligne de partition qui supporte la majorité de vos questions. Les requêtes plus efficaces récupérera les données en spécifiant la clé de partition et la touche de ligne. Requêtes qui spécifient une clé de partition et une gamme de clés de ligne peuvent être satisfaits par la numérisation d'une seule partition ; C'est relativement rapide, parce que les données se trouvent dans l'ordre des lignes principales. Requêtes qui ne spécifient au moins la clé de partition peuvent nécessiter un stockage Azure table à la numérisation de chaque partition pour vos données.

	> [AZURE. ASTUCE] Si une entité a une touche naturelle, puis utilisez-la comme la clé de partition et spécifiez une chaîne vide comme la clé de la ligne. Si une entité a une clé composite composée de deux propriétés, sélectionnez la propriété plus lente évolution comme la clé de partition et l'autre comme la clé de la ligne. Si une entité a plus de deux propriétés essentielles, utilisez une concaténation des propriétés pour fournir les clés de partition et de la ligne.

- Si vous effectuez régulièrement des requêtes qui Rechercher des données à l'aide de domaines autres que les clés de partition et de la ligne, envisagez d'implémenter la [Modèle de Table d'index](https://msdn.microsoft.com/library/dn589791.aspx).
- If you generate partition keys using a monotonic increasing or decreasing sequence (such as "0001", "0002", "0003", …) and each partition only contains a limited amount of data, then Azure table storage may physically group these partitions together on the same server. This mechanism assumes that the application is most likely to perform queries across a contiguous range of partitions (range queries) and is optimized for this case. However, this approach can lead to hotspots focused on a single server as all inserts of new entities will likely be concentrated at one or other end of the contiguous ranges. It can also reduce scalability. To spread the load more evenly across servers, consider hashing the partition key to make the sequence more random.
- Stockage de table Azure prend en charge les opérations transactionnelles pour les entités qui appartiennent à la même partition. Cela signifie qu'une application peut exécuter plusieurs insert, update, delete, remplacer ou les opérations de fusion comme une unité atomique (sous réserve de la transaction ne comprenant pas plus de 100 entités et la charge utile de la demande ne dépassant ne pas 4 MB dans la taille). Les opérations qui s'étendent sur plusieurs partitions ne sont pas transactionnelles et pouvant vous obliger à mettre en œuvre la cohérence éventuelle tel que décrit par la direction de cohérence des données. Pour plus d'informations sur le stockage de la table et les transactions, visitez le [Exécution des Transactions de l'entité groupe](https://msdn.microsoft.com/library/azure/dd894038.aspx) page sur le site Web de Microsoft.
- Donner une grande attention à la granularité de la clé de partition :
	- En utilisant la même clé de partition pour chaque entité provoquera le service de stockage de table pour créer une seule grande partition qui s'est tenue sur un seul serveur empêchant de l'évolution et au contraire se concentrant la charge sur un seul serveur. Ainsi, cette approche consiste uniquement pour les systèmes qui gèrent un petit nombre d'entités. Toutefois, cette approche garantit que toutes les entités puissent participer aux opérations de groupe entité.
	- À l'aide d'une clé de partition unique pour chaque entité provoquera le service de stockage de table créer une partition distincte pour chaque entité, ce qui pourrait entraîner un grand nombre de petites cloisons (selon la taille des entités). Cette approche est plus évolutive que l'utilisation d'une clé de partition unique, mais entité groupe transactions ne sont pas possibles et les requêtes qui aller chercher plus d'une entité peuvent impliquer la lecture de plusieurs serveurs. Toutefois, si l'application effectue des requêtes de plage puis en utilisant une séquence monotone pour générer les clés de partition peuvent permettre d'optimiser ces requêtes.
	- Partager la clé de partition entre un sous-ensemble d'entités vous permet de regrouper des entités dans la même partition. Opérations qui impliquent des entités associées peuvent être effectuées à l'aide d'opérations de groupe entité, et les requêtes qui récupère un ensemble d'entités associées peuvent être satisfaites en accédant à un seul serveur.

Pour plus d'informations sur le partitionnement des données en stockage de table Azure, voir l'article [Conception d'une stratégie de partitionnement évolutive pour le stockage de Table bleu azur](https://msdn.microsoft.com/library/azure/hh508997.aspx) sur le site Web de Microsoft.

## Partitionnement de stockage blob Azure

Azure Blob Storage permet de placer des objets binaires volumineux, actuellement jusqu'à 200 Go en taille pour les blobs de bloc ou 1To pour blob de page (pour les informations les plus récentes, visitez le [Azure Storage évolutivité et cibles de rendement](https://msdn.microsoft.com/library/azure/dn249410.aspx) page sur le site de Microsoft). Utilisez bloc blobs dans des scénarios tels que le streaming où vous avez besoin de télécharger ou télécharger de gros volumes de données rapidement. Utilisez les blobs de page pour des applications nécessitant au hasard plutôt qu'un accès à certaines données de série.

Chaque objet blob (bloc ou page) se tient dans un récipient dans un compte de stockage Azure. Vous pouvez utiliser des conteneurs pour regrouper les blobs connexes qui ont les mêmes exigences de sécurité ensemble, bien que ce regroupement est logique plutôt que physique. À l'intérieur d'un conteneur, chaque objet blob possède un nom unique.

Stockage BLOB est partitionné automatiquement basé sur le nom de l'objet blob. Chaque objet blob se tient à sa propre partition, et bidules dans le même conteneur ne partagent pas une partition. Cette architecture permet de stockage blob Azure équilibrer la charge entre les serveurs transparente comme blobs différents dans le même conteneur peuvent être distribués sur des serveurs différents.

Les actions de l'écriture d'un seul bloc (blob block) ou une page (blob de page) sont atomiques, mais les opérations qui s'étendent des blocs, des pages ou BLOB ne sont pas. Si vous avez besoin assurer la cohérence lors de l'exécution des opérations d'écriture à travers les blocs, les pages et objets BLOB, vous devrez souscrire un verrou en écriture à l'aide d'un bail de blob.

Stockage blob Azure prend en charge des vitesses de transfert pouvant atteindre 60 Mo par seconde ou 500 requêtes par seconde pour chaque objet blob. Si vous prévoyez dépassant ces limites, et les données blob sont relativement statiques, envisagez répliquant blobs à l'aide de la Content Delivery Network (CDN) des Azure. Pour plus d'informations, consultez la page [À l'aide de CDN pour Azure](cdn-how-to-use.md) sur le site Web de Microsoft. Pour des conseils supplémentaires et considérations, voir l'article Content Delivery Network (CDN).

## Partitionnement des files d'attente de Azure storage

Files d'attente de stockage Azur vous permettent d'implémenter une messagerie asynchrone entre les processus. Un compte de stockage Azure peut contenir n'importe quel nombre de files d'attente, et chaque file d'attente peut contenir n'importe quel nombre de messages. La seule limitation est l'espace disponible dans le compte de stockage. La taille maximale d'un message individuel est de 64 Ko. Si vous avez besoin de messages plus gros que cela, alors envisager d'utiliser les files d'attente de Bus de Service Azure.

Each storage queue has a unique name within the storage account in which it is contained. Azure partitions queues based on the name, and all messages for the same queue are stored in the same partition, controlled by a single server. Different queues can be managed by different servers to help balance the load. The allocation of queues to servers is transparent to applications and users. In a large scale application, don't use the same storage queue for all instances of the application as this approach may cause the server hosting the queue to become a hotspot; use different queues for different functional areas of the application. Azure storage queues do not support transactions, so directing messages to different queues should have little impact on messaging consistency.

Une file d'attente de Azure storage peut gérer jusqu'à 2000 messages par seconde.  Si vous avez besoin pour traiter les messages à un rythme plus rapide que cela alors envisager de créer plusieurs files d'attente. Par exemple, dans une application mondiale, créer des files d'attente de stockage distinct dans les comptes de stockage distinct pour gérer des instances de l'application en cours d'exécution dans chaque région.

## Stratégies de partitionnement pour Azure Service Bus

Azur Service Bus utilise un courtier de messages pour gérer les messages envoyés à une file d'attente Service Bus ou sujet. Par défaut, tous les messages envoyés à une file d'attente ou un sujet sont gérées par le même processus de courtier de messages. Cette architecture peut imposer une limitation sur le débit global de la file d'attente de messages. Toutefois, vous pouvez partitionner également une file d'attente ou un sujet lorsqu'il est créé en définissant le _EnablePartitioning_ propriété de la description de file d'attente ou un sujet à _true_. A partitioned queue or topic is divided up into multiple fragments, each of which is backed by a separate message store and message broker. Service Bus takes responsibility for creating and managing these fragments. When an application posts a message to a partitioned queue or topic, Service Bus assigns the message to a fragment for that queue or topic. When an application receives a message from a queue or subscription, Service Bus checks each fragment for the next available message and then passes it to the application for processing. This structure helps to distribute the load across message brokers and message stores, increasing scalability and improving availability; if the message broker or message store for one fragment is temporarily unavailable, Service Bus can retrieve messages from one of the remaining available fragments.

Bus de service affecte un message vers un fragment comme suit :

- Si le message appartient à une session, tous les messages avec la même valeur pour la _ SessionId_  propriété sont envoyés dans le même fragment.
- Si le message n'appartient-elle pas à une session, mais l'expéditeur a spécifié une valeur pour la _PartitionKey_ propriété, puis tous les messages avec le même _PartitionKey_ valeur sont envoyer vers le même fragment.

	> [AZURE. REMARQUE] Si le _SessionId_ et _PartitionKey_ Propriétés sont tous deux spécifiées, puis ils doivent être sur la même valeur sinon que le message sera rejeté.
- Si le _SessionId_ et _PartitionKey_ Propriétés pour un message ne sont pas spécifiées, mais la détection des doublons est activée, la _MessageId_ propriété sera utilisée. Tous les messages avec le même _MessageId_ serez dirigé vers le même fragment.
- Si les messages n'incluent pas un _SessionId, PartitionKey,_ ou _MessageId_ propriété, puis Service de Bus assigne des messages à des fragments de façon alternée. Si un fragment est indisponible, Service Bus passera à la prochaine. De cette façon, une anomalie temporaire dans l'infrastructure de messagerie ne provoque pas l'opération d'envoi-message à l'échec.

Vous devriez considérer les points suivants lors du choix et comment, ou si, à la partition d'une file d'attente de message de Service de Bus ou de la rubrique :

- Sujets et les files d'attente de Bus de Service sont créés dans le cadre d'un espace de noms Service Bus. Service de Bus permet actuellement jusqu'à 100 partitionnées de files d'attente ou des sujets par espace de noms.
- Chaque espace de noms Service Bus impose des quotas sur les ressources disponibles, telles que le nombre d'abonnements par rubrique, le nombre d'envoi simultané et recevoir les requêtes par seconde et le nombre maximal de connexions simultanées peut être établie. Ces quotas sont documentées sur le site Web de Microsoft à la page [Quotas de Bus de service](https://msdn.microsoft.com/library/azure/ee732538.aspx). Si vous envisagez de dépasser ces valeurs, puis créer des espaces de noms supplémentaires avec leurs propres files d'attente et les sujets et le travail à travers ces espaces de noms. Par exemple, dans une application mondiale, créer des espaces de noms distincts dans chaque région et configurer des instances de l'application pour utiliser les files d'attente et les sujets dans l'espace de noms le plus proche.
- Les messages qui sont envoyés dans le cadre d'une opération doivent spécifier une clé de partition. Cela peut être un _SessionId, PartitionKey,_ ou _MessageId_. Tous les messages qui sont envoyés dans le cadre de la même opération doivent spécifier la même clé de partition car ils doivent être manipulés par le même processus de courtier de messages. Vous ne pouvez pas envoyer des messages à différentes files d'attente ou de sujets dans la même transaction.
- Vous ne pouvez pas configurer un partitionnée file d'attente ou un sujet d'être automatiquement supprimés lorsqu'il devienne inactif.
- Si vous générez des solutions multi-plateformes ou hybride, impossible actuellement d'utiliser partitionnées de files d'attente et les sujets avec l'Advanced Message Queuing Protocol (AMQP).

## Stratégies de partitionnement pour Azure DocumentDB

DocumentDB Azur est une base de données NoSQL qui peut stocker des documents. Un document en DocumentDB est une représentation sérialisée au format JSON d'un objet ou autre élément de données. Aucun schéma fixe n'est appliquées sauf que chaque document doit contenir un identifiant unique.

Les documents sont organisés en collections. Une collection vous permet de regrouper les documents connexes. Par exemple, dans un système qui maintient les blogs, vous pourriez stocker le contenu de chaque message de blog comme un document dans une collection et créer des collections pour chaque type d'objet. Par ailleurs, dans une application mutualisée comme un système qui permet aux différents auteurs pour contrôler et gérer leur propre blog postes, vous pouvez partitionner les articles par auteur et créer une collecte séparée pour chaque auteur. L'espace de stockage alloué aux collections est élastique et peut se rétrécir ou se développer selon les besoins.

Collections de documents fournissent un mécanisme naturel pour partitionner les données dans une base de données unique. En interne, une base de données DocumentDB peut s'étendre sur plusieurs serveurs, et DocumentDB peuvent tenter de répartir la charge en distribuant des collections sur les serveurs. Le moyen le plus simple à mettre en œuvre sharding consiste à créer une collection pour chaque éclat.

> [AZURE. REMARQUE] Chaque DocumentDB est alloué des ressources en termes d'une _niveau de performance_. Un niveau de performance est associé avec un _unité de demande_ Limite de fréquence (RU). La limite de taux de RU spécifie le volume des ressources qui sera réservé pour cette collection et est réservé à l'usage exclusif de cette collection. Le coût d'une collection dépend du niveau de performance sélectionné pour cette collection ; plus la performance niveau (et pulsations RU) plus les frais. Vous pouvez ajuster le niveau de performance d'une collection en utilisant le portail de gestion Azure. Pour plus d'informations, consultez la page [Niveaux de performance dans DocumentDB](documentdb-performance-levels.md) sur le site Web de Microsoft.

Toutes les bases de données sont créés dans le contexte d'un compte DocumentDB. Un seul compte de DocumentDB peut contenir plusieurs bases de données et spécifie dans quelle région les bases de données sont créées. Chaque compte DocumentDB met également en œuvre ses propres contrôle d'accès. Vous pouvez utiliser des comptes DocumentDB de géo-localiser éclats (collections au sein de bases de données) à proximité des utilisateurs qui ont besoin d'y accéder et appliquer des restrictions de sorte que seuls les utilisateurs peuvent se connecter à eux.

Chaque compte de DocumentDB possède un quota qui limite le nombre de bases de données et les collections qu'il peut contenir et la quantité de stockage des documents disponible. Ces limites peuvent être modifiées, mais sont décrits sur la page [Quotas et limites DocumentDB](documentdb-limits.md) sur le site Web de Microsoft. Il est théoriquement possible que si vous implémentez un système où tous les fragments appartiennent à la même base de données vous pourriez atteindre la limite de capacité de stockage du compte. Dans ce cas, vous devrez peut-être créer des bases de données et des comptes de DocumentDB supplémentaires et distribuer les éclats sur ces bases de données. Toutefois, même si vous êtes peu susceptible de frapper la capacité de stockage d'une base de données, une bonne raison pour utiliser plusieurs bases de données, c'est que chaque base de données possède son propre ensemble d'utilisateurs et d'autorisations. Vous pouvez utiliser ce mécanisme pour isoler l'accès aux collections sur une base par base de données.

La figure 8 illustre la structure de haut niveau de l'architecture DocumentDB.

![](media/best-practices-data-partitioning/DocumentDBStructure.png)

_La figure 8. -La structure du DocumentDB_

C'est la responsabilité de l'application cliente pour diriger les requêtes vers le serveur approprié, généralement en mettant en place son propre mécanisme de mappage basé sur des attributs des données qui définissent la touche d'éclat. La figure 9 illustre deux bases de données DocumentDB, chacun contenant deux collections en qualité d'éclats. Les données sont sharded par locataire ID et contient les données pour un locataire précis. Les bases de données sont créées dans des comptes distincts de DocumenDB qui sont trouvent dans la même région que les locataires dont les données qu'ils contiennent. La logique de routage dans l'application cliente utilise l'ID de locataire comme la touche d'éclat.

![](media/best-practices-data-partitioning/DocumentDBPartitions.png)

_La figure 9. -Mise en œuvre sharding à l'aide d'Azur DocumentDB_

Vous devez envisager les points suivants lorsque vous décidez comment partitionner des données avec DocumentDB :

- Les ressources disponibles pour une base de données DocumentDB sont soumis aux limitations de quota du compte DocumentDB. Chaque base de données peut contenir plusieurs collections (encore une fois, il y a une limite) et chaque collection est associée à un niveau de performance qui régit la limite de taux de RU (débit réservé) pour cette collection. Pour plus d'informations, visitez le [Quotas et limites DocumentDB](documentdb-limits.md) page sur le site Web de Microsoft.
- Chaque document doit avoir un attribut qui peut être utilisé pour identifier de façon unique ce document dans la collection qu'il occupe. Ceci est différent de la touche éclat qui définit dans la collection à laquelle le document est maintenu. Une collection peut contenir un grand nombre de documents, en théorie seulement limité par la longueur maximale de l'ID de document. L'ID de document peut comporter jusqu'à 255 caractères.
- Toutes les opérations contre un document sont effectuées dans le cadre d'une transaction dont l'étendue correspond à la collection qui contient le document. Si une opération échoue, le travail qu'elle a réalisée est restaurée.  Lorsqu'un document est soumis à une opération, toutes les modifications apportées sont sous réserve de niveau d'isolement d'instantané. Ce mécanisme garantit que si, par exemple, une requête pour créer un nouveau document échoue, un autre utilisateur en interrogeant la base de données en même temps ne verrez pas un document partiel qui est ensuite retiré.
- DocumentDB requêtes ont également une portée limitées au niveau de la collection. Une seule requête ne peut récupérer que les données d'une collection. Si vous devez récupérer des données provenant de plusieurs collections, vous devez interroger chaque collection individuellement et fusionner les résultats dans le code de votre application.
- DocumentDB supports programmable items that can all be stored in a collection alongside documents: stored procedures, user-defined functions, and triggers (written in JavaScript). These items can access any document within the same collection. Furthermore, these items execute either inside the scope of the ambient transaction (in the case of a trigger that fires as the result of a create, delete, or replace operation performed against a document), or by starting a new transaction (in the case of a stored procedure that is executed as the result of an explicit client request). If the code in a programmable item throws an exception, the transaction is rolled back. You can use stored procedures and triggers to maintain integrity and consistency between documents, but these documents must all be part of the same collection.
- Vous devez vous assurer que les collections que vous avez l'intention d'organiser dans les bases de données dans un compte de DocumentDB sont peu susceptibles de dépasser les limites de débit définies par les niveaux de performance des collections. Ces limites sont décrites sur le [Gérer les besoins de capacité DocumentDB](documentdb-manage.md) page sur le site Web de Microsoft. Si vous prévoyez d'arriver à ces limites, imaginez que l'on fractionne les collections à travers des bases de données dans différents comptes de DocumentDB afin de réduire la charge par collection.

## Stratégies de partitionnement pour recherche d'Azur

La possibilité de rechercher des données est souvent la principale méthode de navigation et d'exploration fournis par de nombreuses applications web, permettant aux utilisateurs de trouver rapidement les ressources (par exemple, les produits dans une application de commerce électronique), basés sur des combinaisons de critères de recherche. Le service de recherche de l'Azure fournit des fonctionnalités de recherche de texte intégral sur le contenu web et inclut des fonctionnalités telles que les requêtes de type-ahead, la suggestion basées sur près de matches et navigation faceted. Une description complète de ces fonctionnalités est disponible sur le [Recherche d'Azur Overview](https://msdn.microsoft.com/library/azure/dn798933.aspx) page sur le site Web de Microsoft.

Le service Search stocke du contenu consultable sous forme de documents JSON dans une base de données. Vous définissez les index qui spécifient les champs interrogeables dans ces documents et de fournissent ces définitions au service de recherche. Lorsqu'un utilisateur soumet une demande de recherche, le service de recherche utilise les index appropriés pour trouver aucun élément correspondant.

Pour réduire la contention, le stockage utilisé par le service de recherche peut être divisé en haut en 1, 2, 3, 4, 6, ou 12 partitions et chaque partition peuvent être répliquées jusqu'à 6 fois. Le produit du nombre de partitions multiplié par le nombre de répliques s'appelle la _Unité de recherche_ (SU). Une seule instance du Service Search peut contenir un maximum de 36 SUs (une base de données avec des 12 partitions prend uniquement en charge un maximum de 3 répliques). Vous sont facturés pour chaque SU attribué est à votre service. Comme le volume des augmentations contenus consultables ou le taux de demandes de recherche se développe, vous pouvez ajouter SUs à une instance existante de la fonction de recherche pour gérer la charge supplémentaire. Le Service de recherche elle-même assume la responsabilité de distribuer les documents de manière homogène à travers les partitions, et aucune stratégie de partitionnement manuel n'est actuellement pris en charge.

Chaque partition peut contenir un maximum de 15 millions de documents ou d'occuper des 300 Go d'espace de stockage (selon ce qui est moins élevée, selon la taille de vos documents et indices). Vous pouvez créer jusqu'à 50 index. La performance du service peut varier selon la complexité des documents, les indices disponibles et les effets de la latence du réseau. En moyenne, un seul réplica (1SU) devrait être capable de gérer 15 requêtes par seconde (QPS), même si vous devez effectuer l'analyse comparative avec vos propres données pour obtenir une mesure plus précise du débit. Pour plus d'informations, consultez le [Limites et contraintes (Azur Search API)]( https://msdn.microsoft.com/library/azure/dn798934.aspx) page sur le site Web de Microsoft.

> [AZURE. REMARQUE] Vous pouvez stocker un ensemble limité de types de données dans des documents consultables ; chaînes, booléens, données numériques, données datetime et certaines données géographiques. Pour plus de détails, voir la [Types de données pris en charge (recherche d'Azur)]( https://msdn.microsoft.com/library/azure/dn798938.aspx) page sur le site Web de Microsoft.

Vous avez un contrôle limité sur comment le service de recherche d'Azur partitionne les données pour chaque instance du service. Toutefois, dans un environnement mondial vous pourrez améliorer les performances et réduire la latence et prétention encore en partitionnant le service lui-même à l'aide d'une des stratégies suivantes :

- Créez une instance du service recherche dans chaque région géographique et faire en sorte que les applications clientes sont orientées vers l'instance disponible la plus proche. Cette stratégie nécessite que les mises à jour de contenu interrogeable sont répliquées en temps opportun dans toutes les instances du service.
- Créer deux-tiers du service de recherche ; un service local dans chaque région qui contient les données plus fréquemment consultées par les utilisateurs dans cette région et un service global qui englobe l'ensemble des données. Utilisateurs peuvent diriger les demandes de service local (pour des résultats rapides mais limités) ou au service global (pour des résultats plus lents mais plus complètes). Cette approche est appropriée lorsqu'il y a une importante variation régionale dans les données recherchées.

## Stratégies de partitionnement pour Azure Redis Cache

Azur Redis Cache fournit un service de mise en cache partagé dans le nuage qui repose sur le magasin de données de clé/valeur Redis. Comme son nom l'indique, Azure Redis Cache est conçu comme une solution de mise en cache et donc ne doit être utilisé pour la tenue de données transitoires plutôt que comme une banque de données permanente ; les applications qui utilisent Azure Redis Cache devraient être en mesure de continuer à fonctionner si le cache n'est pas disponible. Azur Redis Cache prend en charge la réplication primaire/secondaire pour fournir une haute disponibilité, mais limite la taille maximale du cache à 53 Go. Si vous avez besoin de plus d'espace que cela, vous devez créer des caches supplémentaires. Pour plus d'informations, visitez le [Cache de Microsoft Azure](http://azure.microsoft.com/services/cache/) page sur le site Web de Microsoft.

Partitioning a Redis data store involves splitting the data across instances of the Redis service. Each instance constitutes a single partition. Azure Redis Cache abstracts the Redis services behind a façade and does not expose them directly. The simplest way to implement partitioning is to create multiple Azure Redis caches and spread the data across them. You can associate each data item with an identifier (a partition key) that specifies in which cache it should be stored. Your client application logic can use this identifier to route requests to the appropriate partition. This scheme is very simple, but if the partitioning scheme changes (if additional Azure Redis Caches are created, for example), client applications may need to be reconfigured.

Native Redis (not Azure Redis Cache) supports server-side partitioning based on Redis clustering. In this approach, the data is divided evenly across servers by using a hashing mechanism. Each Redis server stores metadata that describes the range of hash keys that the partition holds, and also contains information about which hash keys are located in the partitions on other servers. Client applications simply send requests to any of the participating Redis servers (probably the closest server).The Redis server examines the client request and if it can be resolved locally it will perform the requested operation, otherwise it will forward the request on to the appropriate server. This model is implemented by using Redis clustering, and is described in more detail on the [Redis le tutoriel de cluster](http://redis.io/topics/cluster-tutorial) page sur le site Redis. Redis clustering est transparente pour les applications clientes et serveurs Redis supplémentaires peuvent être ajoutés à la grappe (et les données re-partitionnées) sans nécessiter que vous reconfigurer les clients.

> [AZURE. IMPORTANT] Azur Cache Redis ne soutient pas actuellement Redis clustering. Si vous souhaitez mettre en œuvre cette approche avec Azure vous devez implémenter vos propres serveurs Redis en installant Redis sur un ensemble d'Azur des ordinateurs virtuels et les configurer manuellement. La page [Redis en cours d'exécution sur un ordinateur virtuel au Linux CentOS dans Azure](http://blogs.msdn.com/b/tconte/archive/2012/06/08/running-redis-on-a-centos-linux-vm-in-windows-azure.aspx) sur Microsoft site promenades à travers un exemple montrant comment créer et configurer un nœud Redis fonctionnant comme un VM Azure.

La page [Partitionnement : Comment diviser les données entre plusieurs instances de Redis](http://redis.io/topics/partitioning) sur le Redis site Web fournit de plus amples informations sur l'implémentation du partitionnement avec Redis. Le reste de cette section suppose que vous implémentiez côté client ou assistée par procuration de partitionnement.

Vous devez envisager les points suivants lorsque vous décidez comment partitionner des données avec Azure Redis Cache :

- Azur Redis Cache n'est pas prévu d'agir comme une banque de données permanente, donc tout schéma de partitionnement vous implémentez le code de votre application doit être prêt à accepter que les données ne se trouve pas dans le cache et doit être trouvé d'ailleurs.
- Conserver les données qui sont fréquemment consultées ensemble dans la même partition. Redis est un magasin de puissante clé/valeur qui fournit plusieurs mécanismes hautement optimisée pour la structuration des données, allant de simples chaînes (en fait, des données binaires jusqu'à 512 Mo en longueur) pour les types d'agrégats tels que les listes (qui peuvent agir comme les piles et les files d'attente), les ensembles (ordonné et non ordonné) et hachages (qui peuvent regrouper les domaines connexes, tels que les éléments qui représentent les champs d'un objet). Les types d'agrégats permettent d'associer plusieurs valeurs connexes avec la même clé ; une clé Redis identifie une liste, d'un ensemble, ou un hachage plutôt que les éléments de données qu'il contient. Ces types sont tous disponibles avec Azure Redis Cache et sont décrits par le [Types de données](http://redis.io/topics/data-types) page sur le site Redis. Par exemple, dans la partie d'un système de commerce électronique répertoriant les commandes passées par les clients, les détails de chaque client pourraient être stockés dans une table de hachage Redis à clé à l'aide de l'ID de client. Chaque hachage pourrait contenir une collection d'ID de commande pour le client. Un ensemble distinct de Redis pouvait contenir les ordres, encore structurés comme les hachages, indexés à l'aide de l'ID de commande.  La figure 10 illustre cette structure. Notez que Redis n'implémente pas toute forme d'intégrité référentielle, ainsi en est-il de la responsabilité du développeur de maintenir les relations entre customers et orders.

![](media/best-practices-data-partitioning/RedisCustomersandOrders.png)

_La figure 10. -Structure suggérée dans stockage Redis pour l'enregistrement des commandes des clients et leurs détails_

> [AZURE. REMARQUE] En Redis, toutes les clés sont des valeurs de données binaires (comme Redis cordes) et peuvent contenir jusqu'à 512 Mo de données, donc en théorie une clé peut contenir presque n'importe quelle information. Toutefois, vous devez adopter une convention de nommage cohérente pour les touches qui est descriptif du type de données et qui identifie l'entité, mais ce n'est pas excessivement longs. Une approche courante consiste à utiliser les clés de la forme « entité_type: ID", comme"client : 99"pour indiquer la clé pour le client avec l'ID 99.

- Vous pouvez implémenter un partitionnement vertical en stockant des informations connexes dans différentes agrégations dans la même base de données. Par exemple, dans un ecommerce, application que vous pouvez stocker couramment accessible informations sur les produits dans une table de hachage Redis et des informations détaillées moins fréquemment utilisées dans un autre. Les deux hachages pourraient utiliser le même ID de produit dans le cadre de la clé, par exemple « produit :_nn_"où _nn_ est l'ID de produit pour des informations produit et produit"_Détails : _nn_"pour les données détaillées. Cette stratégie peut aider à réduire le volume de données que la plupart des requêtes sont susceptibles de récupérer.
- Repartitioning a Redis data store is a complex and time-consuming task. Redis clustering can repartition data automatically, but this facility is not available with Azure Redis Cache. Therefore, when you design your partitioning scheme, you should endeavor to leave sufficient free space in each partition to allow for expected data growth over time. However, remember that Azure Redis Cache is intended to cache data temporarily, and that data held in the cache can have a limited lifetime specified as a time-to-live (TTL) value. For relatively volatile data the TTL should be short, but for static data the TTL can be a lot longer. You should avoid storing large amounts of long-lived data in the cache if the volume of this data is likely to fill the cache. You can specify an eviction policy that causes Azure Redis Cache to remove data if space is at a premium.

	> [AZURE. REMARQUE] Azur Redis cache vous permet de spécifier la taille maximale du cache (à partir de 250Mo à 53 Go) permet en sélectionnant le niveau de tarification adapté. Cependant, une fois qu'un Cache de Redis Azure a été créé, vous ne peut pas augmenter (ou diminuer) sa taille.

- Redis lots et les transactions ne couvrent plusieurs connexions, donc toutes les données affectées par une transaction ou un traitement se tiendrait dans la même base (éclat).

	> [AZURE. REMARQUE] Une séquence d'opérations dans une transaction Redis n'est pas nécessairement atomique. Les commandes qui composent une transaction sont vérifiés et en file d'attente avant l'exécution, et si une erreur se produit pendant cette phase la toute file d'attente est ignorée. Cependant, une fois que la transaction a été soumise avec succès, les commandes en file d'attente seront exécutés dans l'ordre. Si n'importe quelle commande échoue uniquement cette commande est abandonnée ; toutes les commandes précédentes et suivantes dans la file d'attente sont effectuées. Si vous devez effectuer les opérations atomiques. Pour plus d'informations, visitez le [Transactions](http://redis.io/topics/transactions) page sur le site Redis.

- Redis prend en charge un nombre limité d'opérations atomiques, et les seules opérations de ce type qui prennent en charge plusieurs clés et les valeurs sont MGET (qui retourne une collection des valeurs d'une liste spécifiée de clés) et EESM (qui peut stocker une collection de valeurs d'une liste spécifiée de clés). Si vous avez besoin d'utiliser ces opérations, les paires clé/valeur référencées par les commandes EESM et MGET doivent être stockés dans la même base de données.

## Rééquilibrage des partitions

Comme un système arrive à maturité et les habitudes d'utilisation devient mieux compris, il est possible qu'il peut être nécessaire d'ajuster le schéma de partitionnement. Cela pourrait être dû à des partitions individuelles, attirant un volume disproportionné du trafic et devenir chaud, conduisant à une prétention excessive. En outre, vous pourriez avoir sous-estimé le volume de données dans certaines partitions, vous causer à approchent des limites de la capacité de stockage dans ces partitions. Quelle que soit la cause, il est parfois nécessaire de rééquilibrer les partitions pour répartir la charge plus uniformément.

Dans certains cas, les systèmes de stockage de données qui n'exposent pas publiquement la façon dans laquelle les données sont allouées aux serveurs peuvent automatiquement rééquilibrer les partitions dans les limites des ressources disponibles. Dans d'autres situations, le rééquilibrage est une tâche administrative qui comporte deux étapes :

1. Détermination de la nouvelle stratégie de partitionnement pour déterminer quelles partitions devrez peut-être être diviser (ou éventuellement combinés) et la répartition des données pour ces nouvelles partitions en concevant de nouvelles clés de partition.
2. Migrer les données concernées dans le schéma de partitionnement vieux vers le nouvel ensemble de partitions.

> [AZURE. REMARQUE] La cartographie des collections DocumentDB aux serveurs est transparente, mais vous pourriez encore atteindre les limites de débit et de capacité de stockage d'un compte de DocumentDB, auquel cas vous devrez peut-être repenser votre schéma de partitionnement et de migrer les données.

Selon la technologie de stockage de données et la conception de votre système de stockage de données, vous pouvez être en mesure de migrer les données entre les partitions alors qu'ils sont en cours d'utilisation (migration en ligne). Si ce n'est pas possible, vous devrez établir les partitions affectées temporairement indisponible, tandis que les données sont déplacée (migration hors ligne).

## Migration en mode hors connexion

Migration en mode hors connexion est sans doute l'approche la plus simple, car elle réduit les chances de contention qui se produisent ; les données en cours de migration ne doivent pas changer pendant qu'il est déplacé et restructuré.

Conceptuellement, ce processus comporte les étapes suivantes :

1. Marquer l'éclat en mode hors connexion,
2. Split/merge et déplacer les données vers les nouveaux éclats,
3. Vérifier les données,
4. Mettre les éclats de nouveau en ligne,
5. Supprimer l'ancien éclat.

Pour conserver une disponibilité, il pourrait être possible de marquer l'éclat original en lecture seule dans étape 1 plutôt que de le rend inaccessible. Cela serait permettent aux applications de lire les données, alors qu'il est déplacé, mais pas le changer.

## Migration en ligne

Migration en ligne est plus complexe à réaliser, mais est moins perturbatrice pour les utilisateurs que les données restent disponibles pendant toute la procédure. Le processus est similaire à celle utilisée par migration hors ligne, sauf que l'éclat original n'est pas marqué en mode hors connexion (étape 1). Selon la granularité du processus migratoire (point par point ou éclat de l'éclat), le code d'accès de données dans les applications clientes peuvent avoir à gérer la lecture et l'écriture de données qui s'est tenue à deux endroits (l'éclat original et le nouvel éclat)

Pour obtenir un exemple d'une solution qui prend en charge la migration en ligne, voir la [Service de Split/Merge pour échelle élastique](sql-database-elastic-scale-overview-split-and-merge.md), documentée en ligne sur le site Web de Microsoft.

## Lignes directrices et modèles liés

Les schémas suivants peuvent également être pertinents pour votre scénario, lors de l'examen des stratégies de mise en œuvre de la cohérence des données :

- La page de conseils de cohérence de données, disponible sur le site Web de Microsoft, présente des stratégies pour maintenir la cohérence dans un environnement distribué comme le nuage.
- Le [Conseils partitionnement de données](https://msdn.microsoft.com/library/dn589795.aspx) page sur le site Web Microsoft donne un aperçu de la conception de partitions pour répondre à différents critères dans une solution distribuée.
- Le [Sharding Pattern](https://msdn.microsoft.com/library/dn589797.aspx), décrites sur le site de Microsoft, résume certaines des stratégies communes pour les données sharding.
- Le [Modèle de Table d'index](https://msdn.microsoft.com/library/dn589791.aspx) décrit sur Microsoft site Web explique comment créer des index secondaires sur les données. Cette approche permet à une application récupérer rapidement les données en utilisant des requêtes qui ne fait référence la clé primaire d'une collection.
- Le [Modèle de vue matérialisée](https://msdn.microsoft.com/library/dn589782.aspx) discute sur Microsoft site décrit comment générer des vues préremplies résumer des données pour prendre en charge les opérations de requête rapide. Cette approche peut être utile dans un magasin de données partitionnées si les partitions contenant les données étant résumées sont réparties sur plusieurs sites.
- L'article Content Delivery Network (CDN) fournit des directives supplémentaires sur la configuration et l'utilisation de CDN avec Azure.

## Plus d'informations

- Le [Base de données SQL Azure](https://msdn.microsoft.com/library/azure/ee336279.aspx) page sur le site Web de Microsoft fournit une documentation détaillée décrivant comment créer et utiliser des bases de données SQL.
- La page [Vue d'ensemble SQL Azure Database échelle élastique](sql-database-elastic-scale-introduction.md) sur Microsoft site Web fournit une introduction complète à l'échelle de l'élastique.
- La rubrique [Fractionnement et fusion avec échelle élastique](sql-database-elastic-scale-overview-split-and-merge.md) sur Microsoft site Web contient des informations sur l'utilisation de la fonction Split/Merge pour gérer les tessons échelle élastique.
- La page [Azure Storage évolutivité et cibles de rendement](https://msdn.microsoft.com/library/azure/dn249410.aspx) sur Microsoft site documente les limites actuelles de dimensionnement et le débit de stockage Azure.
- Le [Exécution des Transactions de l'entité groupe](https://msdn.microsoft.com/library/azure/dd894038.aspx) page sur le site Web de Microsoft fournit des informations détaillées sur l'implémentation des opérations transactionnelles sur entités stockées dans le stockage de table Azure.
- L'article [Conception d'une stratégie de partitionnement évolutive pour le stockage de Table bleu azur](https://msdn.microsoft.com/library/azure/hh508997.aspx) sur Microsoft site Web contient des informations détaillées sur le partitionnement des données en stockage de table Azure.
- La page [À l'aide de CDN pour Azure](cdn-how-to-use.md) sur Microsoft site Web explique comment répliquer des données détenues dans le stockage Blob Azure en utilisant le Content Delivery Network (CDN) des Azure.
- La page [Limites de DocumentDB pour la version Preview](documentdb-limits.md) sur Microsoft site Web décrit les limites de courant et les quotas pour Microsoft DocumentDB.
- La page [Gérer les performances et la capacité de DocumentDB](documentdb-manage.md) sur Microsoft site Web contient des informations sur comment Azure DocumentDB alloue des ressources aux bases de données.
- Le [Recherche d'Azur Overview](https://msdn.microsoft.com/library/azure/dn798933.aspx) page sur le site Web de Microsoft fournit une description complète des fonctions disponibles avec le service de recherche d'Azur.
- Le [Limites et contraintes (Azur Search API)](https://msdn.microsoft.com/library/azure/dn798934.aspx) page sur le site Web de Microsoft contient des informations sur la capacité de chaque instance du service Azure Search.
- Le [Types de données pris en charge (recherche d'Azur)](https://msdn.microsoft.com/library/azure/dn798938.aspx) page sur le site Web Microsoft résume les types de données que vous pouvez utiliser dans des index et des documents consultables.
- Le [Cache de Microsoft Azure](http://azure.microsoft.com/services/cache.md) page sur le site Web de Microsoft fournit une introduction à Azure Redis Cache.
- La page [Partitionnement : Comment diviser les données entre plusieurs instances de Redis](http://redis.io/topics/partitioning) sur le Redis site Web fournit des informations sur l'implémentation du partitionnement avec Redis.
- La page [Redis en cours d'exécution sur un ordinateur virtuel au Linux CentOS dans Azure](http://blogs.msdn.com/b/tconte/archive/2012/06/08/running-redis-on-a-centos-linux-vm-in-windows-azure.aspx) sur Microsoft site promenades à travers un exemple montrant comment créer et configurer un nœud Redis fonctionnant comme un VM Azure.
- Le [Types de données](http://redis.io/topics/data-types) page sur le site Redis décrit les types de données qui sont disponibles avec le Redis et Redis de Azure Cache.
