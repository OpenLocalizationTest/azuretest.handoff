<?xml version="1.0" encoding="utf-8"?>
<xliff version="1.2" xmlns="urn:oasis:names:tc:xliff:document:1.2" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="urn:oasis:names:tc:xliff:document:1.2 xliff-core-1.2-transitional.xsd">
  <file datatype="xml" source-language="en-us" target-language="ja-jp" original="2/20/2016 9:04:53 AM" tool-id="MarkdownTransformer" product-name="N/A" product-version="N/A" build-num="1">
    <header>
      <xliffext:olfilehash xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">69cd26a2b756bc3609649f46ce1705de26c50afd</xliffext:olfilehash>
      <tool tool-id="MarkdownTransformer" tool-name="MarkdownToXliff" tool-version="1.0" tool-company="Microsoft" />
    </header>
    <body>
      <group extype="content">
        <group id="101">
          <trans-unit id="101" xml:space="preserve">
            <source>Submit Hive Queries to Hadoop clusters in the Advanced Analytics Process and Technology | Microsoft Azure</source>
            <target state="new">Submit Hive Queries to Hadoop clusters in the Advanced Analytics Process and Technology | Microsoft Azure</target>
          </trans-unit>
          <trans-unit id="102" xml:space="preserve">
            <source>Process Data from Hive Tables with Hive queries.</source>
            <target state="new">Process Data from Hive Tables with Hive queries.</target>
          </trans-unit>
          <trans-unit id="103" xml:space="preserve">
            <source><bpt id="1">&lt;html&gt;</bpt><ept id="1">&lt;/html&gt;</ept><bpt id="2">&lt;html&gt;</bpt><ept id="2">&lt;/html&gt;</ept> Submit Hive Queries to HDInsight Hadoop clusters in the Advanced Analytics Process and Technology</source>
            <target state="new"><bpt id="1">&lt;html&gt;</bpt><ept id="1">&lt;/html&gt;</ept><bpt id="2">&lt;html&gt;</bpt><ept id="2">&lt;/html&gt;</ept> Submit Hive Queries to HDInsight Hadoop clusters in the Advanced Analytics Process and Technology</target>
          </trans-unit>
          <trans-unit id="104" xml:space="preserve">
            <source>This document describes various ways of submitting Hive queries to Hadoop clusters that are managed by an HDInsight service in Azure.</source>
            <target state="new">This document describes various ways of submitting Hive queries to Hadoop clusters that are managed by an HDInsight service in Azure.</target>
          </trans-unit>
          <trans-unit id="105" xml:space="preserve">
            <source>This task is part of the Advanced Analytics Process and Technology (ADAPT) provided by Azure Machine Learning.</source>
            <target state="new">This task is part of the Advanced Analytics Process and Technology (ADAPT) provided by Azure Machine Learning.</target>
          </trans-unit>
          <trans-unit id="106" xml:space="preserve">
            <source>Several data wrangling tasks are discussed: data exploration and feature generation.</source>
            <target state="new">Several data wrangling tasks are discussed: data exploration and feature generation.</target>
          </trans-unit>
          <trans-unit id="107" xml:space="preserve">
            <source>Generic Hive queries that show how to explore data or generate features using Hive in an Azure HDInsight Hadoop cluster.</source>
            <target state="new">Generic Hive queries that show how to explore data or generate features using Hive in an Azure HDInsight Hadoop cluster.</target>
          </trans-unit>
          <trans-unit id="108" xml:space="preserve">
            <source>These Hive queries use embedded Hive User Defined Functions (UDFs) which are provided.</source>
            <target state="new">These Hive queries use embedded Hive User Defined Functions (UDFs) which are provided.</target>
          </trans-unit>
          <trans-unit id="109" xml:space="preserve">
            <source>Examples of queries that are specific to <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>NYC Taxi Trip Data<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept> scenarios are also provided in <bpt id="4CapsExtId1">&lt;link&gt;</bpt><bpt id="4CapsExtId2">&lt;linkText&gt;</bpt>Github repository<ept id="4CapsExtId2">&lt;/linkText&gt;</ept><bpt id="4CapsExtId3">&lt;title&gt;</bpt><ept id="4CapsExtId3">&lt;/title&gt;</ept><ept id="4CapsExtId1">&lt;/link&gt;</ept>.</source>
            <target state="new">Examples of queries that are specific to <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>NYC Taxi Trip Data<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept> scenarios are also provided in <bpt id="4CapsExtId1">&lt;link&gt;</bpt><bpt id="4CapsExtId2">&lt;linkText&gt;</bpt>Github repository<ept id="4CapsExtId2">&lt;/linkText&gt;</ept><bpt id="4CapsExtId3">&lt;title&gt;</bpt><ept id="4CapsExtId3">&lt;/title&gt;</ept><ept id="4CapsExtId1">&lt;/link&gt;</ept>.</target>
          </trans-unit>
          <trans-unit id="110" xml:space="preserve">
            <source>These queries already have data schema specified and are ready to be submitted to run.</source>
            <target state="new">These queries already have data schema specified and are ready to be submitted to run.</target>
          </trans-unit>
          <trans-unit id="111" xml:space="preserve">
            <source>In the final section, parameters that users can tune so that the performance of Hive queries can be improved are discussed.</source>
            <target state="new">In the final section, parameters that users can tune so that the performance of Hive queries can be improved are discussed.</target>
          </trans-unit>
          <trans-unit id="112" xml:space="preserve">
            <source>Prerequisites</source>
            <target state="new">Prerequisites</target>
          </trans-unit>
          <trans-unit id="113" xml:space="preserve">
            <source>This article assumes that you have:</source>
            <target state="new">This article assumes that you have:</target>
          </trans-unit>
          <trans-unit id="114" xml:space="preserve">
            <source>Created an Azure storage account.</source>
            <target state="new">Created an Azure storage account.</target>
          </trans-unit>
          <trans-unit id="115" xml:space="preserve">
            <source>If you need instructions, see <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>Create an Azure Storage account<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept></source>
            <target state="new">If you need instructions, see <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>Create an Azure Storage account<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept></target>
          </trans-unit>
          <trans-unit id="116" xml:space="preserve">
            <source>Provisioned a customized Hadoop cluster with the HDInsight service.</source>
            <target state="new">Provisioned a customized Hadoop cluster with the HDInsight service.</target>
          </trans-unit>
          <trans-unit id="117" xml:space="preserve">
            <source>If you need instructions, see <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>Customize Azure HDInsight Hadoop Clusters for Advanced Analytics<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept>.</source>
            <target state="new">If you need instructions, see <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>Customize Azure HDInsight Hadoop Clusters for Advanced Analytics<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept>.</target>
          </trans-unit>
          <trans-unit id="118" xml:space="preserve">
            <source>The data has been uploaded to Hive tables in Azure HDInsight Hadoop clusters.</source>
            <target state="new">The data has been uploaded to Hive tables in Azure HDInsight Hadoop clusters.</target>
          </trans-unit>
          <trans-unit id="119" xml:space="preserve">
            <source>If it has not, please follow <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>Create and load data to Hive tables<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept> to upload data to Hive tables first.</source>
            <target state="new">If it has not, please follow <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>Create and load data to Hive tables<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept> to upload data to Hive tables first.</target>
          </trans-unit>
          <trans-unit id="120" xml:space="preserve">
            <source>Enabled remote access to the cluster.</source>
            <target state="new">Enabled remote access to the cluster.</target>
          </trans-unit>
          <trans-unit id="121" xml:space="preserve">
            <source>If you need instructions, see <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>Access the Head Node of Hadoop Cluster<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept>.</source>
            <target state="new">If you need instructions, see <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>Access the Head Node of Hadoop Cluster<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept>.</target>
          </trans-unit>
          <trans-unit id="122" xml:space="preserve">
            <source><bpt id="1">&lt;html&gt;</bpt><ept id="1">&lt;/html&gt;</ept><bpt id="2">&lt;html&gt;</bpt><ept id="2">&lt;/html&gt;</ept>How to Submit Hive Queries</source>
            <target state="new"><bpt id="1">&lt;html&gt;</bpt><ept id="1">&lt;/html&gt;</ept><bpt id="2">&lt;html&gt;</bpt><ept id="2">&lt;/html&gt;</ept>How to Submit Hive Queries</target>
          </trans-unit>
          <trans-unit id="123" xml:space="preserve">
            <source>Hive queries can be submitted by using:</source>
            <target state="new">Hive queries can be submitted by using:</target>
          </trans-unit>
          <trans-unit id="124" xml:space="preserve">
            <source>the Hadoop Command Line on the headnode of the cluster</source>
            <target state="new">the Hadoop Command Line on the headnode of the cluster</target>
          </trans-unit>
          <trans-unit id="125" xml:space="preserve">
            <source>the IPython Notebook</source>
            <target state="new">the IPython Notebook</target>
          </trans-unit>
          <trans-unit id="126" xml:space="preserve">
            <source>the Hive Editor</source>
            <target state="new">the Hive Editor</target>
          </trans-unit>
          <trans-unit id="127" xml:space="preserve">
            <source>Azure PowerShell scripts</source>
            <target state="new">Azure PowerShell scripts</target>
          </trans-unit>
          <trans-unit id="128" xml:space="preserve">
            <source>Hive queries are SQL-like.</source>
            <target state="new">Hive queries are SQL-like.</target>
          </trans-unit>
          <trans-unit id="129" xml:space="preserve">
            <source>Users familiar with SQL may find the <bpt id="2">&lt;html&gt;</bpt><ept id="2">&lt;/html&gt;</ept>SQL-to-Hive Cheat Sheet<bpt id="4">&lt;html&gt;</bpt><ept id="4">&lt;/html&gt;</ept> useful.</source>
            <target state="new">Users familiar with SQL may find the <bpt id="2">&lt;html&gt;</bpt><ept id="2">&lt;/html&gt;</ept>SQL-to-Hive Cheat Sheet<bpt id="4">&lt;html&gt;</bpt><ept id="4">&lt;/html&gt;</ept> useful.</target>
          </trans-unit>
          <trans-unit id="130" xml:space="preserve">
            <source>When submitting a Hive query, you can also control the destination of the output from Hive queries, whether it be on the screen or to a local file on the head node or to an Azure blob.</source>
            <target state="new">When submitting a Hive query, you can also control the destination of the output from Hive queries, whether it be on the screen or to a local file on the head node or to an Azure blob.</target>
          </trans-unit>
          <trans-unit id="131" xml:space="preserve">
            <source>Through Hadoop Command Line console in Head Node of Hadoop Cluster</source>
            <target state="new">Through Hadoop Command Line console in Head Node of Hadoop Cluster</target>
          </trans-unit>
          <trans-unit id="132" xml:space="preserve">
            <source>If the query is complex, submitting Hive queries directly from the head node of the Hadoop cluster typically leads to faster turn around than submitting it with a Hive Editor or by using Azure PowerShell scripts.</source>
            <target state="new">If the query is complex, submitting Hive queries directly from the head node of the Hadoop cluster typically leads to faster turn around than submitting it with a Hive Editor or by using Azure PowerShell scripts.</target>
          </trans-unit>
          <trans-unit id="133" xml:space="preserve">
            <source>Log in to the head node of the Hadoop cluster, open the Hadoop Command Line on the desktop of the head node, and enter command</source>
            <target state="new">Log in to the head node of the Hadoop cluster, open the Hadoop Command Line on the desktop of the head node, and enter command</target>
          </trans-unit>
          <trans-unit id="134" xml:space="preserve">
            <source>Users have three ways to submit Hive queries in Hadoop Command Line console:</source>
            <target state="new">Users have three ways to submit Hive queries in Hadoop Command Line console:</target>
          </trans-unit>
          <trans-unit id="135" xml:space="preserve">
            <source>directly from the Hadoop command line</source>
            <target state="new">directly from the Hadoop command line</target>
          </trans-unit>
          <trans-unit id="136" xml:space="preserve">
            <source>using .hql files</source>
            <target state="new">using .hql files</target>
          </trans-unit>
          <trans-unit id="137" xml:space="preserve">
            <source>from the Hive command console</source>
            <target state="new">from the Hive command console</target>
          </trans-unit>
          <trans-unit id="138" xml:space="preserve">
            <source>Submit Hive queries directly from the Hadoop Command Line</source>
            <target state="new">Submit Hive queries directly from the Hadoop Command Line</target>
          </trans-unit>
          <trans-unit id="139" xml:space="preserve">
            <source>Users can run command like</source>
            <target state="new">Users can run command like</target>
          </trans-unit>
          <trans-unit id="140" xml:space="preserve">
            <source>to submit simple Hive queries directly in the Hadoop command line.</source>
            <target state="new">to submit simple Hive queries directly in the Hadoop command line.</target>
          </trans-unit>
          <trans-unit id="141" xml:space="preserve">
            <source>Here is an example, where the red box outlines the command that submits the Hive query, and the green box outlines the output from the Hive query.</source>
            <target state="new">Here is an example, where the red box outlines the command that submits the Hive query, and the green box outlines the output from the Hive query.</target>
          </trans-unit>
          <trans-unit id="142" xml:space="preserve">
            <source><bpt id="1">&lt;linkText&gt;</bpt>Create workspace<ept id="1">&lt;/linkText&gt;</ept></source>
            <target state="new"><bpt id="1">&lt;linkText&gt;</bpt>Create workspace<ept id="1">&lt;/linkText&gt;</ept></target>
          </trans-unit>
          <trans-unit id="143" xml:space="preserve">
            <source>Submit Hive queries in .hql files</source>
            <target state="new">Submit Hive queries in .hql files</target>
          </trans-unit>
          <trans-unit id="144" xml:space="preserve">
            <source>When the Hive query is more complicated and has multiple lines, editing queries in Hadoop command line or Hive command console is not practical.</source>
            <target state="new">When the Hive query is more complicated and has multiple lines, editing queries in Hadoop command line or Hive command console is not practical.</target>
          </trans-unit>
          <trans-unit id="145" xml:space="preserve">
            <source>An alternative is to use a text editor in the head node of the Hadoop cluster and to save the Hive queries in a .hql file in a local directory of the head node.</source>
            <target state="new">An alternative is to use a text editor in the head node of the Hadoop cluster and to save the Hive queries in a .hql file in a local directory of the head node.</target>
          </trans-unit>
          <trans-unit id="146" xml:space="preserve">
            <source>Then the Hive query in the .hql file can be submitted by using the <bpt id="2">&lt;code&gt;</bpt>-f<ept id="2">&lt;/code&gt;</ept> argument in the <bpt id="4">&lt;code&gt;</bpt>hive<ept id="4">&lt;/code&gt;</ept> command as follows:</source>
            <target state="new">Then the Hive query in the .hql file can be submitted by using the <bpt id="2">&lt;code&gt;</bpt>-f<ept id="2">&lt;/code&gt;</ept> argument in the <bpt id="4">&lt;code&gt;</bpt>hive<ept id="4">&lt;/code&gt;</ept> command as follows:</target>
          </trans-unit>
          <trans-unit id="147" xml:space="preserve">
            <source>Suppress progress status screen print of Hive queries</source>
            <target state="new">Suppress progress status screen print of Hive queries</target>
          </trans-unit>
          <trans-unit id="148" xml:space="preserve">
            <source>By default, after Hive query is submitted in the Hadoop Command Line console, the progress of the Map/Reduce job will be printed out on screen.</source>
            <target state="new">By default, after Hive query is submitted in the Hadoop Command Line console, the progress of the Map/Reduce job will be printed out on screen.</target>
          </trans-unit>
          <trans-unit id="149" xml:space="preserve">
            <source>To suppress the screen print of the Map/Reduce job progress, you can use the argument <bpt id="2">&lt;code&gt;</bpt>-S<ept id="2">&lt;/code&gt;</ept> (case-sensitive) argument in the command line as follows:</source>
            <target state="new">To suppress the screen print of the Map/Reduce job progress, you can use the argument <bpt id="2">&lt;code&gt;</bpt>-S<ept id="2">&lt;/code&gt;</ept> (case-sensitive) argument in the command line as follows:</target>
          </trans-unit>
          <trans-unit id="150" xml:space="preserve">
            <source>Submit Hive queries in Hive command console.</source>
            <target state="new">Submit Hive queries in Hive command console.</target>
          </trans-unit>
          <trans-unit id="151" xml:space="preserve">
            <source>Users can also enter the Hive command console by running the  <bpt id="2">&lt;code&gt;</bpt>hive<ept id="2">&lt;/code&gt;</ept> command from the Hadoop command line, and then submit Hive queries from Hive command console at the <bpt id="4">&lt;strong&gt;</bpt>hive&gt;<ept id="4">&lt;/strong&gt;</ept> prompt.</source>
            <target state="new">Users can also enter the Hive command console by running the  <bpt id="2">&lt;code&gt;</bpt>hive<ept id="2">&lt;/code&gt;</ept> command from the Hadoop command line, and then submit Hive queries from Hive command console at the <bpt id="4">&lt;strong&gt;</bpt>hive&gt;<ept id="4">&lt;/strong&gt;</ept> prompt.</target>
          </trans-unit>
          <trans-unit id="152" xml:space="preserve">
            <source>Here is an example.</source>
            <target state="new">Here is an example.</target>
          </trans-unit>
          <trans-unit id="153" xml:space="preserve">
            <source><bpt id="1">&lt;linkText&gt;</bpt>Create workspace<ept id="1">&lt;/linkText&gt;</ept></source>
            <target state="new"><bpt id="1">&lt;linkText&gt;</bpt>Create workspace<ept id="1">&lt;/linkText&gt;</ept></target>
          </trans-unit>
          <trans-unit id="154" xml:space="preserve">
            <source>In this example, the two red boxes highlight the commands used to enter the Hive command console, and the Hive query submitted in Hive command console, respectively.</source>
            <target state="new">In this example, the two red boxes highlight the commands used to enter the Hive command console, and the Hive query submitted in Hive command console, respectively.</target>
          </trans-unit>
          <trans-unit id="155" xml:space="preserve">
            <source>The green box highlights the output from the Hive query.</source>
            <target state="new">The green box highlights the output from the Hive query.</target>
          </trans-unit>
          <trans-unit id="156" xml:space="preserve">
            <source>The previous examples directly output the Hive query results on screen.</source>
            <target state="new">The previous examples directly output the Hive query results on screen.</target>
          </trans-unit>
          <trans-unit id="157" xml:space="preserve">
            <source>Users can also write the output to a local file on the head node, or to an Azure blob.</source>
            <target state="new">Users can also write the output to a local file on the head node, or to an Azure blob.</target>
          </trans-unit>
          <trans-unit id="158" xml:space="preserve">
            <source>Then, users can use other tools to further analyze the output of from Hive queries.</source>
            <target state="new">Then, users can use other tools to further analyze the output of from Hive queries.</target>
          </trans-unit>
          <trans-unit id="159" xml:space="preserve">
            <source>Output Hive query results to a local file.</source>
            <target state="new">Output Hive query results to a local file.</target>
          </trans-unit>
          <trans-unit id="160" xml:space="preserve">
            <source>To output Hive query results to a local directory on the head node, users have to submit the Hive query in the Hadoop Command Line as follows:</source>
            <target state="new">To output Hive query results to a local directory on the head node, users have to submit the Hive query in the Hadoop Command Line as follows:</target>
          </trans-unit>
          <trans-unit id="161" xml:space="preserve">
            <source>Output Hive query results to an Azure blob</source>
            <target state="new">Output Hive query results to an Azure blob</target>
          </trans-unit>
          <trans-unit id="162" xml:space="preserve">
            <source>Users can also output the Hive query results to an Azure blob, within the default container of the Hadoop cluster.</source>
            <target state="new">Users can also output the Hive query results to an Azure blob, within the default container of the Hadoop cluster.</target>
          </trans-unit>
          <trans-unit id="163" xml:space="preserve">
            <source>The Hive query to do this looks like this:</source>
            <target state="new">The Hive query to do this looks like this:</target>
          </trans-unit>
          <trans-unit id="164" xml:space="preserve">
            <source>In the following example, the output of Hive query is written to a blob directory <bpt id="2">&lt;code&gt;</bpt>queryoutputdir<ept id="2">&lt;/code&gt;</ept> within the default container of the Hadoop cluster.</source>
            <target state="new">In the following example, the output of Hive query is written to a blob directory <bpt id="2">&lt;code&gt;</bpt>queryoutputdir<ept id="2">&lt;/code&gt;</ept> within the default container of the Hadoop cluster.</target>
          </trans-unit>
          <trans-unit id="165" xml:space="preserve">
            <source>Here, you must only provide the directory name, without the blob name.</source>
            <target state="new">Here, you must only provide the directory name, without the blob name.</target>
          </trans-unit>
          <trans-unit id="166" xml:space="preserve">
            <source>An error will be thrown out if you provide both the directory and the blob name, such as <bpt id="2">&lt;em&gt;</bpt>wasb:///queryoutputdir/queryoutput.txt<ept id="2">&lt;/em&gt;</ept>.</source>
            <target state="new">An error will be thrown out if you provide both the directory and the blob name, such as <bpt id="2">&lt;em&gt;</bpt>wasb:///queryoutputdir/queryoutput.txt<ept id="2">&lt;/em&gt;</ept>.</target>
          </trans-unit>
          <trans-unit id="167" xml:space="preserve">
            <source><bpt id="1">&lt;linkText&gt;</bpt>Create workspace<ept id="1">&lt;/linkText&gt;</ept></source>
            <target state="new"><bpt id="1">&lt;linkText&gt;</bpt>Create workspace<ept id="1">&lt;/linkText&gt;</ept></target>
          </trans-unit>
          <trans-unit id="168" xml:space="preserve">
            <source>The output of the Hive query can be seen in blob storage by opening the default container of the Hadoop cluster using the Azure Storage Explorer (or equivalent) tool.</source>
            <target state="new">The output of the Hive query can be seen in blob storage by opening the default container of the Hadoop cluster using the Azure Storage Explorer (or equivalent) tool.</target>
          </trans-unit>
          <trans-unit id="169" xml:space="preserve">
            <source>You can apply the filter (highlighted by red box) if you only want to retrieve a blob with specified letters in names.</source>
            <target state="new">You can apply the filter (highlighted by red box) if you only want to retrieve a blob with specified letters in names.</target>
          </trans-unit>
          <trans-unit id="170" xml:space="preserve">
            <source><bpt id="1">&lt;linkText&gt;</bpt>Create workspace<ept id="1">&lt;/linkText&gt;</ept></source>
            <target state="new"><bpt id="1">&lt;linkText&gt;</bpt>Create workspace<ept id="1">&lt;/linkText&gt;</ept></target>
          </trans-unit>
          <trans-unit id="171" xml:space="preserve">
            <source>Through Hive Editor or Azure PowerShell Commands</source>
            <target state="new">Through Hive Editor or Azure PowerShell Commands</target>
          </trans-unit>
          <trans-unit id="172" xml:space="preserve">
            <source>Users can also use the Query Console (Hive Editor) by entering the URL of the form</source>
            <target state="new">Users can also use the Query Console (Hive Editor) by entering the URL of the form</target>
          </trans-unit>
          <trans-unit id="173" xml:space="preserve">
            <source><bpt id="1">&lt;em&gt;</bpt>https://&amp;#60;Hadoop cluster name&gt;.azurehdinsight.net/Home/HiveEditor<ept id="1">&lt;/em&gt;</ept></source>
            <target state="new"><bpt id="1">&lt;em&gt;</bpt>https://&amp;#60;Hadoop cluster name&gt;.azurehdinsight.net/Home/HiveEditor<ept id="1">&lt;/em&gt;</ept></target>
          </trans-unit>
          <trans-unit id="174" xml:space="preserve">
            <source>into a web browser.</source>
            <target state="new">into a web browser.</target>
          </trans-unit>
          <trans-unit id="175" xml:space="preserve">
            <source>Note that you will be asked to input the Hadoop cluster credentials to log in.</source>
            <target state="new">Note that you will be asked to input the Hadoop cluster credentials to log in.</target>
          </trans-unit>
          <trans-unit id="176" xml:space="preserve">
            <source>Alternatively, you can <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>Submit Hive jobs using PowerShell<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept>.</source>
            <target state="new">Alternatively, you can <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>Submit Hive jobs using PowerShell<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept>.</target>
          </trans-unit>
          <trans-unit id="177" xml:space="preserve">
            <source><bpt id="1">&lt;html&gt;</bpt><ept id="1">&lt;/html&gt;</ept><bpt id="2">&lt;html&gt;</bpt><ept id="2">&lt;/html&gt;</ept>Data Exploration</source>
            <target state="new"><bpt id="1">&lt;html&gt;</bpt><ept id="1">&lt;/html&gt;</ept><bpt id="2">&lt;html&gt;</bpt><ept id="2">&lt;/html&gt;</ept>Data Exploration</target>
          </trans-unit>
          <trans-unit id="178" xml:space="preserve">
            <source>Here are a few sample Hive scripts that can be used to explore data in Hive tables.</source>
            <target state="new">Here are a few sample Hive scripts that can be used to explore data in Hive tables.</target>
          </trans-unit>
          <trans-unit id="179" xml:space="preserve">
            <source>Get the count of observations per partition</source>
            <target state="new">Get the count of observations per partition</target>
          </trans-unit>
          <trans-unit id="180" xml:space="preserve">
            <source><bpt id="1">&lt;code&gt;</bpt>SELECT &lt;partitionfieldname&gt;, count(*) from &lt;databasename&gt;.&lt;tablename&gt; group by &lt;partitionfieldname&gt;;<ept id="1">&lt;/code&gt;</ept></source>
            <target state="new"><bpt id="1">&lt;code&gt;</bpt>SELECT &lt;partitionfieldname&gt;, count(*) from &lt;databasename&gt;.&lt;tablename&gt; group by &lt;partitionfieldname&gt;;<ept id="1">&lt;/code&gt;</ept></target>
          </trans-unit>
          <trans-unit id="181" xml:space="preserve">
            <source>Get the count of observations per day</source>
            <target state="new">Get the count of observations per day</target>
          </trans-unit>
          <trans-unit id="182" xml:space="preserve">
            <source><bpt id="1">&lt;code&gt;</bpt>SELECT to_date(&lt;date_columnname&gt;), count(*) from &lt;databasename&gt;.&lt;tablename&gt; group by to_date(&lt;date_columnname&gt;);<ept id="1">&lt;/code&gt;</ept></source>
            <target state="new"><bpt id="1">&lt;code&gt;</bpt>SELECT to_date(&lt;date_columnname&gt;), count(*) from &lt;databasename&gt;.&lt;tablename&gt; group by to_date(&lt;date_columnname&gt;);<ept id="1">&lt;/code&gt;</ept></target>
          </trans-unit>
          <trans-unit id="183" xml:space="preserve">
            <source>Get the levels in a categorical column</source>
            <target state="new">Get the levels in a categorical column</target>
          </trans-unit>
          <trans-unit id="184" xml:space="preserve">
            <source><bpt id="1">&lt;code&gt;</bpt>SELECT  distinct &lt;column_name&gt; from &lt;databasename&gt;.&lt;tablename&gt;<ept id="1">&lt;/code&gt;</ept></source>
            <target state="new"><bpt id="1">&lt;code&gt;</bpt>SELECT  distinct &lt;column_name&gt; from &lt;databasename&gt;.&lt;tablename&gt;<ept id="1">&lt;/code&gt;</ept></target>
          </trans-unit>
          <trans-unit id="185" xml:space="preserve">
            <source>Get the number of levels in combination of two categorical columns</source>
            <target state="new">Get the number of levels in combination of two categorical columns</target>
          </trans-unit>
          <trans-unit id="186" xml:space="preserve">
            <source><bpt id="1">&lt;code&gt;</bpt>SELECT &lt;column_a&gt;, &lt;column_b&gt;, count(*) from &lt;databasename&gt;.&lt;tablename&gt; group by &lt;column_a&gt;, &lt;column_b&gt;<ept id="1">&lt;/code&gt;</ept></source>
            <target state="new"><bpt id="1">&lt;code&gt;</bpt>SELECT &lt;column_a&gt;, &lt;column_b&gt;, count(*) from &lt;databasename&gt;.&lt;tablename&gt; group by &lt;column_a&gt;, &lt;column_b&gt;<ept id="1">&lt;/code&gt;</ept></target>
          </trans-unit>
          <trans-unit id="187" xml:space="preserve">
            <source>Get the distribution for numerical columns</source>
            <target state="new">Get the distribution for numerical columns</target>
          </trans-unit>
          <trans-unit id="188" xml:space="preserve">
            <source><bpt id="1">&lt;code&gt;</bpt>SELECT &lt;column_name&gt;, count(*) from &lt;databasename&gt;.&lt;tablename&gt; group by &lt;column_name&gt;<ept id="1">&lt;/code&gt;</ept></source>
            <target state="new"><bpt id="1">&lt;code&gt;</bpt>SELECT &lt;column_name&gt;, count(*) from &lt;databasename&gt;.&lt;tablename&gt; group by &lt;column_name&gt;<ept id="1">&lt;/code&gt;</ept></target>
          </trans-unit>
          <trans-unit id="189" xml:space="preserve">
            <source>Extract records from joining two tables</source>
            <target state="new">Extract records from joining two tables</target>
          </trans-unit>
          <trans-unit id="190" xml:space="preserve">
            <source><bpt id="1">&lt;html&gt;</bpt><ept id="1">&lt;/html&gt;</ept><bpt id="2">&lt;html&gt;</bpt><ept id="2">&lt;/html&gt;</ept>Feature Generation</source>
            <target state="new"><bpt id="1">&lt;html&gt;</bpt><ept id="1">&lt;/html&gt;</ept><bpt id="2">&lt;html&gt;</bpt><ept id="2">&lt;/html&gt;</ept>Feature Generation</target>
          </trans-unit>
          <trans-unit id="191" xml:space="preserve">
            <source>In this section, we describe ways of generating features using Hive queries.</source>
            <target state="new">In this section, we describe ways of generating features using Hive queries.</target>
          </trans-unit>
          <trans-unit id="192" xml:space="preserve">
            <source>The sample Hive queries in this section assumes that the data has been uploaded to Hive tables in Azure HDInsight Hadoop clusters.</source>
            <target state="new">The sample Hive queries in this section assumes that the data has been uploaded to Hive tables in Azure HDInsight Hadoop clusters.</target>
          </trans-unit>
          <trans-unit id="193" xml:space="preserve">
            <source>If it has not, please follow <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>Create and load data to Hive tables<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept> to upload data to Hive tables first.</source>
            <target state="new">If it has not, please follow <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>Create and load data to Hive tables<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept> to upload data to Hive tables first.</target>
          </trans-unit>
          <trans-unit id="194" xml:space="preserve">
            <source>Once you have generated additional features, you can either add them as columns to the existing table or create a new table with the additional features and primary key, which can then be joined with the original table.</source>
            <target state="new">Once you have generated additional features, you can either add them as columns to the existing table or create a new table with the additional features and primary key, which can then be joined with the original table.</target>
          </trans-unit>
          <trans-unit id="195" xml:space="preserve">
            <source><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>Frequency based Feature Generation<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept></source>
            <target state="new"><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>Frequency based Feature Generation<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept></target>
          </trans-unit>
          <trans-unit id="196" xml:space="preserve">
            <source><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>Risks of Categorical Variables in Binary Classification<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept></source>
            <target state="new"><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>Risks of Categorical Variables in Binary Classification<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept></target>
          </trans-unit>
          <trans-unit id="197" xml:space="preserve">
            <source><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>Extract features from Datetime Field<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept></source>
            <target state="new"><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>Extract features from Datetime Field<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept></target>
          </trans-unit>
          <trans-unit id="198" xml:space="preserve">
            <source><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>Extract features from Text Field<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept></source>
            <target state="new"><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>Extract features from Text Field<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept></target>
          </trans-unit>
          <trans-unit id="199" xml:space="preserve">
            <source><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>Calculate distance between GPS coordinates<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept></source>
            <target state="new"><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>Calculate distance between GPS coordinates<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept></target>
          </trans-unit>
          <trans-unit id="200" xml:space="preserve">
            <source><bpt id="1">&lt;html&gt;</bpt><ept id="1">&lt;/html&gt;</ept><bpt id="2">&lt;html&gt;</bpt><ept id="2">&lt;/html&gt;</ept>Frequency based Feature Generation</source>
            <target state="new"><bpt id="1">&lt;html&gt;</bpt><ept id="1">&lt;/html&gt;</ept><bpt id="2">&lt;html&gt;</bpt><ept id="2">&lt;/html&gt;</ept>Frequency based Feature Generation</target>
          </trans-unit>
          <trans-unit id="201" xml:space="preserve">
            <source>It is often useful to calculate the frequencies of the levels of a categorical variable, or the frequencies of certain combinations of levels from multiple categorical variables.</source>
            <target state="new">It is often useful to calculate the frequencies of the levels of a categorical variable, or the frequencies of certain combinations of levels from multiple categorical variables.</target>
          </trans-unit>
          <trans-unit id="202" xml:space="preserve">
            <source>Users can use the following script to calculate these frequencies:</source>
            <target state="new">Users can use the following script to calculate these frequencies:</target>
          </trans-unit>
          <trans-unit id="203" xml:space="preserve">
            <source><bpt id="1">&lt;html&gt;</bpt><ept id="1">&lt;/html&gt;</ept><bpt id="2">&lt;html&gt;</bpt><ept id="2">&lt;/html&gt;</ept>Risks of Categorical Variables in Binary Classification</source>
            <target state="new"><bpt id="1">&lt;html&gt;</bpt><ept id="1">&lt;/html&gt;</ept><bpt id="2">&lt;html&gt;</bpt><ept id="2">&lt;/html&gt;</ept>Risks of Categorical Variables in Binary Classification</target>
          </trans-unit>
          <trans-unit id="204" xml:space="preserve">
            <source>In binary classification, we need to convert non-numeric categorical variables into numeric features when the models being used only take numeric features.</source>
            <target state="new">In binary classification, we need to convert non-numeric categorical variables into numeric features when the models being used only take numeric features.</target>
          </trans-unit>
          <trans-unit id="205" xml:space="preserve">
            <source>This is done by replacing each non-numeric level with a numeric risk.</source>
            <target state="new">This is done by replacing each non-numeric level with a numeric risk.</target>
          </trans-unit>
          <trans-unit id="206" xml:space="preserve">
            <source>In this section, we show some generic Hive queries that calculate the risk values (log odds) of a categorical variable.</source>
            <target state="new">In this section, we show some generic Hive queries that calculate the risk values (log odds) of a categorical variable.</target>
          </trans-unit>
          <trans-unit id="207" xml:space="preserve">
            <source>In this example, variables <bpt id="2">&lt;code&gt;</bpt>smooth_param1<ept id="2">&lt;/code&gt;</ept> and <bpt id="4">&lt;code&gt;</bpt>smooth_param2<ept id="4">&lt;/code&gt;</ept> are set to smooth the risk values calculated from the data.</source>
            <target state="new">In this example, variables <bpt id="2">&lt;code&gt;</bpt>smooth_param1<ept id="2">&lt;/code&gt;</ept> and <bpt id="4">&lt;code&gt;</bpt>smooth_param2<ept id="4">&lt;/code&gt;</ept> are set to smooth the risk values calculated from the data.</target>
          </trans-unit>
          <trans-unit id="208" xml:space="preserve">
            <source>Risks have a range between -Inf and Inf.</source>
            <target state="new">Risks have a range between -Inf and Inf.</target>
          </trans-unit>
          <trans-unit id="209" xml:space="preserve">
            <source>A risks &gt; 0 indicates that the probability that the target is equal to 1 is greater than 0.5.</source>
            <target state="new">A risks &gt; 0 indicates that the probability that the target is equal to 1 is greater than 0.5.</target>
          </trans-unit>
          <trans-unit id="210" xml:space="preserve">
            <source>After the risk table is calculated, users can assign risk values to a table by joining it with the risk table.</source>
            <target state="new">After the risk table is calculated, users can assign risk values to a table by joining it with the risk table.</target>
          </trans-unit>
          <trans-unit id="211" xml:space="preserve">
            <source>The Hive joining query was provided in previous section.</source>
            <target state="new">The Hive joining query was provided in previous section.</target>
          </trans-unit>
          <trans-unit id="212" xml:space="preserve">
            <source><bpt id="1">&lt;html&gt;</bpt><ept id="1">&lt;/html&gt;</ept><bpt id="2">&lt;html&gt;</bpt><ept id="2">&lt;/html&gt;</ept>Extract features from Datetime Fields</source>
            <target state="new"><bpt id="1">&lt;html&gt;</bpt><ept id="1">&lt;/html&gt;</ept><bpt id="2">&lt;html&gt;</bpt><ept id="2">&lt;/html&gt;</ept>Extract features from Datetime Fields</target>
          </trans-unit>
          <trans-unit id="213" xml:space="preserve">
            <source>Hive comes with a set of UDFs for processing datetime fields.</source>
            <target state="new">Hive comes with a set of UDFs for processing datetime fields.</target>
          </trans-unit>
          <trans-unit id="214" xml:space="preserve">
            <source>In Hive, the default datetime format is 'yyyy-MM-dd 00:00:00' ('1970-01-01 12:21:32' for example).</source>
            <target state="new">In Hive, the default datetime format is 'yyyy-MM-dd 00:00:00' ('1970-01-01 12:21:32' for example).</target>
          </trans-unit>
          <trans-unit id="215" xml:space="preserve">
            <source>In this section, we show examples that extract the day of a month, the month from a datetime field, and other examples that convert a datetime string in a format other than the default format to a datetime string in default format.</source>
            <target state="new">In this section, we show examples that extract the day of a month, the month from a datetime field, and other examples that convert a datetime string in a format other than the default format to a datetime string in default format.</target>
          </trans-unit>
          <trans-unit id="216" xml:space="preserve">
            <source>This Hive query assumes that the <bpt id="2">&lt;em&gt;</bpt>&amp;#60;datetime field&gt;<ept id="2">&lt;/em&gt;</ept> is in the default datetime format.</source>
            <target state="new">This Hive query assumes that the <bpt id="2">&lt;em&gt;</bpt>&amp;#60;datetime field&gt;<ept id="2">&lt;/em&gt;</ept> is in the default datetime format.</target>
          </trans-unit>
          <trans-unit id="217" xml:space="preserve">
            <source>If a datetime field is not in the default format, you need to convert the datetime field into Unix time stamp first, and then convert the Unix time stamp to a datetime string that is in the default format.</source>
            <target state="new">If a datetime field is not in the default format, you need to convert the datetime field into Unix time stamp first, and then convert the Unix time stamp to a datetime string that is in the default format.</target>
          </trans-unit>
          <trans-unit id="218" xml:space="preserve">
            <source>When the datetime is in default format, users can apply the embedded datetime UDFs to extract features.</source>
            <target state="new">When the datetime is in default format, users can apply the embedded datetime UDFs to extract features.</target>
          </trans-unit>
          <trans-unit id="219" xml:space="preserve">
            <source>In this query, if the <bpt id="2">&lt;em&gt;</bpt>&amp;#60;datetime field&gt;<ept id="2">&lt;/em&gt;</ept> has the pattern like <bpt id="4">&lt;em&gt;</bpt>03/26/2015 12:04:39<ept id="4">&lt;/em&gt;</ept>, the <bpt id="6">&lt;em&gt;</bpt>'&amp;#60;pattern of the datetime field&gt;'<ept id="6">&lt;/em&gt;</ept> should be <bpt id="8">&lt;code&gt;</bpt>'MM/dd/yyyy HH:mm:ss'<ept id="8">&lt;/code&gt;</ept>.</source>
            <target state="new">In this query, if the <bpt id="2">&lt;em&gt;</bpt>&amp;#60;datetime field&gt;<ept id="2">&lt;/em&gt;</ept> has the pattern like <bpt id="4">&lt;em&gt;</bpt>03/26/2015 12:04:39<ept id="4">&lt;/em&gt;</ept>, the <bpt id="6">&lt;em&gt;</bpt>'&amp;#60;pattern of the datetime field&gt;'<ept id="6">&lt;/em&gt;</ept> should be <bpt id="8">&lt;code&gt;</bpt>'MM/dd/yyyy HH:mm:ss'<ept id="8">&lt;/code&gt;</ept>.</target>
          </trans-unit>
          <trans-unit id="220" xml:space="preserve">
            <source>To test it, users can run</source>
            <target state="new">To test it, users can run</target>
          </trans-unit>
          <trans-unit id="221" xml:space="preserve">
            <source>The <bpt id="2">&lt;em&gt;</bpt>hivesampletable<ept id="2">&lt;/em&gt;</ept> in this query comes preinstalled on all Azure HDInsight Hadoop clusters by default when the clusters are provisioned.</source>
            <target state="new">The <bpt id="2">&lt;em&gt;</bpt>hivesampletable<ept id="2">&lt;/em&gt;</ept> in this query comes preinstalled on all Azure HDInsight Hadoop clusters by default when the clusters are provisioned.</target>
          </trans-unit>
          <trans-unit id="222" xml:space="preserve">
            <source><bpt id="1">&lt;html&gt;</bpt><ept id="1">&lt;/html&gt;</ept><bpt id="2">&lt;html&gt;</bpt><ept id="2">&lt;/html&gt;</ept>Extract features from Text Fields</source>
            <target state="new"><bpt id="1">&lt;html&gt;</bpt><ept id="1">&lt;/html&gt;</ept><bpt id="2">&lt;html&gt;</bpt><ept id="2">&lt;/html&gt;</ept>Extract features from Text Fields</target>
          </trans-unit>
          <trans-unit id="223" xml:space="preserve">
            <source>When the Hive table has a text field that contains a string of words that are delimited by spaces, the following query extracts the length of the string, and the number of words in the string.</source>
            <target state="new">When the Hive table has a text field that contains a string of words that are delimited by spaces, the following query extracts the length of the string, and the number of words in the string.</target>
          </trans-unit>
          <trans-unit id="224" xml:space="preserve">
            <source><bpt id="1">&lt;html&gt;</bpt><ept id="1">&lt;/html&gt;</ept><bpt id="2">&lt;html&gt;</bpt><ept id="2">&lt;/html&gt;</ept>Calculate distances between sets of GPS coordinates</source>
            <target state="new"><bpt id="1">&lt;html&gt;</bpt><ept id="1">&lt;/html&gt;</ept><bpt id="2">&lt;html&gt;</bpt><ept id="2">&lt;/html&gt;</ept>Calculate distances between sets of GPS coordinates</target>
          </trans-unit>
          <trans-unit id="225" xml:space="preserve">
            <source>The query given in this section can be directly applied to the NYC Taxi Trip Data.</source>
            <target state="new">The query given in this section can be directly applied to the NYC Taxi Trip Data.</target>
          </trans-unit>
          <trans-unit id="226" xml:space="preserve">
            <source>The purpose of this query is to show how to apply an embedded mathematical functions in Hive to generate features.</source>
            <target state="new">The purpose of this query is to show how to apply an embedded mathematical functions in Hive to generate features.</target>
          </trans-unit>
          <trans-unit id="227" xml:space="preserve">
            <source>The fields that are used in this query are the GPS coordinates of pickup and dropoff locations, named <bpt id="2">&lt;em&gt;</bpt>pickup\_longitude<ept id="2">&lt;/em&gt;</ept>, <bpt id="4">&lt;em&gt;</bpt>pickup\_latitude<ept id="4">&lt;/em&gt;</ept>, <bpt id="6">&lt;em&gt;</bpt>dropoff\_longitude<ept id="6">&lt;/em&gt;</ept>, and <bpt id="8">&lt;em&gt;</bpt>dropoff\_latitude<ept id="8">&lt;/em&gt;</ept>.</source>
            <target state="new">The fields that are used in this query are the GPS coordinates of pickup and dropoff locations, named <bpt id="2">&lt;em&gt;</bpt>pickup\_longitude<ept id="2">&lt;/em&gt;</ept>, <bpt id="4">&lt;em&gt;</bpt>pickup\_latitude<ept id="4">&lt;/em&gt;</ept>, <bpt id="6">&lt;em&gt;</bpt>dropoff\_longitude<ept id="6">&lt;/em&gt;</ept>, and <bpt id="8">&lt;em&gt;</bpt>dropoff\_latitude<ept id="8">&lt;/em&gt;</ept>.</target>
          </trans-unit>
          <trans-unit id="228" xml:space="preserve">
            <source>The queries that calculate the direct distance between the pickup and dropoff coordinates are:</source>
            <target state="new">The queries that calculate the direct distance between the pickup and dropoff coordinates are:</target>
          </trans-unit>
          <trans-unit id="229" xml:space="preserve">
            <source>The mathematical equations that calculate the distance between two GPS coordinates can be found on the <bpt id="2">&lt;html&gt;</bpt><ept id="2">&lt;/html&gt;</ept>Movable Type Scripts<bpt id="4">&lt;html&gt;</bpt><ept id="4">&lt;/html&gt;</ept> site, authored by Peter Lapisu.</source>
            <target state="new">The mathematical equations that calculate the distance between two GPS coordinates can be found on the <bpt id="2">&lt;html&gt;</bpt><ept id="2">&lt;/html&gt;</ept>Movable Type Scripts<bpt id="4">&lt;html&gt;</bpt><ept id="4">&lt;/html&gt;</ept> site, authored by Peter Lapisu.</target>
          </trans-unit>
          <trans-unit id="230" xml:space="preserve">
            <source>In his Javascript, the function <bpt id="2">&lt;code&gt;</bpt>toRad()<ept id="2">&lt;/code&gt;</ept> is just <bpt id="4">&lt;em&gt;</bpt>lat_or_lon<ept id="4">&lt;/em&gt;</ept>pi/180*, which converts degrees to radians.</source>
            <target state="new">In his Javascript, the function <bpt id="2">&lt;code&gt;</bpt>toRad()<ept id="2">&lt;/code&gt;</ept> is just <bpt id="4">&lt;em&gt;</bpt>lat_or_lon<ept id="4">&lt;/em&gt;</ept>pi/180*, which converts degrees to radians.</target>
          </trans-unit>
          <trans-unit id="231" xml:space="preserve">
            <source>Here, <bpt id="2">&lt;em&gt;</bpt>lat_or_lon<ept id="2">&lt;/em&gt;</ept> is the latitude or longitude.</source>
            <target state="new">Here, <bpt id="2">&lt;em&gt;</bpt>lat_or_lon<ept id="2">&lt;/em&gt;</ept> is the latitude or longitude.</target>
          </trans-unit>
          <trans-unit id="232" xml:space="preserve">
            <source>Since Hive does not provide the function <bpt id="2">&lt;code&gt;</bpt>atan2<ept id="2">&lt;/code&gt;</ept>, but provides the function <bpt id="4">&lt;code&gt;</bpt>atan<ept id="4">&lt;/code&gt;</ept>, the <bpt id="6">&lt;code&gt;</bpt>atan2<ept id="6">&lt;/code&gt;</ept> function is implemented by <bpt id="8">&lt;code&gt;</bpt>atan<ept id="8">&lt;/code&gt;</ept> function in the above Hive query using the definition provided in <bpt id="10">&lt;html&gt;</bpt><ept id="10">&lt;/html&gt;</ept>Wikipedia<bpt id="12">&lt;html&gt;</bpt><ept id="12">&lt;/html&gt;</ept>.</source>
            <target state="new">Since Hive does not provide the function <bpt id="2">&lt;code&gt;</bpt>atan2<ept id="2">&lt;/code&gt;</ept>, but provides the function <bpt id="4">&lt;code&gt;</bpt>atan<ept id="4">&lt;/code&gt;</ept>, the <bpt id="6">&lt;code&gt;</bpt>atan2<ept id="6">&lt;/code&gt;</ept> function is implemented by <bpt id="8">&lt;code&gt;</bpt>atan<ept id="8">&lt;/code&gt;</ept> function in the above Hive query using the definition provided in <bpt id="10">&lt;html&gt;</bpt><ept id="10">&lt;/html&gt;</ept>Wikipedia<bpt id="12">&lt;html&gt;</bpt><ept id="12">&lt;/html&gt;</ept>.</target>
          </trans-unit>
          <trans-unit id="233" xml:space="preserve">
            <source><bpt id="1">&lt;linkText&gt;</bpt>Create workspace<ept id="1">&lt;/linkText&gt;</ept></source>
            <target state="new"><bpt id="1">&lt;linkText&gt;</bpt>Create workspace<ept id="1">&lt;/linkText&gt;</ept></target>
          </trans-unit>
          <trans-unit id="234" xml:space="preserve">
            <source>A full list of Hive embedded UDFs can be found in the <bpt id="2">&lt;strong&gt;</bpt>Built-in Functions<ept id="2">&lt;/strong&gt;</ept> section on the <bpt id="4">&lt;html&gt;</bpt><ept id="4">&lt;/html&gt;</ept>Apache Hive wiki<bpt id="6">&lt;html&gt;</bpt><ept id="6">&lt;/html&gt;</ept>).</source>
            <target state="new">A full list of Hive embedded UDFs can be found in the <bpt id="2">&lt;strong&gt;</bpt>Built-in Functions<ept id="2">&lt;/strong&gt;</ept> section on the <bpt id="4">&lt;html&gt;</bpt><ept id="4">&lt;/html&gt;</ept>Apache Hive wiki<bpt id="6">&lt;html&gt;</bpt><ept id="6">&lt;/html&gt;</ept>).</target>
          </trans-unit>
          <trans-unit id="235" xml:space="preserve">
            <source><bpt id="1">&lt;html&gt;</bpt><ept id="1">&lt;/html&gt;</ept><bpt id="2">&lt;html&gt;</bpt><ept id="2">&lt;/html&gt;</ept> Advanced topics: Tune Hive Parameters to Improve Query Speed</source>
            <target state="new"><bpt id="1">&lt;html&gt;</bpt><ept id="1">&lt;/html&gt;</ept><bpt id="2">&lt;html&gt;</bpt><ept id="2">&lt;/html&gt;</ept> Advanced topics: Tune Hive Parameters to Improve Query Speed</target>
          </trans-unit>
          <trans-unit id="236" xml:space="preserve">
            <source>The default parameter settings of Hive cluster might not be suitable for the Hive queries and the data that the queries are processing.</source>
            <target state="new">The default parameter settings of Hive cluster might not be suitable for the Hive queries and the data that the queries are processing.</target>
          </trans-unit>
          <trans-unit id="237" xml:space="preserve">
            <source>In this section, we discuss some parameters that users can tune that improve the performance of Hive queries.</source>
            <target state="new">In this section, we discuss some parameters that users can tune that improve the performance of Hive queries.</target>
          </trans-unit>
          <trans-unit id="238" xml:space="preserve">
            <source>Users need to add the parameter tuning queries before the queries of processing data.</source>
            <target state="new">Users need to add the parameter tuning queries before the queries of processing data.</target>
          </trans-unit>
          <trans-unit id="239" xml:space="preserve">
            <source><bpt id="1">&lt;strong&gt;</bpt>Java heap space<ept id="1">&lt;/strong&gt;</ept>: For queries involving joining large datasets, or processing long records, <bpt id="3">&lt;strong&gt;</bpt>running out of heap space<ept id="3">&lt;/strong&gt;</ept> is one of the common error.</source>
            <target state="new"><bpt id="1">&lt;strong&gt;</bpt>Java heap space<ept id="1">&lt;/strong&gt;</ept>: For queries involving joining large datasets, or processing long records, <bpt id="3">&lt;strong&gt;</bpt>running out of heap space<ept id="3">&lt;/strong&gt;</ept> is one of the common error.</target>
          </trans-unit>
          <trans-unit id="240" xml:space="preserve">
            <source>This can be tuned by setting parameters <bpt id="2">&lt;em&gt;</bpt>mapreduce.map.java.opts<ept id="2">&lt;/em&gt;</ept> and <bpt id="4">&lt;em&gt;</bpt>mapreduce.task.io.sort.mb<ept id="4">&lt;/em&gt;</ept> to desired values.</source>
            <target state="new">This can be tuned by setting parameters <bpt id="2">&lt;em&gt;</bpt>mapreduce.map.java.opts<ept id="2">&lt;/em&gt;</ept> and <bpt id="4">&lt;em&gt;</bpt>mapreduce.task.io.sort.mb<ept id="4">&lt;/em&gt;</ept> to desired values.</target>
          </trans-unit>
          <trans-unit id="241" xml:space="preserve">
            <source>Here is an example:</source>
            <target state="new">Here is an example:</target>
          </trans-unit>
          <trans-unit id="242" xml:space="preserve">
            <source>This parameter allocates 4GB memory to Java heap space and also makes sorting more efficient by allocating more memory for it.</source>
            <target state="new">This parameter allocates 4GB memory to Java heap space and also makes sorting more efficient by allocating more memory for it.</target>
          </trans-unit>
          <trans-unit id="243" xml:space="preserve">
            <source>It is a good idea to play with these allocations if there are any job failure errors related to heap space.</source>
            <target state="new">It is a good idea to play with these allocations if there are any job failure errors related to heap space.</target>
          </trans-unit>
          <trans-unit id="244" xml:space="preserve">
            <source><bpt id="1">&lt;strong&gt;</bpt>DFS block size<ept id="1">&lt;/strong&gt;</ept> : This parameter sets the smallest unit of data that the file system stores.</source>
            <target state="new"><bpt id="1">&lt;strong&gt;</bpt>DFS block size<ept id="1">&lt;/strong&gt;</ept> : This parameter sets the smallest unit of data that the file system stores.</target>
          </trans-unit>
          <trans-unit id="245" xml:space="preserve">
            <source>As an example, if the DFS block size is 128MB, then any data of size less than and up to 128MB is stored in a single block, while data that is larger than 128MB is allotted extra blocks.</source>
            <target state="new">As an example, if the DFS block size is 128MB, then any data of size less than and up to 128MB is stored in a single block, while data that is larger than 128MB is allotted extra blocks.</target>
          </trans-unit>
          <trans-unit id="246" xml:space="preserve">
            <source>Choosing a very small block size causes large overheads in Hadoop since the name node has to process many more requests to find the relevant block pertaining to the file.</source>
            <target state="new">Choosing a very small block size causes large overheads in Hadoop since the name node has to process many more requests to find the relevant block pertaining to the file.</target>
          </trans-unit>
          <trans-unit id="247" xml:space="preserve">
            <source>A recommended setting when dealing with gigabytes (or larger) data is :</source>
            <target state="new">A recommended setting when dealing with gigabytes (or larger) data is :</target>
          </trans-unit>
          <trans-unit id="248" xml:space="preserve">
            <source><bpt id="1">&lt;strong&gt;</bpt>Optimizing join operation in Hive<ept id="1">&lt;/strong&gt;</ept> : While join operations in the map/reduce framework typically take place in the reduce phase, sometimes, enormous gains can be achieved by scheduling joins in the map phase (also called "mapjoins").</source>
            <target state="new"><bpt id="1">&lt;strong&gt;</bpt>Optimizing join operation in Hive<ept id="1">&lt;/strong&gt;</ept> : While join operations in the map/reduce framework typically take place in the reduce phase, sometimes, enormous gains can be achieved by scheduling joins in the map phase (also called "mapjoins").</target>
          </trans-unit>
          <trans-unit id="249" xml:space="preserve">
            <source>To direct Hive to do this whenever possible, we can set :</source>
            <target state="new">To direct Hive to do this whenever possible, we can set :</target>
          </trans-unit>
          <trans-unit id="250" xml:space="preserve">
            <source><bpt id="1">&lt;strong&gt;</bpt>Specifying the number of mappers to Hive<ept id="1">&lt;/strong&gt;</ept> : While Hadoop allows the user to set the number of reducers, the number of mappers is typically not be set by the user.</source>
            <target state="new"><bpt id="1">&lt;strong&gt;</bpt>Specifying the number of mappers to Hive<ept id="1">&lt;/strong&gt;</ept> : While Hadoop allows the user to set the number of reducers, the number of mappers is typically not be set by the user.</target>
          </trans-unit>
          <trans-unit id="251" xml:space="preserve">
            <source>A trick that allows some degree of control on this number is to choose the Hadoop variables, <bpt id="2">&lt;em&gt;</bpt>mapred.min.split.size<ept id="2">&lt;/em&gt;</ept> and <bpt id="4">&lt;em&gt;</bpt>mapred.max.split.size<ept id="4">&lt;/em&gt;</ept> as the size of each map task is determined by :</source>
            <target state="new">A trick that allows some degree of control on this number is to choose the Hadoop variables, <bpt id="2">&lt;em&gt;</bpt>mapred.min.split.size<ept id="2">&lt;/em&gt;</ept> and <bpt id="4">&lt;em&gt;</bpt>mapred.max.split.size<ept id="4">&lt;/em&gt;</ept> as the size of each map task is determined by :</target>
          </trans-unit>
          <trans-unit id="252" xml:space="preserve">
            <source>Typically, the default value of <bpt id="2">&lt;em&gt;</bpt>mapred.min.split.size<ept id="2">&lt;/em&gt;</ept> is 0, that of <bpt id="4">&lt;em&gt;</bpt>mapred.max.split.size<ept id="4">&lt;/em&gt;</ept> is <bpt id="6">&lt;strong&gt;</bpt>Long.MAX<ept id="6">&lt;/strong&gt;</ept> and that of <bpt id="8">&lt;em&gt;</bpt>dfs.block.size<ept id="8">&lt;/em&gt;</ept> is 64MB.</source>
            <target state="new">Typically, the default value of <bpt id="2">&lt;em&gt;</bpt>mapred.min.split.size<ept id="2">&lt;/em&gt;</ept> is 0, that of <bpt id="4">&lt;em&gt;</bpt>mapred.max.split.size<ept id="4">&lt;/em&gt;</ept> is <bpt id="6">&lt;strong&gt;</bpt>Long.MAX<ept id="6">&lt;/strong&gt;</ept> and that of <bpt id="8">&lt;em&gt;</bpt>dfs.block.size<ept id="8">&lt;/em&gt;</ept> is 64MB.</target>
          </trans-unit>
          <trans-unit id="253" xml:space="preserve">
            <source>As we can see, given the data size, tuning these parameters by "setting" them allows us to tune the number of mappers used.</source>
            <target state="new">As we can see, given the data size, tuning these parameters by "setting" them allows us to tune the number of mappers used.</target>
          </trans-unit>
          <trans-unit id="254" xml:space="preserve">
            <source>A few other more <bpt id="2">&lt;strong&gt;</bpt>advanced options<ept id="2">&lt;/strong&gt;</ept> for optimizing Hive performance are mentioned below.</source>
            <target state="new">A few other more <bpt id="2">&lt;strong&gt;</bpt>advanced options<ept id="2">&lt;/strong&gt;</ept> for optimizing Hive performance are mentioned below.</target>
          </trans-unit>
          <trans-unit id="255" xml:space="preserve">
            <source>These allow you to set the memory allocated to map and reduce tasks, and can be useful in tweaking performance.</source>
            <target state="new">These allow you to set the memory allocated to map and reduce tasks, and can be useful in tweaking performance.</target>
          </trans-unit>
          <trans-unit id="256" xml:space="preserve">
            <source>Please keep in mind that the <bpt id="2">&lt;em&gt;</bpt>mapreduce.reduce.memory.mb<ept id="2">&lt;/em&gt;</ept> cannot be greater than the physical memory size of each worker node in the Hadoop cluster.</source>
            <target state="new">Please keep in mind that the <bpt id="2">&lt;em&gt;</bpt>mapreduce.reduce.memory.mb<ept id="2">&lt;/em&gt;</ept> cannot be greater than the physical memory size of each worker node in the Hadoop cluster.</target>
          </trans-unit>
        </group>
      </group>
    </body>
  </file>
</xliff>