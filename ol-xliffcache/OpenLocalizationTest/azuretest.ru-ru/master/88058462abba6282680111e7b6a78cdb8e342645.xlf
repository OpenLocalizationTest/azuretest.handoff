<?xml version="1.0" encoding="utf-8"?>
<xliff version="1.2" xmlns="urn:oasis:names:tc:xliff:document:1.2" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="urn:oasis:names:tc:xliff:document:1.2 xliff-core-1.2-transitional.xsd">
  <file datatype="xml" source-language="en-us" target-language="ru-ru" original="2/22/2016 1:02:38 AM" tool-id="MarkdownTransformer" product-name="N/A" product-version="N/A" build-num="1">
    <header>
      <xliffext:olfilehash xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">88058462abba6282680111e7b6a78cdb8e342645</xliffext:olfilehash>
      <tool tool-id="MarkdownTransformer" tool-name="MarkdownToXliff" tool-version="1.0" tool-company="Microsoft" />
    </header>
    <body>
      <group extype="content">
        <group id="101">
          <trans-unit id="101" xml:space="preserve">
            <source>Create and load data into Hive tables from Blob storage | Microsoft Azure</source>
            <target state="new">Create and load data into Hive tables from Blob storage | Microsoft Azure</target>
          </trans-unit>
          <trans-unit id="102" xml:space="preserve">
            <source>Create Hive tables and load data in blob to hive tables</source>
            <target state="new">Create Hive tables and load data in blob to hive tables</target>
          </trans-unit>
          <trans-unit id="103" xml:space="preserve">
            <source>Create and load data into Hive tables from Azure blob storage</source>
            <target state="new">Create and load data into Hive tables from Azure blob storage</target>
          </trans-unit>
          <trans-unit id="104" xml:space="preserve">
            <source>In this document, generic Hive queries that create Hive tables and load data from Azure blob storage are presented.</source>
            <target state="new">In this document, generic Hive queries that create Hive tables and load data from Azure blob storage are presented.</target>
          </trans-unit>
          <trans-unit id="105" xml:space="preserve">
            <source>These Hive queries are shared in the GitHub repository.</source>
            <target state="new">These Hive queries are shared in the GitHub repository.</target>
          </trans-unit>
          <trans-unit id="106" xml:space="preserve">
            <source><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>Github repository<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept>.</source>
            <target state="new"><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>Github repository<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept>.</target>
          </trans-unit>
          <trans-unit id="107" xml:space="preserve">
            <source>If you create an Azure virtual machine by following the instructions in "Set Up an Azure Virtual Machine with IPython Notebook Server", this script file has been downloaded to the <bpt id="2">&lt;code&gt;</bpt>C:\Users\&lt;user name&gt;\Documents\Data Science Scripts<ept id="2">&lt;/code&gt;</ept> directory on the virtual machine. You need to plug in your own data schema and Azure blob storage configuration in the corresponding fields in these queries and these Hive queries should be ready for submission.</source>
            <target state="new">If you create an Azure virtual machine by following the instructions in "Set Up an Azure Virtual Machine with IPython Notebook Server", this script file has been downloaded to the <bpt id="2">&lt;code&gt;</bpt>C:\Users\&lt;user name&gt;\Documents\Data Science Scripts<ept id="2">&lt;/code&gt;</ept> directory on the virtual machine. You need to plug in your own data schema and Azure blob storage configuration in the corresponding fields in these queries and these Hive queries should be ready for submission.</target>
          </trans-unit>
          <trans-unit id="108" xml:space="preserve">
            <source>We assume that the data for Hive tables is in <bpt id="2">&lt;strong&gt;</bpt>uncompressed<ept id="2">&lt;/strong&gt;</ept> tabular format, and the data has been uploaded to the default or additional container of the storage account used by the Hadoop cluster.</source>
            <target state="new">We assume that the data for Hive tables is in <bpt id="2">&lt;strong&gt;</bpt>uncompressed<ept id="2">&lt;/strong&gt;</ept> tabular format, and the data has been uploaded to the default or additional container of the storage account used by the Hadoop cluster.</target>
          </trans-unit>
          <trans-unit id="109" xml:space="preserve">
            <source>If users want to practice on the _NYC Taxi Trip Data_, they need to first <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>download all 24 files<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept> (12 Trip files, and 12 Fair files), <bpt id="4">&lt;strong&gt;</bpt>unzip<ept id="4">&lt;/strong&gt;</ept> all files into .csv files, and upload them to the default or additional container of the Azure storage account that are used when the <bpt id="6CapsExtId1">&lt;link&gt;</bpt><bpt id="6CapsExtId2">&lt;linkText&gt;</bpt>Azure HDInsight Hadoop cluster is customized<ept id="6CapsExtId2">&lt;/linkText&gt;</ept><bpt id="6CapsExtId3">&lt;title&gt;</bpt><ept id="6CapsExtId3">&lt;/title&gt;</ept><ept id="6CapsExtId1">&lt;/link&gt;</ept>.</source>
            <target state="new">If users want to practice on the _NYC Taxi Trip Data_, they need to first <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>download all 24 files<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept> (12 Trip files, and 12 Fair files), <bpt id="4">&lt;strong&gt;</bpt>unzip<ept id="4">&lt;/strong&gt;</ept> all files into .csv files, and upload them to the default or additional container of the Azure storage account that are used when the <bpt id="6CapsExtId1">&lt;link&gt;</bpt><bpt id="6CapsExtId2">&lt;linkText&gt;</bpt>Azure HDInsight Hadoop cluster is customized<ept id="6CapsExtId2">&lt;/linkText&gt;</ept><bpt id="6CapsExtId3">&lt;title&gt;</bpt><ept id="6CapsExtId3">&lt;/title&gt;</ept><ept id="6CapsExtId1">&lt;/link&gt;</ept>.</target>
          </trans-unit>
          <trans-unit id="110" xml:space="preserve">
            <source>Hive queries can be submitted in the Hadoop Command Line on the head node of the Hadoop cluster.</source>
            <target state="new">Hive queries can be submitted in the Hadoop Command Line on the head node of the Hadoop cluster.</target>
          </trans-unit>
          <trans-unit id="111" xml:space="preserve">
            <source>You need to:</source>
            <target state="new">You need to:</target>
          </trans-unit>
          <trans-unit id="112" xml:space="preserve">
            <source><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>Enable remote access to the head node of the Hadoop cluster, and log on to the head node<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept>.</source>
            <target state="new"><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>Enable remote access to the head node of the Hadoop cluster, and log on to the head node<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept>.</target>
          </trans-unit>
          <trans-unit id="113" xml:space="preserve">
            <source><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>Submit the Hive queries in the Hadoop Command Line on the head node<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept>.</source>
            <target state="new"><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>Submit the Hive queries in the Hadoop Command Line on the head node<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept>.</target>
          </trans-unit>
          <trans-unit id="114" xml:space="preserve">
            <source>Users can also use [Query Console (Hive Editor)] by entering the URL in a web browser `https://<bpt id="2">&lt;html&gt;</bpt><ept id="2">&lt;/html&gt;</ept>.azurehdinsight.net/Home/HiveEditor (you will be asked to input the Hadoop cluster credentials to log in), or can <bpt id="4CapsExtId1">&lt;link&gt;</bpt><bpt id="4CapsExtId2">&lt;linkText&gt;</bpt>submit Hive jobs using PowerShell<ept id="4CapsExtId2">&lt;/linkText&gt;</ept><bpt id="4CapsExtId3">&lt;title&gt;</bpt><ept id="4CapsExtId3">&lt;/title&gt;</ept><ept id="4CapsExtId1">&lt;/link&gt;</ept>.</source>
            <target state="new">Users can also use [Query Console (Hive Editor)] by entering the URL in a web browser `https://<bpt id="2">&lt;html&gt;</bpt><ept id="2">&lt;/html&gt;</ept>.azurehdinsight.net/Home/HiveEditor (you will be asked to input the Hadoop cluster credentials to log in), or can <bpt id="4CapsExtId1">&lt;link&gt;</bpt><bpt id="4CapsExtId2">&lt;linkText&gt;</bpt>submit Hive jobs using PowerShell<ept id="4CapsExtId2">&lt;/linkText&gt;</ept><bpt id="4CapsExtId3">&lt;title&gt;</bpt><ept id="4CapsExtId3">&lt;/title&gt;</ept><ept id="4CapsExtId1">&lt;/link&gt;</ept>.</target>
          </trans-unit>
          <trans-unit id="115" xml:space="preserve">
            <source><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>Step 1: Create Hive database and tables<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept></source>
            <target state="new"><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>Step 1: Create Hive database and tables<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept></target>
          </trans-unit>
          <trans-unit id="116" xml:space="preserve">
            <source><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>Step 2: Load data to Hive tables<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept></source>
            <target state="new"><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>Step 2: Load data to Hive tables<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept></target>
          </trans-unit>
          <trans-unit id="117" xml:space="preserve">
            <source><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>Advanced topics: partitioned table and store Hive data in ORC format<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept></source>
            <target state="new"><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>Advanced topics: partitioned table and store Hive data in ORC format<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept></target>
          </trans-unit>
          <trans-unit id="118" xml:space="preserve">
            <source><bpt id="1">&lt;html&gt;</bpt><ept id="1">&lt;/html&gt;</ept><bpt id="2">&lt;html&gt;</bpt><ept id="2">&lt;/html&gt;</ept>Create Hive database and tables</source>
            <target state="new"><bpt id="1">&lt;html&gt;</bpt><ept id="1">&lt;/html&gt;</ept><bpt id="2">&lt;html&gt;</bpt><ept id="2">&lt;/html&gt;</ept>Create Hive database and tables</target>
          </trans-unit>
          <trans-unit id="119" xml:space="preserve">
            <source>Here are the descriptions of the fields that users need to plug in and other configurations:</source>
            <target state="new">Here are the descriptions of the fields that users need to plug in and other configurations:</target>
          </trans-unit>
          <trans-unit id="120" xml:space="preserve">
            <source><bpt id="1">&lt;code&gt;</bpt>&lt;database name&gt;<ept id="1">&lt;/code&gt;</ept>: the name of the database users want to create. If users just want to use the default database, the query <bpt id="3">&lt;code&gt;</bpt>create database...<ept id="3">&lt;/code&gt;</ept> can be omitted.</source>
            <target state="new"><bpt id="1">&lt;code&gt;</bpt>&lt;database name&gt;<ept id="1">&lt;/code&gt;</ept>: the name of the database users want to create. If users just want to use the default database, the query <bpt id="3">&lt;code&gt;</bpt>create database...<ept id="3">&lt;/code&gt;</ept> can be omitted.</target>
          </trans-unit>
          <trans-unit id="121" xml:space="preserve">
            <source><bpt id="1">&lt;code&gt;</bpt>&lt;table name&gt;<ept id="1">&lt;/code&gt;</ept>: the name of the table users want to create within the specified database. If users want to use the default database, the table can be directly referred by <bpt id="3">&lt;code&gt;</bpt>&lt;table name&gt;<ept id="3">&lt;/code&gt;</ept> without <bpt id="5">&lt;code&gt;</bpt>&lt;database name&gt;.<ept id="5">&lt;/code&gt;</ept>.</source>
            <target state="new"><bpt id="1">&lt;code&gt;</bpt>&lt;table name&gt;<ept id="1">&lt;/code&gt;</ept>: the name of the table users want to create within the specified database. If users want to use the default database, the table can be directly referred by <bpt id="3">&lt;code&gt;</bpt>&lt;table name&gt;<ept id="3">&lt;/code&gt;</ept> without <bpt id="5">&lt;code&gt;</bpt>&lt;database name&gt;.<ept id="5">&lt;/code&gt;</ept>.</target>
          </trans-unit>
          <trans-unit id="122" xml:space="preserve">
            <source><bpt id="1">&lt;code&gt;</bpt>&lt;field separator&gt;<ept id="1">&lt;/code&gt;</ept>: the separator that separates fields in the data file to be uploaded to the Hive table.</source>
            <target state="new"><bpt id="1">&lt;code&gt;</bpt>&lt;field separator&gt;<ept id="1">&lt;/code&gt;</ept>: the separator that separates fields in the data file to be uploaded to the Hive table.</target>
          </trans-unit>
          <trans-unit id="123" xml:space="preserve">
            <source><bpt id="1">&lt;code&gt;</bpt>&lt;line separator&gt;<ept id="1">&lt;/code&gt;</ept>: the separator that separates lines in the data file.</source>
            <target state="new"><bpt id="1">&lt;code&gt;</bpt>&lt;line separator&gt;<ept id="1">&lt;/code&gt;</ept>: the separator that separates lines in the data file.</target>
          </trans-unit>
          <trans-unit id="124" xml:space="preserve">
            <source><bpt id="1">&lt;code&gt;</bpt>&lt;storage location&gt;<ept id="1">&lt;/code&gt;</ept>: the Azure storage location to save the data of Hive tables. If users do not specify <bpt id="3">&lt;code&gt;</bpt>LOCATION '&lt;storage location&gt;'<ept id="3">&lt;/code&gt;</ept>, by default the database and the tables are stored in <bpt id="5">&lt;code&gt;</bpt>hive/warehouse/<ept id="5">&lt;/code&gt;</ept> directory in the default container of the Hive cluster. If a user wants to specify the storage location,  the storage location has to be within the default container for the database and tables. This location has to be referred as relative location to the default container of the cluster in the format of <bpt id="7">&lt;code&gt;</bpt>'wasb:///&lt;directory 1&gt;/'<ept id="7">&lt;/code&gt;</ept> or <bpt id="9">&lt;code&gt;</bpt>'wasb:///&lt;directory 1&gt;/&lt;directory 2&gt;/'<ept id="9">&lt;/code&gt;</ept>, etc. After the query is executed, the relative directories will be created within the default container.</source>
            <target state="new"><bpt id="1">&lt;code&gt;</bpt>&lt;storage location&gt;<ept id="1">&lt;/code&gt;</ept>: the Azure storage location to save the data of Hive tables. If users do not specify <bpt id="3">&lt;code&gt;</bpt>LOCATION '&lt;storage location&gt;'<ept id="3">&lt;/code&gt;</ept>, by default the database and the tables are stored in <bpt id="5">&lt;code&gt;</bpt>hive/warehouse/<ept id="5">&lt;/code&gt;</ept> directory in the default container of the Hive cluster. If a user wants to specify the storage location,  the storage location has to be within the default container for the database and tables. This location has to be referred as relative location to the default container of the cluster in the format of <bpt id="7">&lt;code&gt;</bpt>'wasb:///&lt;directory 1&gt;/'<ept id="7">&lt;/code&gt;</ept> or <bpt id="9">&lt;code&gt;</bpt>'wasb:///&lt;directory 1&gt;/&lt;directory 2&gt;/'<ept id="9">&lt;/code&gt;</ept>, etc. After the query is executed, the relative directories will be created within the default container.</target>
          </trans-unit>
          <trans-unit id="125" xml:space="preserve">
            <source><bpt id="1">&lt;code&gt;</bpt>TBLPROPERTIES("skip.header.line.count"="1")<ept id="1">&lt;/code&gt;</ept>: If the data file has a header line, users have to add this property at the <bpt id="3">&lt;strong&gt;</bpt>end<ept id="3">&lt;/strong&gt;</ept> of the <bpt id="5">&lt;code&gt;</bpt>create table<ept id="5">&lt;/code&gt;</ept> query.</source>
            <target state="new"><bpt id="1">&lt;code&gt;</bpt>TBLPROPERTIES("skip.header.line.count"="1")<ept id="1">&lt;/code&gt;</ept>: If the data file has a header line, users have to add this property at the <bpt id="3">&lt;strong&gt;</bpt>end<ept id="3">&lt;/strong&gt;</ept> of the <bpt id="5">&lt;code&gt;</bpt>create table<ept id="5">&lt;/code&gt;</ept> query.</target>
          </trans-unit>
          <trans-unit id="126" xml:space="preserve">
            <source>Otherwise, the header line will be loaded as a record to the table.</source>
            <target state="new">Otherwise, the header line will be loaded as a record to the table.</target>
          </trans-unit>
          <trans-unit id="127" xml:space="preserve">
            <source>If the data file does not have a header line, this configuration can be omitted in the query.</source>
            <target state="new">If the data file does not have a header line, this configuration can be omitted in the query.</target>
          </trans-unit>
          <trans-unit id="128" xml:space="preserve">
            <source><bpt id="1">&lt;html&gt;</bpt><ept id="1">&lt;/html&gt;</ept><bpt id="2">&lt;html&gt;</bpt><ept id="2">&lt;/html&gt;</ept>Load data to Hive tables</source>
            <target state="new"><bpt id="1">&lt;html&gt;</bpt><ept id="1">&lt;/html&gt;</ept><bpt id="2">&lt;html&gt;</bpt><ept id="2">&lt;/html&gt;</ept>Load data to Hive tables</target>
          </trans-unit>
          <trans-unit id="129" xml:space="preserve">
            <source><bpt id="1">&lt;code&gt;</bpt>&lt;path to blob data&gt;<ept id="1">&lt;/code&gt;</ept>: If the blob file to be uploaded to Hive table is in the default container of the HDInsight Hadoop cluster, the <bpt id="3">&lt;code&gt;</bpt>&lt;path to blob data&gt;<ept id="3">&lt;/code&gt;</ept> should be in the format <bpt id="5">&lt;code&gt;</bpt>'wasb:///&lt;directory in this container&gt;/&lt;blob file name&gt;'<ept id="5">&lt;/code&gt;</ept>. The blob file can also be in the additional container of the HDInsight Hadoop cluster. In this case, <bpt id="7">&lt;code&gt;</bpt>&lt;path to blob data&gt;<ept id="7">&lt;/code&gt;</ept> should be in the format <bpt id="9">&lt;code&gt;</bpt>'wasb://&lt;container name&gt;@&lt;storage account name&gt;.blob.windows.core.net/&lt;blob file name&gt;'<ept id="9">&lt;/code&gt;</ept>.</source>
            <target state="new"><bpt id="1">&lt;code&gt;</bpt>&lt;path to blob data&gt;<ept id="1">&lt;/code&gt;</ept>: If the blob file to be uploaded to Hive table is in the default container of the HDInsight Hadoop cluster, the <bpt id="3">&lt;code&gt;</bpt>&lt;path to blob data&gt;<ept id="3">&lt;/code&gt;</ept> should be in the format <bpt id="5">&lt;code&gt;</bpt>'wasb:///&lt;directory in this container&gt;/&lt;blob file name&gt;'<ept id="5">&lt;/code&gt;</ept>. The blob file can also be in the additional container of the HDInsight Hadoop cluster. In this case, <bpt id="7">&lt;code&gt;</bpt>&lt;path to blob data&gt;<ept id="7">&lt;/code&gt;</ept> should be in the format <bpt id="9">&lt;code&gt;</bpt>'wasb://&lt;container name&gt;@&lt;storage account name&gt;.blob.windows.core.net/&lt;blob file name&gt;'<ept id="9">&lt;/code&gt;</ept>.</target>
          </trans-unit>
          <trans-unit id="130" xml:space="preserve">
            <source>The blob data to be uploaded to Hive table has to be in the default or additional container of the storage account for the Hadoop cluster.</source>
            <target state="new">The blob data to be uploaded to Hive table has to be in the default or additional container of the storage account for the Hadoop cluster.</target>
          </trans-unit>
          <trans-unit id="131" xml:space="preserve">
            <source>Otherwise, the <bpt id="2">&lt;code&gt;</bpt>LOAD DATA<ept id="2">&lt;/code&gt;</ept> query will fail complaining that it cannot access the data.</source>
            <target state="new">Otherwise, the <bpt id="2">&lt;code&gt;</bpt>LOAD DATA<ept id="2">&lt;/code&gt;</ept> query will fail complaining that it cannot access the data.</target>
          </trans-unit>
          <trans-unit id="132" xml:space="preserve">
            <source><bpt id="1">&lt;html&gt;</bpt><ept id="1">&lt;/html&gt;</ept><bpt id="2">&lt;html&gt;</bpt><ept id="2">&lt;/html&gt;</ept>Advanced topics: partitioned table and store Hive data in ORC format</source>
            <target state="new"><bpt id="1">&lt;html&gt;</bpt><ept id="1">&lt;/html&gt;</ept><bpt id="2">&lt;html&gt;</bpt><ept id="2">&lt;/html&gt;</ept>Advanced topics: partitioned table and store Hive data in ORC format</target>
          </trans-unit>
          <trans-unit id="133" xml:space="preserve">
            <source>If the data is large, partitioning the table will be beneficial for queries that only need to scan a few partitions of the table.</source>
            <target state="new">If the data is large, partitioning the table will be beneficial for queries that only need to scan a few partitions of the table.</target>
          </trans-unit>
          <trans-unit id="134" xml:space="preserve">
            <source>For instance,  it is reasonable to partition the log data of a web site by dates.</source>
            <target state="new">For instance,  it is reasonable to partition the log data of a web site by dates.</target>
          </trans-unit>
          <trans-unit id="135" xml:space="preserve">
            <source>In addition to partition the table, it is also beneficial to store the Hive data in ORC format.</source>
            <target state="new">In addition to partition the table, it is also beneficial to store the Hive data in ORC format.</target>
          </trans-unit>
          <trans-unit id="136" xml:space="preserve">
            <source>ORC stands for "Optimized Row Columnar".</source>
            <target state="new">ORC stands for "Optimized Row Columnar".</target>
          </trans-unit>
          <trans-unit id="137" xml:space="preserve">
            <source>Please refer to _"<bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>Using ORC files improves performance when Hive is reading, writing, and processing data<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept>."_ for more details.</source>
            <target state="new">Please refer to _"<bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>Using ORC files improves performance when Hive is reading, writing, and processing data<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept>."_ for more details.</target>
          </trans-unit>
          <trans-unit id="138" xml:space="preserve">
            <source>Partitioned table</source>
            <target state="new">Partitioned table</target>
          </trans-unit>
          <trans-unit id="139" xml:space="preserve">
            <source>The queries of creating a partitioned table and loading data to it are as follows:</source>
            <target state="new">The queries of creating a partitioned table and loading data to it are as follows:</target>
          </trans-unit>
          <trans-unit id="140" xml:space="preserve">
            <source>When querying partitioned tables, it is recommended to add the partition condition in the <bpt id="2">&lt;strong&gt;</bpt>beginning<ept id="2">&lt;/strong&gt;</ept> of the <bpt id="4">&lt;code&gt;</bpt>where<ept id="4">&lt;/code&gt;</ept> clause so that the searching efficacy can be significantly improved.</source>
            <target state="new">When querying partitioned tables, it is recommended to add the partition condition in the <bpt id="2">&lt;strong&gt;</bpt>beginning<ept id="2">&lt;/strong&gt;</ept> of the <bpt id="4">&lt;code&gt;</bpt>where<ept id="4">&lt;/code&gt;</ept> clause so that the searching efficacy can be significantly improved.</target>
          </trans-unit>
          <trans-unit id="141" xml:space="preserve">
            <source><bpt id="1">&lt;html&gt;</bpt><ept id="1">&lt;/html&gt;</ept><bpt id="2">&lt;html&gt;</bpt><ept id="2">&lt;/html&gt;</ept>Store Hive data in ORC format</source>
            <target state="new"><bpt id="1">&lt;html&gt;</bpt><ept id="1">&lt;/html&gt;</ept><bpt id="2">&lt;html&gt;</bpt><ept id="2">&lt;/html&gt;</ept>Store Hive data in ORC format</target>
          </trans-unit>
          <trans-unit id="142" xml:space="preserve">
            <source>Users cannot directly load data in blob to Hive tables in ORC storage format.</source>
            <target state="new">Users cannot directly load data in blob to Hive tables in ORC storage format.</target>
          </trans-unit>
          <trans-unit id="143" xml:space="preserve">
            <source>Here are the steps that the users need to take in order to load data from Azure blobs to Hive tables stored in ORC format.</source>
            <target state="new">Here are the steps that the users need to take in order to load data from Azure blobs to Hive tables stored in ORC format.</target>
          </trans-unit>
          <trans-unit id="144" xml:space="preserve">
            <source>Create an external table <bpt id="2">&lt;strong&gt;</bpt>STORED AS TEXTFILE<ept id="2">&lt;/strong&gt;</ept> and load data from blob storage to the table.</source>
            <target state="new">Create an external table <bpt id="2">&lt;strong&gt;</bpt>STORED AS TEXTFILE<ept id="2">&lt;/strong&gt;</ept> and load data from blob storage to the table.</target>
          </trans-unit>
          <trans-unit id="145" xml:space="preserve">
            <source>Create an internal table with the same schema as the external table in step 1, and the same field delimiter.</source>
            <target state="new">Create an internal table with the same schema as the external table in step 1, and the same field delimiter.</target>
          </trans-unit>
          <trans-unit id="146" xml:space="preserve">
            <source>And store the Hive data in the ORC format</source>
            <target state="new">And store the Hive data in the ORC format</target>
          </trans-unit>
          <trans-unit id="147" xml:space="preserve">
            <source>Select data from the external table in step 1 and insert into the ORC table</source>
            <target state="new">Select data from the external table in step 1 and insert into the ORC table</target>
          </trans-unit>
          <trans-unit id="148" xml:space="preserve">
            <source>If the TEXTFILE table <bpt id="2">&lt;code&gt;</bpt>&lt;database name&gt;.&lt;external textfile table name&gt;<ept id="2">&lt;/code&gt;</ept> has partitions, in STEP 3, <bpt id="4">&lt;code&gt;</bpt>SELECT * FROM &lt;database name&gt;.&lt;external textfile table name&gt;<ept id="4">&lt;/code&gt;</ept> will select the partition variable as a field in the returned data set. Inserting it to the <bpt id="6">&lt;code&gt;</bpt>&lt;database name&gt;.&lt;ORC table name&gt;<ept id="6">&lt;/code&gt;</ept> will fail since <bpt id="8">&lt;code&gt;</bpt>&lt;database name&gt;.&lt;ORC table name&gt;<ept id="8">&lt;/code&gt;</ept> does not have the partition variable as a field in the table schema. In this case, users need to specifically select the fields to be inserted to <bpt id="10">&lt;code&gt;</bpt>&lt;database name&gt;.&lt;ORC table name&gt;<ept id="10">&lt;/code&gt;</ept> like follows:</source>
            <target state="new">If the TEXTFILE table <bpt id="2">&lt;code&gt;</bpt>&lt;database name&gt;.&lt;external textfile table name&gt;<ept id="2">&lt;/code&gt;</ept> has partitions, in STEP 3, <bpt id="4">&lt;code&gt;</bpt>SELECT * FROM &lt;database name&gt;.&lt;external textfile table name&gt;<ept id="4">&lt;/code&gt;</ept> will select the partition variable as a field in the returned data set. Inserting it to the <bpt id="6">&lt;code&gt;</bpt>&lt;database name&gt;.&lt;ORC table name&gt;<ept id="6">&lt;/code&gt;</ept> will fail since <bpt id="8">&lt;code&gt;</bpt>&lt;database name&gt;.&lt;ORC table name&gt;<ept id="8">&lt;/code&gt;</ept> does not have the partition variable as a field in the table schema. In this case, users need to specifically select the fields to be inserted to <bpt id="10">&lt;code&gt;</bpt>&lt;database name&gt;.&lt;ORC table name&gt;<ept id="10">&lt;/code&gt;</ept> like follows:</target>
          </trans-unit>
          <trans-unit id="149" xml:space="preserve">
            <source>It is safe to drop the <bpt id="2">&lt;code&gt;</bpt>&lt;external textfile table name&gt;<ept id="2">&lt;/code&gt;</ept> using the following query after all data has been inserted into <bpt id="4">&lt;code&gt;</bpt>&lt;database name&gt;.&lt;ORC table name&gt;<ept id="4">&lt;/code&gt;</ept>:</source>
            <target state="new">It is safe to drop the <bpt id="2">&lt;code&gt;</bpt>&lt;external textfile table name&gt;<ept id="2">&lt;/code&gt;</ept> using the following query after all data has been inserted into <bpt id="4">&lt;code&gt;</bpt>&lt;database name&gt;.&lt;ORC table name&gt;<ept id="4">&lt;/code&gt;</ept>:</target>
          </trans-unit>
          <trans-unit id="150" xml:space="preserve">
            <source>Now we have a table with data in the ORC format ready to use.</source>
            <target state="new">Now we have a table with data in the ORC format ready to use.</target>
          </trans-unit>
          <trans-unit id="151" xml:space="preserve">
            <source>test</source>
            <target state="new">test</target>
          </trans-unit>
        </group>
      </group>
    </body>
  </file>
</xliff>