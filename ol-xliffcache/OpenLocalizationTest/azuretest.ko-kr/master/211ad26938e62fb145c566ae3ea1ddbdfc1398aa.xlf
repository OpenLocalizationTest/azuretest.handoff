<?xml version="1.0" encoding="utf-8"?>
<xliff version="1.2" xmlns="urn:oasis:names:tc:xliff:document:1.2" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="urn:oasis:names:tc:xliff:document:1.2 xliff-core-1.2-transitional.xsd">
  <file datatype="xml" source-language="en-us" target-language="ko-kr" original="2/21/2016 5:32:33 AM" tool-id="MarkdownTransformer" product-name="N/A" product-version="N/A" build-num="1">
    <header>
      <xliffext:olfilehash xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">211ad26938e62fb145c566ae3ea1ddbdfc1398aa</xliffext:olfilehash>
      <tool tool-id="MarkdownTransformer" tool-name="MarkdownToXliff" tool-version="1.0" tool-company="Microsoft" />
    </header>
    <body>
      <group extype="content">
        <group id="101">
          <trans-unit id="101" xml:space="preserve">
            <source>Learn about Stream Analytics key concepts | Microsoft Azure</source>
            <target state="new">Learn about Stream Analytics key concepts | Microsoft Azure</target>
          </trans-unit>
          <trans-unit id="102" xml:space="preserve">
            <source>Learn key concepts of Azure Stream Analytics: Components of a stream analytics job, including supported inputs and outputs, job configuration, and metrics.</source>
            <target state="new">Learn key concepts of Azure Stream Analytics: Components of a stream analytics job, including supported inputs and outputs, job configuration, and metrics.</target>
          </trans-unit>
          <trans-unit id="103" xml:space="preserve">
            <source>Stream Analytics key concepts: Guide to the basics of a stream analytics job</source>
            <target state="new">Stream Analytics key concepts: Guide to the basics of a stream analytics job</target>
          </trans-unit>
          <trans-unit id="104" xml:space="preserve">
            <source>Azure Stream Analytics is a fully managed service providing low-latency, highly available, scalable, complex event processing over a data stream in the cloud.</source>
            <target state="new">Azure Stream Analytics is a fully managed service providing low-latency, highly available, scalable, complex event processing over a data stream in the cloud.</target>
          </trans-unit>
          <trans-unit id="105" xml:space="preserve">
            <source>Stream Analytics enables customers to set up streaming jobs to analyze data streams, and allows customers to drive near real-time analytics.</source>
            <target state="new">Stream Analytics enables customers to set up streaming jobs to analyze data streams, and allows customers to drive near real-time analytics.</target>
          </trans-unit>
          <trans-unit id="106" xml:space="preserve">
            <source>This article explains the key concepts of a Stream Analytics job.</source>
            <target state="new">This article explains the key concepts of a Stream Analytics job.</target>
          </trans-unit>
          <trans-unit id="107" xml:space="preserve">
            <source>What can you do in Stream Analytics?</source>
            <target state="new">What can you do in Stream Analytics?</target>
          </trans-unit>
          <trans-unit id="108" xml:space="preserve">
            <source>With Stream Analytics, you can:</source>
            <target state="new">With Stream Analytics, you can:</target>
          </trans-unit>
          <trans-unit id="109" xml:space="preserve">
            <source>Perform complex event processing on high-volume and high-velocity data streams.</source>
            <target state="new">Perform complex event processing on high-volume and high-velocity data streams.</target>
          </trans-unit>
          <trans-unit id="110" xml:space="preserve">
            <source>Collect event data from globally distributed assets or equipment, such as connected cars or utility grids.</source>
            <target state="new">Collect event data from globally distributed assets or equipment, such as connected cars or utility grids.</target>
          </trans-unit>
          <trans-unit id="111" xml:space="preserve">
            <source>Process telemetry data for near real-time monitoring and diagnostics.</source>
            <target state="new">Process telemetry data for near real-time monitoring and diagnostics.</target>
          </trans-unit>
          <trans-unit id="112" xml:space="preserve">
            <source>Capture and archive real-time events for future processing</source>
            <target state="new">Capture and archive real-time events for future processing</target>
          </trans-unit>
          <trans-unit id="113" xml:space="preserve">
            <source>For more information, see <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>Introduction to Azure Stream Analytics<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept>.</source>
            <target state="new">For more information, see <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>Introduction to Azure Stream Analytics<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept>.</target>
          </trans-unit>
          <trans-unit id="114" xml:space="preserve">
            <source>A Stream Analytics job includes all of the following:</source>
            <target state="new">A Stream Analytics job includes all of the following:</target>
          </trans-unit>
          <trans-unit id="115" xml:space="preserve">
            <source>One or more input sources</source>
            <target state="new">One or more input sources</target>
          </trans-unit>
          <trans-unit id="116" xml:space="preserve">
            <source>A query over an incoming data stream</source>
            <target state="new">A query over an incoming data stream</target>
          </trans-unit>
          <trans-unit id="117" xml:space="preserve">
            <source>An output target.</source>
            <target state="new">An output target.</target>
          </trans-unit>
          <trans-unit id="118" xml:space="preserve">
            <source>Inputs</source>
            <target state="new">Inputs</target>
          </trans-unit>
          <trans-unit id="119" xml:space="preserve">
            <source>Data stream</source>
            <target state="new">Data stream</target>
          </trans-unit>
          <trans-unit id="120" xml:space="preserve">
            <source>Each Stream Analytics job definition must contain at least one data stream input source to be consumed and transformed by the job.</source>
            <target state="new">Each Stream Analytics job definition must contain at least one data stream input source to be consumed and transformed by the job.</target>
          </trans-unit>
          <trans-unit id="121" xml:space="preserve">
            <source><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>Azure Blob storage<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept> and <bpt id="3CapsExtId1">&lt;link&gt;</bpt><bpt id="3CapsExtId2">&lt;linkText&gt;</bpt>Azure Event Hubs<ept id="3CapsExtId2">&lt;/linkText&gt;</ept><bpt id="3CapsExtId3">&lt;title&gt;</bpt><ept id="3CapsExtId3">&lt;/title&gt;</ept><ept id="3CapsExtId1">&lt;/link&gt;</ept> are supported as data stream input sources.</source>
            <target state="new"><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>Azure Blob storage<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept> and <bpt id="3CapsExtId1">&lt;link&gt;</bpt><bpt id="3CapsExtId2">&lt;linkText&gt;</bpt>Azure Event Hubs<ept id="3CapsExtId2">&lt;/linkText&gt;</ept><bpt id="3CapsExtId3">&lt;title&gt;</bpt><ept id="3CapsExtId3">&lt;/title&gt;</ept><ept id="3CapsExtId1">&lt;/link&gt;</ept> are supported as data stream input sources.</target>
          </trans-unit>
          <trans-unit id="122" xml:space="preserve">
            <source>Event Hubs input sources are used to collect event streams from multiple different devices and services, while Blob storage can be used an input source for ingesting large amounts of data.</source>
            <target state="new">Event Hubs input sources are used to collect event streams from multiple different devices and services, while Blob storage can be used an input source for ingesting large amounts of data.</target>
          </trans-unit>
          <trans-unit id="123" xml:space="preserve">
            <source>Because blobs do not stream data, Stream Analytics jobs over blobs will not be temporal in nature unless the records in the blob contain timestamps.</source>
            <target state="new">Because blobs do not stream data, Stream Analytics jobs over blobs will not be temporal in nature unless the records in the blob contain timestamps.</target>
          </trans-unit>
          <trans-unit id="124" xml:space="preserve">
            <source>Reference data</source>
            <target state="new">Reference data</target>
          </trans-unit>
          <trans-unit id="125" xml:space="preserve">
            <source>Stream Analytics also supports a second type of input source: reference data.</source>
            <target state="new">Stream Analytics also supports a second type of input source: reference data.</target>
          </trans-unit>
          <trans-unit id="126" xml:space="preserve">
            <source>This is auxiliary data used for performing correlation and lookups, and the data here is usually static or infrequently changing.</source>
            <target state="new">This is auxiliary data used for performing correlation and lookups, and the data here is usually static or infrequently changing.</target>
          </trans-unit>
          <trans-unit id="127" xml:space="preserve">
            <source><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>Azure Blob storage<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept> is the only supported input source for reference data.</source>
            <target state="new"><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>Azure Blob storage<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept> is the only supported input source for reference data.</target>
          </trans-unit>
          <trans-unit id="128" xml:space="preserve">
            <source>Reference data source blobs are limited to 50MB in size.</source>
            <target state="new">Reference data source blobs are limited to 50MB in size.</target>
          </trans-unit>
          <trans-unit id="129" xml:space="preserve">
            <source>To enable support for refreshing reference data the user needs to specify a list of blobs in the input configuration using the {date} and {time} tokens inside the path pattern.</source>
            <target state="new">To enable support for refreshing reference data the user needs to specify a list of blobs in the input configuration using the {date} and {time} tokens inside the path pattern.</target>
          </trans-unit>
          <trans-unit id="130" xml:space="preserve">
            <source>The job will load the corresponding blob based on the date and time encoded in the blob names using UTC time zone.</source>
            <target state="new">The job will load the corresponding blob based on the date and time encoded in the blob names using UTC time zone.</target>
          </trans-unit>
          <trans-unit id="131" xml:space="preserve">
            <source>For example if the job has a reference input configured in the portal with the path pattern such as: /sample/{date}/{time}/products.csv where the date format is “YYYY-MM-DD” and the time format is “HH:mm” than the job will pick up a file named /sample/2015-04-16/17:30/products.csv at 5:30 PM on April 16th 2015 UTC time zone (which is equivalent to 10:30 AM on April 16th 2015 using PST time zone).</source>
            <target state="new">For example if the job has a reference input configured in the portal with the path pattern such as: /sample/{date}/{time}/products.csv where the date format is “YYYY-MM-DD” and the time format is “HH:mm” than the job will pick up a file named /sample/2015-04-16/17:30/products.csv at 5:30 PM on April 16th 2015 UTC time zone (which is equivalent to 10:30 AM on April 16th 2015 using PST time zone).</target>
          </trans-unit>
          <trans-unit id="132" xml:space="preserve">
            <source>Serialization</source>
            <target state="new">Serialization</target>
          </trans-unit>
          <trans-unit id="133" xml:space="preserve">
            <source>To ensure correct behavior of queries, Stream Analytics must be aware of the serialization format being used on incoming data streams.</source>
            <target state="new">To ensure correct behavior of queries, Stream Analytics must be aware of the serialization format being used on incoming data streams.</target>
          </trans-unit>
          <trans-unit id="134" xml:space="preserve">
            <source>Currently supported serialization formats are JSON, CSV, and Avro for data streams and CSV or JSON for reference data.</source>
            <target state="new">Currently supported serialization formats are JSON, CSV, and Avro for data streams and CSV or JSON for reference data.</target>
          </trans-unit>
          <trans-unit id="135" xml:space="preserve">
            <source>Generated properties</source>
            <target state="new">Generated properties</target>
          </trans-unit>
          <trans-unit id="136" xml:space="preserve">
            <source>Depending on the input type used in the job, some additional fields with event metadata will be generated.</source>
            <target state="new">Depending on the input type used in the job, some additional fields with event metadata will be generated.</target>
          </trans-unit>
          <trans-unit id="137" xml:space="preserve">
            <source>These fields can be queried against just like other input columns.</source>
            <target state="new">These fields can be queried against just like other input columns.</target>
          </trans-unit>
          <trans-unit id="138" xml:space="preserve">
            <source>If an existing event has a field that has the same name as one of the properties below, it will be overwritten with the input metadata.</source>
            <target state="new">If an existing event has a field that has the same name as one of the properties below, it will be overwritten with the input metadata.</target>
          </trans-unit>
          <trans-unit id="139" xml:space="preserve">
            <source>Property</source>
            <target state="new">Property</target>
          </trans-unit>
          <trans-unit id="140" xml:space="preserve">
            <source>Description</source>
            <target state="new">Description</target>
          </trans-unit>
          <trans-unit id="141" xml:space="preserve">
            <source>Blob</source>
            <target state="new">Blob</target>
          </trans-unit>
          <trans-unit id="142" xml:space="preserve">
            <source>BlobName</source>
            <target state="new">BlobName</target>
          </trans-unit>
          <trans-unit id="143" xml:space="preserve">
            <source>The name of the input blob that the event came from.</source>
            <target state="new">The name of the input blob that the event came from.</target>
          </trans-unit>
          <trans-unit id="144" xml:space="preserve">
            <source>EventProcessedUtcTime</source>
            <target state="new">EventProcessedUtcTime</target>
          </trans-unit>
          <trans-unit id="145" xml:space="preserve">
            <source>The date and time that the blob record was processed.</source>
            <target state="new">The date and time that the blob record was processed.</target>
          </trans-unit>
          <trans-unit id="146" xml:space="preserve">
            <source>BlobLastModifiedUtcTime</source>
            <target state="new">BlobLastModifiedUtcTime</target>
          </trans-unit>
          <trans-unit id="147" xml:space="preserve">
            <source>The date and time that the blob was last modified.</source>
            <target state="new">The date and time that the blob was last modified.</target>
          </trans-unit>
          <trans-unit id="148" xml:space="preserve">
            <source>PartitionId</source>
            <target state="new">PartitionId</target>
          </trans-unit>
          <trans-unit id="149" xml:space="preserve">
            <source>The zero-based partition ID for the input adapter.</source>
            <target state="new">The zero-based partition ID for the input adapter.</target>
          </trans-unit>
          <trans-unit id="150" xml:space="preserve">
            <source>Event Hub</source>
            <target state="new">Event Hub</target>
          </trans-unit>
          <trans-unit id="151" xml:space="preserve">
            <source>EventProcessedUtcTime</source>
            <target state="new">EventProcessedUtcTime</target>
          </trans-unit>
          <trans-unit id="152" xml:space="preserve">
            <source>The date and time that the event was processed.</source>
            <target state="new">The date and time that the event was processed.</target>
          </trans-unit>
          <trans-unit id="153" xml:space="preserve">
            <source>EventEnqueuedUtcTime</source>
            <target state="new">EventEnqueuedUtcTime</target>
          </trans-unit>
          <trans-unit id="154" xml:space="preserve">
            <source>The date and time that the event was received by Event Hubs.</source>
            <target state="new">The date and time that the event was received by Event Hubs.</target>
          </trans-unit>
          <trans-unit id="155" xml:space="preserve">
            <source>PartitionId</source>
            <target state="new">PartitionId</target>
          </trans-unit>
          <trans-unit id="156" xml:space="preserve">
            <source>The zero-based partition ID for the input adapter.</source>
            <target state="new">The zero-based partition ID for the input adapter.</target>
          </trans-unit>
          <trans-unit id="157" xml:space="preserve">
            <source>Partition(s) with slow or no input data</source>
            <target state="new">Partition(s) with slow or no input data</target>
          </trans-unit>
          <trans-unit id="158" xml:space="preserve">
            <source>When reading from input sources that have multiple partitions, and one or more partitions lag behind or do not have data, the streaming job needs to decide how to handle this situation in order to keep events flowing through the system.</source>
            <target state="new">When reading from input sources that have multiple partitions, and one or more partitions lag behind or do not have data, the streaming job needs to decide how to handle this situation in order to keep events flowing through the system.</target>
          </trans-unit>
          <trans-unit id="159" xml:space="preserve">
            <source>Input setting ‘Maximum allowed arrival delay’ controls that behavior and is set by default to wait for the data indefinitely, which means events’ timestamps will not be altered, but also that events will flow based on the slowest input partition, and will stop flowing if one or more input partitions do not have data.</source>
            <target state="new">Input setting ‘Maximum allowed arrival delay’ controls that behavior and is set by default to wait for the data indefinitely, which means events’ timestamps will not be altered, but also that events will flow based on the slowest input partition, and will stop flowing if one or more input partitions do not have data.</target>
          </trans-unit>
          <trans-unit id="160" xml:space="preserve">
            <source>This is useful if the data is distributed uniformly across input partitions, and time consistency among events is critical.</source>
            <target state="new">This is useful if the data is distributed uniformly across input partitions, and time consistency among events is critical.</target>
          </trans-unit>
          <trans-unit id="161" xml:space="preserve">
            <source>You can also decide to wait for only a limited time: ‘Maximum allowed arrival delay’ determines the delay after which the job will decide to move forward, leaving the lagging input partitions behind, and acting on events according to ‘Action for late events’ setting, dropping their events or adjusting their events’ timestamps if data arrives later.</source>
            <target state="new">You can also decide to wait for only a limited time: ‘Maximum allowed arrival delay’ determines the delay after which the job will decide to move forward, leaving the lagging input partitions behind, and acting on events according to ‘Action for late events’ setting, dropping their events or adjusting their events’ timestamps if data arrives later.</target>
          </trans-unit>
          <trans-unit id="162" xml:space="preserve">
            <source>This is useful if latency is critical and timestamp shift is tolerated, but input may not be uniformly distributed.</source>
            <target state="new">This is useful if latency is critical and timestamp shift is tolerated, but input may not be uniformly distributed.</target>
          </trans-unit>
          <trans-unit id="163" xml:space="preserve">
            <source>Partition(s) with out of order events</source>
            <target state="new">Partition(s) with out of order events</target>
          </trans-unit>
          <trans-unit id="164" xml:space="preserve">
            <source>When streaming job query uses the TIMESTAMP BY keyword, there are no guarantees about the order in which the events will arrive to input, Some events in the same input partition may be lagging, parameter ‘Maximum allowed disorder within an input’ causes the streaming job to act on events that are outside of the order tolerance, according to ‘Action for late events’ setting, dropping their events or adjusting their events’ timestamps.</source>
            <target state="new">When streaming job query uses the TIMESTAMP BY keyword, there are no guarantees about the order in which the events will arrive to input, Some events in the same input partition may be lagging, parameter ‘Maximum allowed disorder within an input’ causes the streaming job to act on events that are outside of the order tolerance, according to ‘Action for late events’ setting, dropping their events or adjusting their events’ timestamps.</target>
          </trans-unit>
          <trans-unit id="165" xml:space="preserve">
            <source>Additional resources</source>
            <target state="new">Additional resources</target>
          </trans-unit>
          <trans-unit id="166" xml:space="preserve">
            <source>For details on creating input sources, see <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>Azure Event Hubs developer guide<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept> and <bpt id="4CapsExtId1">&lt;link&gt;</bpt><bpt id="4CapsExtId2">&lt;linkText&gt;</bpt>Use Azure Blob Storage<ept id="4CapsExtId2">&lt;/linkText&gt;</ept><bpt id="4CapsExtId3">&lt;title&gt;</bpt><ept id="4CapsExtId3">&lt;/title&gt;</ept><ept id="4CapsExtId1">&lt;/link&gt;</ept>.</source>
            <target state="new">For details on creating input sources, see <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>Azure Event Hubs developer guide<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept> and <bpt id="4CapsExtId1">&lt;link&gt;</bpt><bpt id="4CapsExtId2">&lt;linkText&gt;</bpt>Use Azure Blob Storage<ept id="4CapsExtId2">&lt;/linkText&gt;</ept><bpt id="4CapsExtId3">&lt;title&gt;</bpt><ept id="4CapsExtId3">&lt;/title&gt;</ept><ept id="4CapsExtId1">&lt;/link&gt;</ept>.</target>
          </trans-unit>
          <trans-unit id="167" xml:space="preserve">
            <source>Query</source>
            <target state="new">Query</target>
          </trans-unit>
          <trans-unit id="168" xml:space="preserve">
            <source>The logic to filter, manipulate, and process incoming data is defined in the query of Stream Analytics jobs.</source>
            <target state="new">The logic to filter, manipulate, and process incoming data is defined in the query of Stream Analytics jobs.</target>
          </trans-unit>
          <trans-unit id="169" xml:space="preserve">
            <source>Queries are written in the Stream Analytics query language, an SQL-like language that is largely a subset of standard Transact-SQL syntax with some specific extensions for temporal queries.</source>
            <target state="new">Queries are written in the Stream Analytics query language, an SQL-like language that is largely a subset of standard Transact-SQL syntax with some specific extensions for temporal queries.</target>
          </trans-unit>
          <trans-unit id="170" xml:space="preserve">
            <source>Windowing</source>
            <target state="new">Windowing</target>
          </trans-unit>
          <trans-unit id="171" xml:space="preserve">
            <source>Windowing extensions allow aggregations and computations to be performed over subsets of events that fall within some period of time.</source>
            <target state="new">Windowing extensions allow aggregations and computations to be performed over subsets of events that fall within some period of time.</target>
          </trans-unit>
          <trans-unit id="172" xml:space="preserve">
            <source>Windowing functions are invoked through the <bpt id="2">&lt;strong&gt;</bpt>GROUP BY<ept id="2">&lt;/strong&gt;</ept> statement.</source>
            <target state="new">Windowing functions are invoked through the <bpt id="2">&lt;strong&gt;</bpt>GROUP BY<ept id="2">&lt;/strong&gt;</ept> statement.</target>
          </trans-unit>
          <trans-unit id="173" xml:space="preserve">
            <source>For example, the following query counts the events received per second:</source>
            <target state="new">For example, the following query counts the events received per second:</target>
          </trans-unit>
          <trans-unit id="174" xml:space="preserve">
            <source>Execution steps</source>
            <target state="new">Execution steps</target>
          </trans-unit>
          <trans-unit id="175" xml:space="preserve">
            <source>For more complex queries, the standard SQL clause <bpt id="2">&lt;strong&gt;</bpt>WITH<ept id="2">&lt;/strong&gt;</ept> can be used to specify a temporary named result set.</source>
            <target state="new">For more complex queries, the standard SQL clause <bpt id="2">&lt;strong&gt;</bpt>WITH<ept id="2">&lt;/strong&gt;</ept> can be used to specify a temporary named result set.</target>
          </trans-unit>
          <trans-unit id="176" xml:space="preserve">
            <source>For example, this query uses <bpt id="2">&lt;strong&gt;</bpt>WITH<ept id="2">&lt;/strong&gt;</ept> to perform a transformation with two execution steps:</source>
            <target state="new">For example, this query uses <bpt id="2">&lt;strong&gt;</bpt>WITH<ept id="2">&lt;/strong&gt;</ept> to perform a transformation with two execution steps:</target>
          </trans-unit>
          <trans-unit id="177" xml:space="preserve">
            <source>To learn more about the query language, see <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>Azure Stream Analytics Query Language Reference<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept>.</source>
            <target state="new">To learn more about the query language, see <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>Azure Stream Analytics Query Language Reference<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept>.</target>
          </trans-unit>
          <trans-unit id="178" xml:space="preserve">
            <source>Output</source>
            <target state="new">Output</target>
          </trans-unit>
          <trans-unit id="179" xml:space="preserve">
            <source>The output target is where the results of the Stream Analytics job will be written to.</source>
            <target state="new">The output target is where the results of the Stream Analytics job will be written to.</target>
          </trans-unit>
          <trans-unit id="180" xml:space="preserve">
            <source>Results are written continuously to the output target as the job processes input events.</source>
            <target state="new">Results are written continuously to the output target as the job processes input events.</target>
          </trans-unit>
          <trans-unit id="181" xml:space="preserve">
            <source>The following output targets are supported:</source>
            <target state="new">The following output targets are supported:</target>
          </trans-unit>
          <trans-unit id="182" xml:space="preserve">
            <source>Azure Event Hubs - Choose Event Hubs as an output target for scenarios when multiple streaming pipelines need to be composed together, such as issuing commands back to devices.</source>
            <target state="new">Azure Event Hubs - Choose Event Hubs as an output target for scenarios when multiple streaming pipelines need to be composed together, such as issuing commands back to devices.</target>
          </trans-unit>
          <trans-unit id="183" xml:space="preserve">
            <source>Azure Blob storage - Use Blob storage for long-term archival of output or for storing data for later processing.</source>
            <target state="new">Azure Blob storage - Use Blob storage for long-term archival of output or for storing data for later processing.</target>
          </trans-unit>
          <trans-unit id="184" xml:space="preserve">
            <source>Azure Table storage - Azure Table storage is a structured data store with fewer constraints on the schema.</source>
            <target state="new">Azure Table storage - Azure Table storage is a structured data store with fewer constraints on the schema.</target>
          </trans-unit>
          <trans-unit id="185" xml:space="preserve">
            <source>Entities with different schema and different types can be stored in the same Azure table.</source>
            <target state="new">Entities with different schema and different types can be stored in the same Azure table.</target>
          </trans-unit>
          <trans-unit id="186" xml:space="preserve">
            <source>Azure Table storage can be used to store data for persistence and efficient retrieval.</source>
            <target state="new">Azure Table storage can be used to store data for persistence and efficient retrieval.</target>
          </trans-unit>
          <trans-unit id="187" xml:space="preserve">
            <source>For more information, see <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>Introduction to Azure Storage<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept> and <bpt id="4CapsExtId1">&lt;link&gt;</bpt><bpt id="4CapsExtId2">&lt;linkText&gt;</bpt>Designing a Scalable Partitioning Strategy for Azure Table Storage<ept id="4CapsExtId2">&lt;/linkText&gt;</ept><bpt id="4CapsExtId3">&lt;title&gt;</bpt><ept id="4CapsExtId3">&lt;/title&gt;</ept><ept id="4CapsExtId1">&lt;/link&gt;</ept>.</source>
            <target state="new">For more information, see <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>Introduction to Azure Storage<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept> and <bpt id="4CapsExtId1">&lt;link&gt;</bpt><bpt id="4CapsExtId2">&lt;linkText&gt;</bpt>Designing a Scalable Partitioning Strategy for Azure Table Storage<ept id="4CapsExtId2">&lt;/linkText&gt;</ept><bpt id="4CapsExtId3">&lt;title&gt;</bpt><ept id="4CapsExtId3">&lt;/title&gt;</ept><ept id="4CapsExtId1">&lt;/link&gt;</ept>.</target>
          </trans-unit>
          <trans-unit id="188" xml:space="preserve">
            <source>Azure SQL Database - This output target is appropriate for data that is relational in nature or for applications that depend on content being hosted in a database.</source>
            <target state="new">Azure SQL Database - This output target is appropriate for data that is relational in nature or for applications that depend on content being hosted in a database.</target>
          </trans-unit>
          <trans-unit id="189" xml:space="preserve">
            <source>Streaming Units</source>
            <target state="new">Streaming Units</target>
          </trans-unit>
          <trans-unit id="190" xml:space="preserve">
            <source>As part of providing a more predictable performance experience for customers, Azure Stream Analytics uses Streaming Units (SUs) to represent the resources and power to execute a job.</source>
            <target state="new">As part of providing a more predictable performance experience for customers, Azure Stream Analytics uses Streaming Units (SUs) to represent the resources and power to execute a job.</target>
          </trans-unit>
          <trans-unit id="191" xml:space="preserve">
            <source>SUs provide a way to describe the relative event processing capacity based on a blended measure of CPU, memory, and read and write rates.</source>
            <target state="new">SUs provide a way to describe the relative event processing capacity based on a blended measure of CPU, memory, and read and write rates.</target>
          </trans-unit>
          <trans-unit id="192" xml:space="preserve">
            <source>Each streaming unit corresponds to roughly 1MB/second of throughput.</source>
            <target state="new">Each streaming unit corresponds to roughly 1MB/second of throughput.</target>
          </trans-unit>
          <trans-unit id="193" xml:space="preserve">
            <source>Each Azure Stream Analytics job needs a minimum of one streaming unit, which is the default for all jobs.</source>
            <target state="new">Each Azure Stream Analytics job needs a minimum of one streaming unit, which is the default for all jobs.</target>
          </trans-unit>
          <trans-unit id="194" xml:space="preserve">
            <source>To learn more about selecting the right number of SU’s for a job, see <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>Scale Azure Stream Analytics jobs<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept></source>
            <target state="new">To learn more about selecting the right number of SU’s for a job, see <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>Scale Azure Stream Analytics jobs<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept></target>
          </trans-unit>
          <trans-unit id="195" xml:space="preserve">
            <source>Scale jobs</source>
            <target state="new">Scale jobs</target>
          </trans-unit>
          <trans-unit id="196" xml:space="preserve">
            <source>The SU % Utilization metric defined below, is an indicator for the need to scale an Azure Stream Analytics job.</source>
            <target state="new">The SU % Utilization metric defined below, is an indicator for the need to scale an Azure Stream Analytics job.</target>
          </trans-unit>
          <trans-unit id="197" xml:space="preserve">
            <source>High SU % Utilization may be a result of large window in a query, large events in input, large out of order tolerance window, or a combination of the above.</source>
            <target state="new">High SU % Utilization may be a result of large window in a query, large events in input, large out of order tolerance window, or a combination of the above.</target>
          </trans-unit>
          <trans-unit id="198" xml:space="preserve">
            <source>Partitioning the query, or breaking down the query into more steps, and adding more SUs from the Scale tab are both strategies to avoid such a condition.</source>
            <target state="new">Partitioning the query, or breaking down the query into more steps, and adding more SUs from the Scale tab are both strategies to avoid such a condition.</target>
          </trans-unit>
          <trans-unit id="199" xml:space="preserve">
            <source>You may observe a baseline resource utilization even without input events, because the system consumes certain amount of resource.</source>
            <target state="new">You may observe a baseline resource utilization even without input events, because the system consumes certain amount of resource.</target>
          </trans-unit>
          <trans-unit id="200" xml:space="preserve">
            <source>The amount of resource consumed by the system may also fluctuate over time.</source>
            <target state="new">The amount of resource consumed by the system may also fluctuate over time.</target>
          </trans-unit>
          <trans-unit id="201" xml:space="preserve">
            <source>For details, see <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>Scale Azure Stream Analytics jobs<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept>.</source>
            <target state="new">For details, see <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>Scale Azure Stream Analytics jobs<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept>.</target>
          </trans-unit>
          <trans-unit id="202" xml:space="preserve">
            <source>Monitor and troubleshoot jobs</source>
            <target state="new">Monitor and troubleshoot jobs</target>
          </trans-unit>
          <trans-unit id="203" xml:space="preserve">
            <source>Regional monitoring Storage account</source>
            <target state="new">Regional monitoring Storage account</target>
          </trans-unit>
          <trans-unit id="204" xml:space="preserve">
            <source>To enable job monitoring, Stream Analytics requires you to designate an Azure Storage account for monitoring data in each region that contains Stream Analytics jobs.</source>
            <target state="new">To enable job monitoring, Stream Analytics requires you to designate an Azure Storage account for monitoring data in each region that contains Stream Analytics jobs.</target>
          </trans-unit>
          <trans-unit id="205" xml:space="preserve">
            <source>This is configured at the time of job creation.</source>
            <target state="new">This is configured at the time of job creation.</target>
          </trans-unit>
          <trans-unit id="206" xml:space="preserve">
            <source>Metrics</source>
            <target state="new">Metrics</target>
          </trans-unit>
          <trans-unit id="207" xml:space="preserve">
            <source>The following metrics are available for monitoring the usage and performance of Stream Analytics jobs:</source>
            <target state="new">The following metrics are available for monitoring the usage and performance of Stream Analytics jobs:</target>
          </trans-unit>
          <trans-unit id="208" xml:space="preserve">
            <source>SU % Utilizaiton - An indicator of the relative event processing capacity for one or more of the query steps.</source>
            <target state="new">SU % Utilizaiton - An indicator of the relative event processing capacity for one or more of the query steps.</target>
          </trans-unit>
          <trans-unit id="209" xml:space="preserve">
            <source>Should this indicator reach 80%, or above, there is high probability that event processing may be delayed or stopped making progress.</source>
            <target state="new">Should this indicator reach 80%, or above, there is high probability that event processing may be delayed or stopped making progress.</target>
          </trans-unit>
          <trans-unit id="210" xml:space="preserve">
            <source>Errors - Number of error messages incurred by a Stream Analytics job.</source>
            <target state="new">Errors - Number of error messages incurred by a Stream Analytics job.</target>
          </trans-unit>
          <trans-unit id="211" xml:space="preserve">
            <source>Input events - Amount of data received by the Stream Analytics job, in terms of event count.</source>
            <target state="new">Input events - Amount of data received by the Stream Analytics job, in terms of event count.</target>
          </trans-unit>
          <trans-unit id="212" xml:space="preserve">
            <source>Output events - Amount of data sent by the Stream Analytics job to the output target, in terms of event count.</source>
            <target state="new">Output events - Amount of data sent by the Stream Analytics job to the output target, in terms of event count.</target>
          </trans-unit>
          <trans-unit id="213" xml:space="preserve">
            <source>Out-of-order events - Number of events received out of order that were either dropped or given an adjusted timestamp, based on the out-of-order policy.</source>
            <target state="new">Out-of-order events - Number of events received out of order that were either dropped or given an adjusted timestamp, based on the out-of-order policy.</target>
          </trans-unit>
          <trans-unit id="214" xml:space="preserve">
            <source>Data conversion errors - Number of data conversion errors incurred by a Stream Analytics job.</source>
            <target state="new">Data conversion errors - Number of data conversion errors incurred by a Stream Analytics job.</target>
          </trans-unit>
          <trans-unit id="215" xml:space="preserve">
            <source>Operation logs</source>
            <target state="new">Operation logs</target>
          </trans-unit>
          <trans-unit id="216" xml:space="preserve">
            <source>The best approach to debugging or troubleshooting a Stream Analytics job is through Azure operation logs.</source>
            <target state="new">The best approach to debugging or troubleshooting a Stream Analytics job is through Azure operation logs.</target>
          </trans-unit>
          <trans-unit id="217" xml:space="preserve">
            <source>Operation logs can be accessed in the <bpt id="2">&lt;strong&gt;</bpt>Management Services<ept id="2">&lt;/strong&gt;</ept> section of the portal.</source>
            <target state="new">Operation logs can be accessed in the <bpt id="2">&lt;strong&gt;</bpt>Management Services<ept id="2">&lt;/strong&gt;</ept> section of the portal.</target>
          </trans-unit>
          <trans-unit id="218" xml:space="preserve">
            <source>To inspect logs for your job, set <bpt id="2">&lt;strong&gt;</bpt>Service Type<ept id="2">&lt;/strong&gt;</ept> to <bpt id="4">&lt;strong&gt;</bpt>Stream Analytics<ept id="4">&lt;/strong&gt;</ept> and <bpt id="6">&lt;strong&gt;</bpt>Service Name<ept id="6">&lt;/strong&gt;</ept> to the name of your job.</source>
            <target state="new">To inspect logs for your job, set <bpt id="2">&lt;strong&gt;</bpt>Service Type<ept id="2">&lt;/strong&gt;</ept> to <bpt id="4">&lt;strong&gt;</bpt>Stream Analytics<ept id="4">&lt;/strong&gt;</ept> and <bpt id="6">&lt;strong&gt;</bpt>Service Name<ept id="6">&lt;/strong&gt;</ept> to the name of your job.</target>
          </trans-unit>
          <trans-unit id="219" xml:space="preserve">
            <source>Manage jobs</source>
            <target state="new">Manage jobs</target>
          </trans-unit>
          <trans-unit id="220" xml:space="preserve">
            <source>Start and stop jobs</source>
            <target state="new">Start and stop jobs</target>
          </trans-unit>
          <trans-unit id="221" xml:space="preserve">
            <source>When starting a job, you're prompted to specify a <bpt id="2">&lt;strong&gt;</bpt>Start Output<ept id="2">&lt;/strong&gt;</ept> value, which determines when this job will start producing resulting output.</source>
            <target state="new">When starting a job, you're prompted to specify a <bpt id="2">&lt;strong&gt;</bpt>Start Output<ept id="2">&lt;/strong&gt;</ept> value, which determines when this job will start producing resulting output.</target>
          </trans-unit>
          <trans-unit id="222" xml:space="preserve">
            <source>If the associated query includes a window, the job will begin picking up input from the input sources at the start of the window duration required, in order to produce the first output event at the specified time.</source>
            <target state="new">If the associated query includes a window, the job will begin picking up input from the input sources at the start of the window duration required, in order to produce the first output event at the specified time.</target>
          </trans-unit>
          <trans-unit id="223" xml:space="preserve">
            <source>There are three options: <bpt id="2">&lt;strong&gt;</bpt>Job Start Time<ept id="2">&lt;/strong&gt;</ept>, <bpt id="4">&lt;strong&gt;</bpt>Custom<ept id="4">&lt;/strong&gt;</ept>, and <bpt id="6">&lt;strong&gt;</bpt>Last Stopped Time<ept id="6">&lt;/strong&gt;</ept>.</source>
            <target state="new">There are three options: <bpt id="2">&lt;strong&gt;</bpt>Job Start Time<ept id="2">&lt;/strong&gt;</ept>, <bpt id="4">&lt;strong&gt;</bpt>Custom<ept id="4">&lt;/strong&gt;</ept>, and <bpt id="6">&lt;strong&gt;</bpt>Last Stopped Time<ept id="6">&lt;/strong&gt;</ept>.</target>
          </trans-unit>
          <trans-unit id="224" xml:space="preserve">
            <source>The default setting is <bpt id="2">&lt;strong&gt;</bpt>Job Start Time<ept id="2">&lt;/strong&gt;</ept>.</source>
            <target state="new">The default setting is <bpt id="2">&lt;strong&gt;</bpt>Job Start Time<ept id="2">&lt;/strong&gt;</ept>.</target>
          </trans-unit>
          <trans-unit id="225" xml:space="preserve">
            <source>For cases when a job has been stopped temporarily, the best practice is to choose <bpt id="2">&lt;strong&gt;</bpt>Last Stopped Time<ept id="2">&lt;/strong&gt;</ept> for the <bpt id="4">&lt;strong&gt;</bpt>Start Output<ept id="4">&lt;/strong&gt;</ept> value in order to resume the job from the last output time and avoid data loss.</source>
            <target state="new">For cases when a job has been stopped temporarily, the best practice is to choose <bpt id="2">&lt;strong&gt;</bpt>Last Stopped Time<ept id="2">&lt;/strong&gt;</ept> for the <bpt id="4">&lt;strong&gt;</bpt>Start Output<ept id="4">&lt;/strong&gt;</ept> value in order to resume the job from the last output time and avoid data loss.</target>
          </trans-unit>
          <trans-unit id="226" xml:space="preserve">
            <source>For the <bpt id="2">&lt;strong&gt;</bpt>Custom<ept id="2">&lt;/strong&gt;</ept> option, you must specify a date and time.</source>
            <target state="new">For the <bpt id="2">&lt;strong&gt;</bpt>Custom<ept id="2">&lt;/strong&gt;</ept> option, you must specify a date and time.</target>
          </trans-unit>
          <trans-unit id="227" xml:space="preserve">
            <source>This setting is useful for specifying how much historical data in the input sources to consume or for picking up data ingestion from a specific time.</source>
            <target state="new">This setting is useful for specifying how much historical data in the input sources to consume or for picking up data ingestion from a specific time.</target>
          </trans-unit>
          <trans-unit id="228" xml:space="preserve">
            <source>Configure jobs</source>
            <target state="new">Configure jobs</target>
          </trans-unit>
          <trans-unit id="229" xml:space="preserve">
            <source>You can adjust the following top-level settings for a Stream Analytics job:</source>
            <target state="new">You can adjust the following top-level settings for a Stream Analytics job:</target>
          </trans-unit>
          <trans-unit id="230" xml:space="preserve">
            <source><bpt id="1">&lt;strong&gt;</bpt>Start output<ept id="1">&lt;/strong&gt;</ept> - Use this setting to specify when this job will start producing resulting output.</source>
            <target state="new"><bpt id="1">&lt;strong&gt;</bpt>Start output<ept id="1">&lt;/strong&gt;</ept> - Use this setting to specify when this job will start producing resulting output.</target>
          </trans-unit>
          <trans-unit id="231" xml:space="preserve">
            <source>If the associated query includes a window, the job will begin picking up input from the input sources at the start of the window duration required in order to produce the first output event at the specified time.</source>
            <target state="new">If the associated query includes a window, the job will begin picking up input from the input sources at the start of the window duration required in order to produce the first output event at the specified time.</target>
          </trans-unit>
          <trans-unit id="232" xml:space="preserve">
            <source>There are two options, <bpt id="2">&lt;strong&gt;</bpt>Job Start Time<ept id="2">&lt;/strong&gt;</ept> and <bpt id="4">&lt;strong&gt;</bpt>Custom<ept id="4">&lt;/strong&gt;</ept>.</source>
            <target state="new">There are two options, <bpt id="2">&lt;strong&gt;</bpt>Job Start Time<ept id="2">&lt;/strong&gt;</ept> and <bpt id="4">&lt;strong&gt;</bpt>Custom<ept id="4">&lt;/strong&gt;</ept>.</target>
          </trans-unit>
          <trans-unit id="233" xml:space="preserve">
            <source>The default setting is <bpt id="2">&lt;strong&gt;</bpt>Job Start Time<ept id="2">&lt;/strong&gt;</ept>.</source>
            <target state="new">The default setting is <bpt id="2">&lt;strong&gt;</bpt>Job Start Time<ept id="2">&lt;/strong&gt;</ept>.</target>
          </trans-unit>
          <trans-unit id="234" xml:space="preserve">
            <source>For the <bpt id="2">&lt;strong&gt;</bpt>Custom<ept id="2">&lt;/strong&gt;</ept> option, you must specify a date and time.</source>
            <target state="new">For the <bpt id="2">&lt;strong&gt;</bpt>Custom<ept id="2">&lt;/strong&gt;</ept> option, you must specify a date and time.</target>
          </trans-unit>
          <trans-unit id="235" xml:space="preserve">
            <source>This setting is useful for specifying how much historical data in the input sources to consume or for picking up data ingestion from a specific time, such as when a job was last stopped.</source>
            <target state="new">This setting is useful for specifying how much historical data in the input sources to consume or for picking up data ingestion from a specific time, such as when a job was last stopped.</target>
          </trans-unit>
          <trans-unit id="236" xml:space="preserve">
            <source><bpt id="1">&lt;strong&gt;</bpt>Out of order policy<ept id="1">&lt;/strong&gt;</ept> - Settings for handling events that do not arrive to the Stream Analytics job sequentially.</source>
            <target state="new"><bpt id="1">&lt;strong&gt;</bpt>Out of order policy<ept id="1">&lt;/strong&gt;</ept> - Settings for handling events that do not arrive to the Stream Analytics job sequentially.</target>
          </trans-unit>
          <trans-unit id="237" xml:space="preserve">
            <source>You can designate a time threshold to reorder events within by specifying a tolerance window and also determine an action to take on events outside this window: <bpt id="2">&lt;strong&gt;</bpt>Drop<ept id="2">&lt;/strong&gt;</ept> or <bpt id="4">&lt;strong&gt;</bpt>Adjust<ept id="4">&lt;/strong&gt;</ept>.</source>
            <target state="new">You can designate a time threshold to reorder events within by specifying a tolerance window and also determine an action to take on events outside this window: <bpt id="2">&lt;strong&gt;</bpt>Drop<ept id="2">&lt;/strong&gt;</ept> or <bpt id="4">&lt;strong&gt;</bpt>Adjust<ept id="4">&lt;/strong&gt;</ept>.</target>
          </trans-unit>
          <trans-unit id="238" xml:space="preserve">
            <source><bpt id="1">&lt;strong&gt;</bpt>Drop<ept id="1">&lt;/strong&gt;</ept> will drop all events received out of order, and <bpt id="3">&lt;strong&gt;</bpt>Adjust<ept id="3">&lt;/strong&gt;</ept> will change the System.</source>
            <target state="new"><bpt id="1">&lt;strong&gt;</bpt>Drop<ept id="1">&lt;/strong&gt;</ept> will drop all events received out of order, and <bpt id="3">&lt;strong&gt;</bpt>Adjust<ept id="3">&lt;/strong&gt;</ept> will change the System.</target>
          </trans-unit>
          <trans-unit id="239" xml:space="preserve">
            <source>Timestamp of out-of-order events to the timestamp of the most recently received ordered event.</source>
            <target state="new">Timestamp of out-of-order events to the timestamp of the most recently received ordered event.</target>
          </trans-unit>
          <trans-unit id="240" xml:space="preserve">
            <source><bpt id="1">&lt;strong&gt;</bpt>Late arrival policy<ept id="1">&lt;/strong&gt;</ept> - When reading from input sources that have multiple partitions, and one or more partitions lag behind or do not have data, the streaming job needs to decide how to handle this situation in order to keep events flowing through the system.</source>
            <target state="new"><bpt id="1">&lt;strong&gt;</bpt>Late arrival policy<ept id="1">&lt;/strong&gt;</ept> - When reading from input sources that have multiple partitions, and one or more partitions lag behind or do not have data, the streaming job needs to decide how to handle this situation in order to keep events flowing through the system.</target>
          </trans-unit>
          <trans-unit id="241" xml:space="preserve">
            <source>Input setting ‘Maximum allowed arrival delay’ controls that behavior and is set by default to wait for the data indefinitely, which means events’ timestamps will not be altered, but also that events will flow based on the slowest input partition, and will stop flowing if one or more input partitions do not have data.</source>
            <target state="new">Input setting ‘Maximum allowed arrival delay’ controls that behavior and is set by default to wait for the data indefinitely, which means events’ timestamps will not be altered, but also that events will flow based on the slowest input partition, and will stop flowing if one or more input partitions do not have data.</target>
          </trans-unit>
          <trans-unit id="242" xml:space="preserve">
            <source>This is useful if the data is distributed uniformly across input partitions, and time consistency among events is critical.</source>
            <target state="new">This is useful if the data is distributed uniformly across input partitions, and time consistency among events is critical.</target>
          </trans-unit>
          <trans-unit id="243" xml:space="preserve">
            <source>User can also decide to only wait for a limited time, ‘Maximum allowed arrival delay’ determines the delay after which the job will decide to move forward, leaving the lagging input partitions behind, and acting on events according to ‘Action for late events’ setting, dropping their events or adjusting their events’ timestamps if data arrives later.</source>
            <target state="new">User can also decide to only wait for a limited time, ‘Maximum allowed arrival delay’ determines the delay after which the job will decide to move forward, leaving the lagging input partitions behind, and acting on events according to ‘Action for late events’ setting, dropping their events or adjusting their events’ timestamps if data arrives later.</target>
          </trans-unit>
          <trans-unit id="244" xml:space="preserve">
            <source>This is useful if latency is critical and timestamp shift is tolerated, but input may not be uniformly distributed.</source>
            <target state="new">This is useful if latency is critical and timestamp shift is tolerated, but input may not be uniformly distributed.</target>
          </trans-unit>
          <trans-unit id="245" xml:space="preserve">
            <source><bpt id="1">&lt;strong&gt;</bpt>Locale<ept id="1">&lt;/strong&gt;</ept> - Use this setting to specify the internationalization preference for the Stream Analytics job.</source>
            <target state="new"><bpt id="1">&lt;strong&gt;</bpt>Locale<ept id="1">&lt;/strong&gt;</ept> - Use this setting to specify the internationalization preference for the Stream Analytics job.</target>
          </trans-unit>
          <trans-unit id="246" xml:space="preserve">
            <source>While timestamps of data are locale neutral, settings here impact how the job will parse, compare, and sort data.</source>
            <target state="new">While timestamps of data are locale neutral, settings here impact how the job will parse, compare, and sort data.</target>
          </trans-unit>
          <trans-unit id="247" xml:space="preserve">
            <source>For the preview release, only <bpt id="2">&lt;strong&gt;</bpt>en-US<ept id="2">&lt;/strong&gt;</ept> is supported.</source>
            <target state="new">For the preview release, only <bpt id="2">&lt;strong&gt;</bpt>en-US<ept id="2">&lt;/strong&gt;</ept> is supported.</target>
          </trans-unit>
          <trans-unit id="248" xml:space="preserve">
            <source>Status</source>
            <target state="new">Status</target>
          </trans-unit>
          <trans-unit id="249" xml:space="preserve">
            <source>The status of Stream Analytics jobs can be inspected in the Azure portal.</source>
            <target state="new">The status of Stream Analytics jobs can be inspected in the Azure portal.</target>
          </trans-unit>
          <trans-unit id="250" xml:space="preserve">
            <source>Running jobs can be in one of two states: <bpt id="2">&lt;strong&gt;</bpt>Running<ept id="2">&lt;/strong&gt;</ept>, or <bpt id="4">&lt;strong&gt;</bpt>Degraded<ept id="4">&lt;/strong&gt;</ept>.</source>
            <target state="new">Running jobs can be in one of two states: <bpt id="2">&lt;strong&gt;</bpt>Running<ept id="2">&lt;/strong&gt;</ept>, or <bpt id="4">&lt;strong&gt;</bpt>Degraded<ept id="4">&lt;/strong&gt;</ept>.</target>
          </trans-unit>
          <trans-unit id="251" xml:space="preserve">
            <source>The definition for each of these states is below:</source>
            <target state="new">The definition for each of these states is below:</target>
          </trans-unit>
          <trans-unit id="252" xml:space="preserve">
            <source><bpt id="1">&lt;strong&gt;</bpt>Running<ept id="1">&lt;/strong&gt;</ept> - The job is allocated, processing input, or waiting to process input.</source>
            <target state="new"><bpt id="1">&lt;strong&gt;</bpt>Running<ept id="1">&lt;/strong&gt;</ept> - The job is allocated, processing input, or waiting to process input.</target>
          </trans-unit>
          <trans-unit id="253" xml:space="preserve">
            <source>If the job shows a Running state without producing output, it is likely that the data processing time window is large or the query logic is complicated.</source>
            <target state="new">If the job shows a Running state without producing output, it is likely that the data processing time window is large or the query logic is complicated.</target>
          </trans-unit>
          <trans-unit id="254" xml:space="preserve">
            <source>Another reason may be that currently there isn't any data being sent to the job.</source>
            <target state="new">Another reason may be that currently there isn't any data being sent to the job.</target>
          </trans-unit>
          <trans-unit id="255" xml:space="preserve">
            <source><bpt id="1">&lt;strong&gt;</bpt>Degraded<ept id="1">&lt;/strong&gt;</ept> - This state indicates that a Stream Analytics job is encountering one of the following errors: input/output communication errors, query errors, or retry-able run-time errors.</source>
            <target state="new"><bpt id="1">&lt;strong&gt;</bpt>Degraded<ept id="1">&lt;/strong&gt;</ept> - This state indicates that a Stream Analytics job is encountering one of the following errors: input/output communication errors, query errors, or retry-able run-time errors.</target>
          </trans-unit>
          <trans-unit id="256" xml:space="preserve">
            <source>To distinguish what type of error(s) the job is encountering, view the operation logs.</source>
            <target state="new">To distinguish what type of error(s) the job is encountering, view the operation logs.</target>
          </trans-unit>
          <trans-unit id="257" xml:space="preserve">
            <source>Get support</source>
            <target state="new">Get support</target>
          </trans-unit>
          <trans-unit id="258" xml:space="preserve">
            <source>For further assistance, try our <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>Azure Stream Analytics forum<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept>.</source>
            <target state="new">For further assistance, try our <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>Azure Stream Analytics forum<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept>.</target>
          </trans-unit>
          <trans-unit id="259" xml:space="preserve">
            <source>Next steps</source>
            <target state="new">Next steps</target>
          </trans-unit>
          <trans-unit id="260" xml:space="preserve">
            <source>Now that you're familiar with the key concepts of Stream Analytics, try:</source>
            <target state="new">Now that you're familiar with the key concepts of Stream Analytics, try:</target>
          </trans-unit>
          <trans-unit id="261" xml:space="preserve">
            <source><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>Introduction to Azure Stream Analytics<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept></source>
            <target state="new"><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>Introduction to Azure Stream Analytics<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept></target>
          </trans-unit>
          <trans-unit id="262" xml:space="preserve">
            <source><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>Get started using Azure Stream Analytics<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept></source>
            <target state="new"><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>Get started using Azure Stream Analytics<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept></target>
          </trans-unit>
          <trans-unit id="263" xml:space="preserve">
            <source><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>Scale Azure Stream Analytics jobs<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept></source>
            <target state="new"><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>Scale Azure Stream Analytics jobs<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept></target>
          </trans-unit>
          <trans-unit id="264" xml:space="preserve">
            <source><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>Azure Stream Analytics Query Language Reference<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept></source>
            <target state="new"><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>Azure Stream Analytics Query Language Reference<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept></target>
          </trans-unit>
          <trans-unit id="265" xml:space="preserve">
            <source><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>Azure Stream Analytics Management REST API Reference<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept></source>
            <target state="new"><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>Azure Stream Analytics Management REST API Reference<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept></target>
          </trans-unit>
        </group>
      </group>
    </body>
  </file>
</xliff>