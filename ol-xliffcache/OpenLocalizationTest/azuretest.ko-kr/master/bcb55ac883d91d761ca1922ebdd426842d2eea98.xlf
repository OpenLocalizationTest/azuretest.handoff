<?xml version="1.0" encoding="utf-8"?>
<xliff version="1.2" xmlns="urn:oasis:names:tc:xliff:document:1.2" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="urn:oasis:names:tc:xliff:document:1.2 xliff-core-1.2-transitional.xsd">
  <file datatype="xml" source-language="en-us" target-language="ko-kr" original="2/21/2016 3:26:12 AM" tool-id="MarkdownTransformer" product-name="N/A" product-version="N/A" build-num="1">
    <header>
      <xliffext:olfilehash xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">bcb55ac883d91d761ca1922ebdd426842d2eea98</xliffext:olfilehash>
      <tool tool-id="MarkdownTransformer" tool-name="MarkdownToXliff" tool-version="1.0" tool-company="Microsoft" />
    </header>
    <body>
      <group extype="content">
        <group id="101">
          <trans-unit id="101" xml:space="preserve">
            <source>Submit Hive Queries to Hadoop clusters in the advanced analytics process | Microsoft Azure</source>
            <target state="new">Submit Hive Queries to Hadoop clusters in the advanced analytics process | Microsoft Azure</target>
          </trans-unit>
          <trans-unit id="102" xml:space="preserve">
            <source>Process Data from Hive Tables</source>
            <target state="new">Process Data from Hive Tables</target>
          </trans-unit>
          <trans-unit id="103" xml:space="preserve">
            <source><bpt id="1">&lt;html&gt;</bpt><ept id="1">&lt;/html&gt;</ept><bpt id="2">&lt;html&gt;</bpt><ept id="2">&lt;/html&gt;</ept> Submit Hive Queries to HDInsight Hadoop clusters in the advanced analytics process</source>
            <target state="new"><bpt id="1">&lt;html&gt;</bpt><ept id="1">&lt;/html&gt;</ept><bpt id="2">&lt;html&gt;</bpt><ept id="2">&lt;/html&gt;</ept> Submit Hive Queries to HDInsight Hadoop clusters in the advanced analytics process</target>
          </trans-unit>
          <trans-unit id="104" xml:space="preserve">
            <source>This document describes various ways of submitting Hive queries to Hadoop clusters that are managed by an HDInsight service in Azure.</source>
            <target state="new">This document describes various ways of submitting Hive queries to Hadoop clusters that are managed by an HDInsight service in Azure.</target>
          </trans-unit>
          <trans-unit id="105" xml:space="preserve">
            <source>Hive queries can be submitted by using:</source>
            <target state="new">Hive queries can be submitted by using:</target>
          </trans-unit>
          <trans-unit id="106" xml:space="preserve">
            <source>the Hadoop Command Line on the headnode of the cluster</source>
            <target state="new">the Hadoop Command Line on the headnode of the cluster</target>
          </trans-unit>
          <trans-unit id="107" xml:space="preserve">
            <source>the IPython Notebook</source>
            <target state="new">the IPython Notebook</target>
          </trans-unit>
          <trans-unit id="108" xml:space="preserve">
            <source>the Hive Editor</source>
            <target state="new">the Hive Editor</target>
          </trans-unit>
          <trans-unit id="109" xml:space="preserve">
            <source>Azure PowerShell scripts</source>
            <target state="new">Azure PowerShell scripts</target>
          </trans-unit>
          <trans-unit id="110" xml:space="preserve">
            <source>Generic Hive queries that can be used to explore the data or to generate features that use embedded Hive User Defined Functions (UDFs) are provided.</source>
            <target state="new">Generic Hive queries that can be used to explore the data or to generate features that use embedded Hive User Defined Functions (UDFs) are provided.</target>
          </trans-unit>
          <trans-unit id="111" xml:space="preserve">
            <source>Examples of queries the specific to <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>NYC Taxi Trip Data<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept> scenarios are also provided in <bpt id="4CapsExtId1">&lt;link&gt;</bpt><bpt id="4CapsExtId2">&lt;linkText&gt;</bpt>Github repository<ept id="4CapsExtId2">&lt;/linkText&gt;</ept><bpt id="4CapsExtId3">&lt;title&gt;</bpt><ept id="4CapsExtId3">&lt;/title&gt;</ept><ept id="4CapsExtId1">&lt;/link&gt;</ept>.</source>
            <target state="new">Examples of queries the specific to <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>NYC Taxi Trip Data<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept> scenarios are also provided in <bpt id="4CapsExtId1">&lt;link&gt;</bpt><bpt id="4CapsExtId2">&lt;linkText&gt;</bpt>Github repository<ept id="4CapsExtId2">&lt;/linkText&gt;</ept><bpt id="4CapsExtId3">&lt;title&gt;</bpt><ept id="4CapsExtId3">&lt;/title&gt;</ept><ept id="4CapsExtId1">&lt;/link&gt;</ept>.</target>
          </trans-unit>
          <trans-unit id="112" xml:space="preserve">
            <source>These queries already have data schema specified and are ready to be submitted to run for this scenario.</source>
            <target state="new">These queries already have data schema specified and are ready to be submitted to run for this scenario.</target>
          </trans-unit>
          <trans-unit id="113" xml:space="preserve">
            <source>In the final section, parameters that users can tune to improve the performance of Hive queries are discussed.</source>
            <target state="new">In the final section, parameters that users can tune to improve the performance of Hive queries are discussed.</target>
          </trans-unit>
          <trans-unit id="114" xml:space="preserve">
            <source>Prerequisites</source>
            <target state="new">Prerequisites</target>
          </trans-unit>
          <trans-unit id="115" xml:space="preserve">
            <source>This article assumes that you have:</source>
            <target state="new">This article assumes that you have:</target>
          </trans-unit>
          <trans-unit id="116" xml:space="preserve">
            <source>created an Azure storage account.</source>
            <target state="new">created an Azure storage account.</target>
          </trans-unit>
          <trans-unit id="117" xml:space="preserve">
            <source>If you need instructions for this task, see <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>Create an Azure Storage account<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept></source>
            <target state="new">If you need instructions for this task, see <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>Create an Azure Storage account<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept></target>
          </trans-unit>
          <trans-unit id="118" xml:space="preserve">
            <source>provisioned an Hadoop cluster with the HDInsight service.</source>
            <target state="new">provisioned an Hadoop cluster with the HDInsight service.</target>
          </trans-unit>
          <trans-unit id="119" xml:space="preserve">
            <source>If you need instructions, see <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>Provision an HDInsight cluster<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept>.</source>
            <target state="new">If you need instructions, see <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>Provision an HDInsight cluster<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept>.</target>
          </trans-unit>
          <trans-unit id="120" xml:space="preserve">
            <source>the data has been uploaded to Hive tables in Azure HDInsight Hadoop clusters.</source>
            <target state="new">the data has been uploaded to Hive tables in Azure HDInsight Hadoop clusters.</target>
          </trans-unit>
          <trans-unit id="121" xml:space="preserve">
            <source>If it has not, please follow the instructions provided at <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>Create and load data to Hive tables<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept> to upload data to Hive tables first.</source>
            <target state="new">If it has not, please follow the instructions provided at <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>Create and load data to Hive tables<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept> to upload data to Hive tables first.</target>
          </trans-unit>
          <trans-unit id="122" xml:space="preserve">
            <source>enabled remote access to the cluster.</source>
            <target state="new">enabled remote access to the cluster.</target>
          </trans-unit>
          <trans-unit id="123" xml:space="preserve">
            <source>If you need instructions, see <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>Access the Head Node of Hadoop Cluster<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept>.</source>
            <target state="new">If you need instructions, see <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>Access the Head Node of Hadoop Cluster<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept>.</target>
          </trans-unit>
          <trans-unit id="124" xml:space="preserve">
            <source><bpt id="1">&lt;html&gt;</bpt><ept id="1">&lt;/html&gt;</ept><bpt id="2">&lt;html&gt;</bpt><ept id="2">&lt;/html&gt;</ept>How to submit Hive queries</source>
            <target state="new"><bpt id="1">&lt;html&gt;</bpt><ept id="1">&lt;/html&gt;</ept><bpt id="2">&lt;html&gt;</bpt><ept id="2">&lt;/html&gt;</ept>How to submit Hive queries</target>
          </trans-unit>
          <trans-unit id="125" xml:space="preserve">
            <source><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>Submit Hive queries through Hadoop Command Line in headnode of Hadoop cluster<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept></source>
            <target state="new"><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>Submit Hive queries through Hadoop Command Line in headnode of Hadoop cluster<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept></target>
          </trans-unit>
          <trans-unit id="126" xml:space="preserve">
            <source><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>Submit Hive queries with the Hive Editor<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept></source>
            <target state="new"><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>Submit Hive queries with the Hive Editor<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept></target>
          </trans-unit>
          <trans-unit id="127" xml:space="preserve">
            <source><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>Submit Hive queries with Azure PowerShell Commands<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept></source>
            <target state="new"><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>Submit Hive queries with Azure PowerShell Commands<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept></target>
          </trans-unit>
          <trans-unit id="128" xml:space="preserve">
            <source><bpt id="1">&lt;html&gt;</bpt><ept id="1">&lt;/html&gt;</ept><bpt id="2">&lt;html&gt;</bpt><ept id="2">&lt;/html&gt;</ept> 1. Submit Hive queries through Hadoop Command Line in headnode of Hadoop cluster</source>
            <target state="new"><bpt id="1">&lt;html&gt;</bpt><ept id="1">&lt;/html&gt;</ept><bpt id="2">&lt;html&gt;</bpt><ept id="2">&lt;/html&gt;</ept> 1. Submit Hive queries through Hadoop Command Line in headnode of Hadoop cluster</target>
          </trans-unit>
          <trans-unit id="129" xml:space="preserve">
            <source>If the Hive query is complex, submitting it directly in the head node of the Hadoop cluster typically leads to faster turn around than submitting it with a Hive Editor or Azure PowerShell scripts.</source>
            <target state="new">If the Hive query is complex, submitting it directly in the head node of the Hadoop cluster typically leads to faster turn around than submitting it with a Hive Editor or Azure PowerShell scripts.</target>
          </trans-unit>
          <trans-unit id="130" xml:space="preserve">
            <source>Log in to the head node of the Hadoop cluster, open the Hadoop Command Line on the desktop of the head node, and enter command <bpt id="2">&lt;code&gt;</bpt>cd %hive_home%\bin<ept id="2">&lt;/code&gt;</ept>.</source>
            <target state="new">Log in to the head node of the Hadoop cluster, open the Hadoop Command Line on the desktop of the head node, and enter command <bpt id="2">&lt;code&gt;</bpt>cd %hive_home%\bin<ept id="2">&lt;/code&gt;</ept>.</target>
          </trans-unit>
          <trans-unit id="131" xml:space="preserve">
            <source>Users have three ways to submit Hive queries in the Hadoop Command Line:</source>
            <target state="new">Users have three ways to submit Hive queries in the Hadoop Command Line:</target>
          </trans-unit>
          <trans-unit id="132" xml:space="preserve">
            <source>directly</source>
            <target state="new">directly</target>
          </trans-unit>
          <trans-unit id="133" xml:space="preserve">
            <source>using .hql files</source>
            <target state="new">using .hql files</target>
          </trans-unit>
          <trans-unit id="134" xml:space="preserve">
            <source>with the Hive command console</source>
            <target state="new">with the Hive command console</target>
          </trans-unit>
          <trans-unit id="135" xml:space="preserve">
            <source>Submit Hive queries directly in Hadoop Command Line.</source>
            <target state="new">Submit Hive queries directly in Hadoop Command Line.</target>
          </trans-unit>
          <trans-unit id="136" xml:space="preserve">
            <source>Users can run command like <bpt id="2">&lt;code&gt;</bpt>hive -e "&lt;your hive query&gt;;<ept id="2">&lt;/code&gt;</ept> to submit simple Hive queries directly in Hadoop Command Line. Here is an example, where the red box outlines the command that submits the Hive query, and the green box outlines the output from the Hive query.</source>
            <target state="new">Users can run command like <bpt id="2">&lt;code&gt;</bpt>hive -e "&lt;your hive query&gt;;<ept id="2">&lt;/code&gt;</ept> to submit simple Hive queries directly in Hadoop Command Line. Here is an example, where the red box outlines the command that submits the Hive query, and the green box outlines the output from the Hive query.</target>
          </trans-unit>
          <trans-unit id="137" xml:space="preserve">
            <source><bpt id="1">&lt;linkText&gt;</bpt>Create workspace<ept id="1">&lt;/linkText&gt;</ept></source>
            <target state="new"><bpt id="1">&lt;linkText&gt;</bpt>Create workspace<ept id="1">&lt;/linkText&gt;</ept></target>
          </trans-unit>
          <trans-unit id="138" xml:space="preserve">
            <source>Submit Hive queries in .hql files</source>
            <target state="new">Submit Hive queries in .hql files</target>
          </trans-unit>
          <trans-unit id="139" xml:space="preserve">
            <source>When the Hive query is more complicated and has multiple lines, editing queries in command line or Hive command console is not practical.</source>
            <target state="new">When the Hive query is more complicated and has multiple lines, editing queries in command line or Hive command console is not practical.</target>
          </trans-unit>
          <trans-unit id="140" xml:space="preserve">
            <source>An alternative is to use a text editor in the head node of the Hadoop cluster to save the Hive queries in a .hql file in a local directory of the head node.</source>
            <target state="new">An alternative is to use a text editor in the head node of the Hadoop cluster to save the Hive queries in a .hql file in a local directory of the head node.</target>
          </trans-unit>
          <trans-unit id="141" xml:space="preserve">
            <source>Then the Hive query in the .hql file can be submitted by using the <bpt id="2">&lt;code&gt;</bpt>-f<ept id="2">&lt;/code&gt;</ept> argument as follows:</source>
            <target state="new">Then the Hive query in the .hql file can be submitted by using the <bpt id="2">&lt;code&gt;</bpt>-f<ept id="2">&lt;/code&gt;</ept> argument as follows:</target>
          </trans-unit>
          <trans-unit id="142" xml:space="preserve">
            <source><bpt id="1">&lt;linkText&gt;</bpt>Create workspace<ept id="1">&lt;/linkText&gt;</ept></source>
            <target state="new"><bpt id="1">&lt;linkText&gt;</bpt>Create workspace<ept id="1">&lt;/linkText&gt;</ept></target>
          </trans-unit>
          <trans-unit id="143" xml:space="preserve">
            <source><bpt id="1">&lt;strong&gt;</bpt>Suppress progress status screen print of Hive queries<ept id="1">&lt;/strong&gt;</ept></source>
            <target state="new"><bpt id="1">&lt;strong&gt;</bpt>Suppress progress status screen print of Hive queries<ept id="1">&lt;/strong&gt;</ept></target>
          </trans-unit>
          <trans-unit id="144" xml:space="preserve">
            <source>By default, after Hive query is submitted in Hadoop Command Line, the progress of the Map/Reduce job will be printed out on screen.</source>
            <target state="new">By default, after Hive query is submitted in Hadoop Command Line, the progress of the Map/Reduce job will be printed out on screen.</target>
          </trans-unit>
          <trans-unit id="145" xml:space="preserve">
            <source>To suppress the screen print of the Map/Reduce job progress, you can use an argument <bpt id="2">&lt;code&gt;</bpt>-S<ept id="2">&lt;/code&gt;</ept> ("S" in upper case) in the command line as follows:</source>
            <target state="new">To suppress the screen print of the Map/Reduce job progress, you can use an argument <bpt id="2">&lt;code&gt;</bpt>-S<ept id="2">&lt;/code&gt;</ept> ("S" in upper case) in the command line as follows:</target>
          </trans-unit>
          <trans-unit id="146" xml:space="preserve">
            <source>Submit Hive queries in Hive command console.</source>
            <target state="new">Submit Hive queries in Hive command console.</target>
          </trans-unit>
          <trans-unit id="147" xml:space="preserve">
            <source>Users can also first enter the Hive command console by running command <bpt id="2">&lt;code&gt;</bpt>hive<ept id="2">&lt;/code&gt;</ept> in Hadoop Command Line, and then submit Hive queries in Hive command console.</source>
            <target state="new">Users can also first enter the Hive command console by running command <bpt id="2">&lt;code&gt;</bpt>hive<ept id="2">&lt;/code&gt;</ept> in Hadoop Command Line, and then submit Hive queries in Hive command console.</target>
          </trans-unit>
          <trans-unit id="148" xml:space="preserve">
            <source>Here is an example.</source>
            <target state="new">Here is an example.</target>
          </trans-unit>
          <trans-unit id="149" xml:space="preserve">
            <source>In this example, the two red boxes highlight the commands used to enter the Hive command console, and the Hive query submitted in Hive command console, respectively.</source>
            <target state="new">In this example, the two red boxes highlight the commands used to enter the Hive command console, and the Hive query submitted in Hive command console, respectively.</target>
          </trans-unit>
          <trans-unit id="150" xml:space="preserve">
            <source>The green box highlights the output from the Hive query.</source>
            <target state="new">The green box highlights the output from the Hive query.</target>
          </trans-unit>
          <trans-unit id="151" xml:space="preserve">
            <source><bpt id="1">&lt;linkText&gt;</bpt>Create workspace<ept id="1">&lt;/linkText&gt;</ept></source>
            <target state="new"><bpt id="1">&lt;linkText&gt;</bpt>Create workspace<ept id="1">&lt;/linkText&gt;</ept></target>
          </trans-unit>
          <trans-unit id="152" xml:space="preserve">
            <source>The previous examples directly output the Hive query results on screen.</source>
            <target state="new">The previous examples directly output the Hive query results on screen.</target>
          </trans-unit>
          <trans-unit id="153" xml:space="preserve">
            <source>Users can also write the output to a local file on the head node, or to an Azure blob.</source>
            <target state="new">Users can also write the output to a local file on the head node, or to an Azure blob.</target>
          </trans-unit>
          <trans-unit id="154" xml:space="preserve">
            <source>Then, users can use other tools to further analyze the output of Hive queries.</source>
            <target state="new">Then, users can use other tools to further analyze the output of Hive queries.</target>
          </trans-unit>
          <trans-unit id="155" xml:space="preserve">
            <source><bpt id="1">&lt;strong&gt;</bpt>Output Hive query results to a local file.<ept id="1">&lt;/strong&gt;</ept></source>
            <target state="new"><bpt id="1">&lt;strong&gt;</bpt>Output Hive query results to a local file.<ept id="1">&lt;/strong&gt;</ept></target>
          </trans-unit>
          <trans-unit id="156" xml:space="preserve">
            <source>To output Hive query results to a local directory on the head node, users have to submit the Hive query in the Hadoop Command Line as follows:</source>
            <target state="new">To output Hive query results to a local directory on the head node, users have to submit the Hive query in the Hadoop Command Line as follows:</target>
          </trans-unit>
          <trans-unit id="157" xml:space="preserve">
            <source>In the following example, the output of Hive query is written into a file <bpt id="2">&lt;code&gt;</bpt>hivequeryoutput.txt<ept id="2">&lt;/code&gt;</ept> in directory <bpt id="4">&lt;code&gt;</bpt>C:\apps\temp<ept id="4">&lt;/code&gt;</ept>.</source>
            <target state="new">In the following example, the output of Hive query is written into a file <bpt id="2">&lt;code&gt;</bpt>hivequeryoutput.txt<ept id="2">&lt;/code&gt;</ept> in directory <bpt id="4">&lt;code&gt;</bpt>C:\apps\temp<ept id="4">&lt;/code&gt;</ept>.</target>
          </trans-unit>
          <trans-unit id="158" xml:space="preserve">
            <source><bpt id="1">&lt;linkText&gt;</bpt>Create workspace<ept id="1">&lt;/linkText&gt;</ept></source>
            <target state="new"><bpt id="1">&lt;linkText&gt;</bpt>Create workspace<ept id="1">&lt;/linkText&gt;</ept></target>
          </trans-unit>
          <trans-unit id="159" xml:space="preserve">
            <source><bpt id="1">&lt;strong&gt;</bpt>Output Hive query results to an Azure blob<ept id="1">&lt;/strong&gt;</ept></source>
            <target state="new"><bpt id="1">&lt;strong&gt;</bpt>Output Hive query results to an Azure blob<ept id="1">&lt;/strong&gt;</ept></target>
          </trans-unit>
          <trans-unit id="160" xml:space="preserve">
            <source>Users can also output the Hive query results to an Azure blob, within the default container of the Hadoop cluster.</source>
            <target state="new">Users can also output the Hive query results to an Azure blob, within the default container of the Hadoop cluster.</target>
          </trans-unit>
          <trans-unit id="161" xml:space="preserve">
            <source>The Hive query has to be like this:</source>
            <target state="new">The Hive query has to be like this:</target>
          </trans-unit>
          <trans-unit id="162" xml:space="preserve">
            <source>In the following example, the output of Hive query is written to a blob directory <bpt id="2">&lt;code&gt;</bpt>queryoutputdir<ept id="2">&lt;/code&gt;</ept> within the default container of the Hadoop cluster.</source>
            <target state="new">In the following example, the output of Hive query is written to a blob directory <bpt id="2">&lt;code&gt;</bpt>queryoutputdir<ept id="2">&lt;/code&gt;</ept> within the default container of the Hadoop cluster.</target>
          </trans-unit>
          <trans-unit id="163" xml:space="preserve">
            <source>Here, you only need to provide the directory name, without the blob name.</source>
            <target state="new">Here, you only need to provide the directory name, without the blob name.</target>
          </trans-unit>
          <trans-unit id="164" xml:space="preserve">
            <source>An error will be thrown out if you provide both directory and blob names, such as <bpt id="2">&lt;code&gt;</bpt>wasb:///queryoutputdir/queryoutput.txt<ept id="2">&lt;/code&gt;</ept>.</source>
            <target state="new">An error will be thrown out if you provide both directory and blob names, such as <bpt id="2">&lt;code&gt;</bpt>wasb:///queryoutputdir/queryoutput.txt<ept id="2">&lt;/code&gt;</ept>.</target>
          </trans-unit>
          <trans-unit id="165" xml:space="preserve">
            <source><bpt id="1">&lt;linkText&gt;</bpt>Create workspace<ept id="1">&lt;/linkText&gt;</ept></source>
            <target state="new"><bpt id="1">&lt;linkText&gt;</bpt>Create workspace<ept id="1">&lt;/linkText&gt;</ept></target>
          </trans-unit>
          <trans-unit id="166" xml:space="preserve">
            <source>If you open the default container of the Hadoop cluster using tools like Azure Storage Explorer, you will see the output of the Hive query as follows.</source>
            <target state="new">If you open the default container of the Hadoop cluster using tools like Azure Storage Explorer, you will see the output of the Hive query as follows.</target>
          </trans-unit>
          <trans-unit id="167" xml:space="preserve">
            <source>You can apply the filter (highlighted by red box) to only retrieve the blob with specified letters in names.</source>
            <target state="new">You can apply the filter (highlighted by red box) to only retrieve the blob with specified letters in names.</target>
          </trans-unit>
          <trans-unit id="168" xml:space="preserve">
            <source><bpt id="1">&lt;linkText&gt;</bpt>Create workspace<ept id="1">&lt;/linkText&gt;</ept></source>
            <target state="new"><bpt id="1">&lt;linkText&gt;</bpt>Create workspace<ept id="1">&lt;/linkText&gt;</ept></target>
          </trans-unit>
          <trans-unit id="169" xml:space="preserve">
            <source><bpt id="1">&lt;html&gt;</bpt><ept id="1">&lt;/html&gt;</ept><bpt id="2">&lt;html&gt;</bpt><ept id="2">&lt;/html&gt;</ept> 2. Submit Hive queries with the Hive Editor</source>
            <target state="new"><bpt id="1">&lt;html&gt;</bpt><ept id="1">&lt;/html&gt;</ept><bpt id="2">&lt;html&gt;</bpt><ept id="2">&lt;/html&gt;</ept> 2. Submit Hive queries with the Hive Editor</target>
          </trans-unit>
          <trans-unit id="170" xml:space="preserve">
            <source>Users can also use Query Console (Hive Editor) by entering the URL in a web browser <bpt id="2">&lt;code&gt;</bpt>https://&lt;Hadoop cluster name&gt;.azurehdinsight.net/Home/HiveEditor<ept id="2">&lt;/code&gt;</ept> (you will be asked to input the Hadoop cluster credentials to log in),</source>
            <target state="new">Users can also use Query Console (Hive Editor) by entering the URL in a web browser <bpt id="2">&lt;code&gt;</bpt>https://&lt;Hadoop cluster name&gt;.azurehdinsight.net/Home/HiveEditor<ept id="2">&lt;/code&gt;</ept> (you will be asked to input the Hadoop cluster credentials to log in),</target>
          </trans-unit>
          <trans-unit id="171" xml:space="preserve">
            <source><bpt id="1">&lt;html&gt;</bpt><ept id="1">&lt;/html&gt;</ept><bpt id="2">&lt;html&gt;</bpt><ept id="2">&lt;/html&gt;</ept> 3. Submit Hive queries with Azure PowerShell Commands</source>
            <target state="new"><bpt id="1">&lt;html&gt;</bpt><ept id="1">&lt;/html&gt;</ept><bpt id="2">&lt;html&gt;</bpt><ept id="2">&lt;/html&gt;</ept> 3. Submit Hive queries with Azure PowerShell Commands</target>
          </trans-unit>
          <trans-unit id="172" xml:space="preserve">
            <source>Users can also us PowerShell to submit Hive queries.</source>
            <target state="new">Users can also us PowerShell to submit Hive queries.</target>
          </trans-unit>
          <trans-unit id="173" xml:space="preserve">
            <source>For instructions, see <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>Submit Hive jobs using PowerShell<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept>.</source>
            <target state="new">For instructions, see <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>Submit Hive jobs using PowerShell<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept>.</target>
          </trans-unit>
          <trans-unit id="174" xml:space="preserve">
            <source><bpt id="1">&lt;html&gt;</bpt><ept id="1">&lt;/html&gt;</ept><bpt id="2">&lt;html&gt;</bpt><ept id="2">&lt;/html&gt;</ept>Data Exploration, Feature Engineering and Hive Parameter Tuning</source>
            <target state="new"><bpt id="1">&lt;html&gt;</bpt><ept id="1">&lt;/html&gt;</ept><bpt id="2">&lt;html&gt;</bpt><ept id="2">&lt;/html&gt;</ept>Data Exploration, Feature Engineering and Hive Parameter Tuning</target>
          </trans-unit>
          <trans-unit id="175" xml:space="preserve">
            <source>We describe the following data wrangling tasks in this section using Hive in Azure HDInsight Hadoop clusters:</source>
            <target state="new">We describe the following data wrangling tasks in this section using Hive in Azure HDInsight Hadoop clusters:</target>
          </trans-unit>
          <trans-unit id="176" xml:space="preserve">
            <source><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>Data Exploration<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept></source>
            <target state="new"><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>Data Exploration<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept></target>
          </trans-unit>
          <trans-unit id="177" xml:space="preserve">
            <source><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>Feature Generation<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept></source>
            <target state="new"><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>Feature Generation<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept></target>
          </trans-unit>
          <trans-unit id="178" xml:space="preserve">
            <source>The sample Hive queries assume that the data has been uploaded to Hive tables in Azure HDInsight Hadoop clusters.</source>
            <target state="new">The sample Hive queries assume that the data has been uploaded to Hive tables in Azure HDInsight Hadoop clusters.</target>
          </trans-unit>
          <trans-unit id="179" xml:space="preserve">
            <source>If it has not, please follow <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>Create and load data to Hive tables<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept> to upload data to Hive tables first.</source>
            <target state="new">If it has not, please follow <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>Create and load data to Hive tables<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept> to upload data to Hive tables first.</target>
          </trans-unit>
          <trans-unit id="180" xml:space="preserve">
            <source><bpt id="1">&lt;html&gt;</bpt><ept id="1">&lt;/html&gt;</ept><bpt id="2">&lt;html&gt;</bpt><ept id="2">&lt;/html&gt;</ept>Data Exploration</source>
            <target state="new"><bpt id="1">&lt;html&gt;</bpt><ept id="1">&lt;/html&gt;</ept><bpt id="2">&lt;html&gt;</bpt><ept id="2">&lt;/html&gt;</ept>Data Exploration</target>
          </trans-unit>
          <trans-unit id="181" xml:space="preserve">
            <source>Here are a few sample Hive scripts that can be used to explore data in Hive tables.</source>
            <target state="new">Here are a few sample Hive scripts that can be used to explore data in Hive tables.</target>
          </trans-unit>
          <trans-unit id="182" xml:space="preserve">
            <source>Get the count of observations per partition</source>
            <target state="new">Get the count of observations per partition</target>
          </trans-unit>
          <trans-unit id="183" xml:space="preserve">
            <source><bpt id="1">&lt;code&gt;</bpt>SELECT &lt;partitionfieldname&gt;, count(*) from &lt;databasename&gt;.&lt;tablename&gt; group by &lt;partitionfieldname&gt;;<ept id="1">&lt;/code&gt;</ept></source>
            <target state="new"><bpt id="1">&lt;code&gt;</bpt>SELECT &lt;partitionfieldname&gt;, count(*) from &lt;databasename&gt;.&lt;tablename&gt; group by &lt;partitionfieldname&gt;;<ept id="1">&lt;/code&gt;</ept></target>
          </trans-unit>
          <trans-unit id="184" xml:space="preserve">
            <source>Get the count of observations per day</source>
            <target state="new">Get the count of observations per day</target>
          </trans-unit>
          <trans-unit id="185" xml:space="preserve">
            <source><bpt id="1">&lt;code&gt;</bpt>SELECT to_date(&lt;date_columnname&gt;), count(*) from &lt;databasename&gt;.&lt;tablename&gt; group by to_date(&lt;date_columnname&gt;);<ept id="1">&lt;/code&gt;</ept></source>
            <target state="new"><bpt id="1">&lt;code&gt;</bpt>SELECT to_date(&lt;date_columnname&gt;), count(*) from &lt;databasename&gt;.&lt;tablename&gt; group by to_date(&lt;date_columnname&gt;);<ept id="1">&lt;/code&gt;</ept></target>
          </trans-unit>
          <trans-unit id="186" xml:space="preserve">
            <source>Get the levels in a categorical column</source>
            <target state="new">Get the levels in a categorical column</target>
          </trans-unit>
          <trans-unit id="187" xml:space="preserve">
            <source><bpt id="1">&lt;code&gt;</bpt>SELECT  distinct &lt;column_name&gt; from &lt;databasename&gt;.&lt;tablename&gt;<ept id="1">&lt;/code&gt;</ept></source>
            <target state="new"><bpt id="1">&lt;code&gt;</bpt>SELECT  distinct &lt;column_name&gt; from &lt;databasename&gt;.&lt;tablename&gt;<ept id="1">&lt;/code&gt;</ept></target>
          </trans-unit>
          <trans-unit id="188" xml:space="preserve">
            <source>Get the number of levels in combination of two categorical columns</source>
            <target state="new">Get the number of levels in combination of two categorical columns</target>
          </trans-unit>
          <trans-unit id="189" xml:space="preserve">
            <source><bpt id="1">&lt;code&gt;</bpt>SELECT &lt;column_a&gt;, &lt;column_b&gt;, count(*) from &lt;databasename&gt;.&lt;tablename&gt; group by &lt;column_a&gt;, &lt;column_b&gt;<ept id="1">&lt;/code&gt;</ept></source>
            <target state="new"><bpt id="1">&lt;code&gt;</bpt>SELECT &lt;column_a&gt;, &lt;column_b&gt;, count(*) from &lt;databasename&gt;.&lt;tablename&gt; group by &lt;column_a&gt;, &lt;column_b&gt;<ept id="1">&lt;/code&gt;</ept></target>
          </trans-unit>
          <trans-unit id="190" xml:space="preserve">
            <source>Get the distribution for numerical columns</source>
            <target state="new">Get the distribution for numerical columns</target>
          </trans-unit>
          <trans-unit id="191" xml:space="preserve">
            <source><bpt id="1">&lt;code&gt;</bpt>SELECT &lt;column_name&gt;, count(*) from &lt;databasename&gt;.&lt;tablename&gt; group by &lt;column_name&gt;<ept id="1">&lt;/code&gt;</ept></source>
            <target state="new"><bpt id="1">&lt;code&gt;</bpt>SELECT &lt;column_name&gt;, count(*) from &lt;databasename&gt;.&lt;tablename&gt; group by &lt;column_name&gt;<ept id="1">&lt;/code&gt;</ept></target>
          </trans-unit>
          <trans-unit id="192" xml:space="preserve">
            <source>Extract records from joining two tables</source>
            <target state="new">Extract records from joining two tables</target>
          </trans-unit>
          <trans-unit id="193" xml:space="preserve">
            <source><bpt id="1">&lt;html&gt;</bpt><ept id="1">&lt;/html&gt;</ept><bpt id="2">&lt;html&gt;</bpt><ept id="2">&lt;/html&gt;</ept>Feature Generation</source>
            <target state="new"><bpt id="1">&lt;html&gt;</bpt><ept id="1">&lt;/html&gt;</ept><bpt id="2">&lt;html&gt;</bpt><ept id="2">&lt;/html&gt;</ept>Feature Generation</target>
          </trans-unit>
          <trans-unit id="194" xml:space="preserve">
            <source>In this section, we describe ways of generating features using Hive queries:</source>
            <target state="new">In this section, we describe ways of generating features using Hive queries:</target>
          </trans-unit>
          <trans-unit id="195" xml:space="preserve">
            <source><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>Frequency based Feature Generation<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept></source>
            <target state="new"><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>Frequency based Feature Generation<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept></target>
          </trans-unit>
          <trans-unit id="196" xml:space="preserve">
            <source><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>Risks of Categorical Variables in Binary Classification<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept></source>
            <target state="new"><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>Risks of Categorical Variables in Binary Classification<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept></target>
          </trans-unit>
          <trans-unit id="197" xml:space="preserve">
            <source><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>Extract features from Datetime Field<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept></source>
            <target state="new"><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>Extract features from Datetime Field<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept></target>
          </trans-unit>
          <trans-unit id="198" xml:space="preserve">
            <source><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>Extract features from Text Field<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept></source>
            <target state="new"><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>Extract features from Text Field<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept></target>
          </trans-unit>
          <trans-unit id="199" xml:space="preserve">
            <source><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>Calculate distance between GPS coordinates<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept></source>
            <target state="new"><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>Calculate distance between GPS coordinates<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept></target>
          </trans-unit>
          <trans-unit id="200" xml:space="preserve">
            <source>Once you generate additional features, you can either add them as columns to the existing table or create a new table with the additional features and primary key, which can then be joined with the original table.</source>
            <target state="new">Once you generate additional features, you can either add them as columns to the existing table or create a new table with the additional features and primary key, which can then be joined with the original table.</target>
          </trans-unit>
          <trans-unit id="201" xml:space="preserve">
            <source><bpt id="1">&lt;html&gt;</bpt><ept id="1">&lt;/html&gt;</ept><bpt id="2">&lt;html&gt;</bpt><ept id="2">&lt;/html&gt;</ept> Frequency based Feature Generation</source>
            <target state="new"><bpt id="1">&lt;html&gt;</bpt><ept id="1">&lt;/html&gt;</ept><bpt id="2">&lt;html&gt;</bpt><ept id="2">&lt;/html&gt;</ept> Frequency based Feature Generation</target>
          </trans-unit>
          <trans-unit id="202" xml:space="preserve">
            <source>Sometimes, it is valuable to calculate the frequencies of the levels of a categorical variable, or the frequencies of level combinations of multiple categorical variables.</source>
            <target state="new">Sometimes, it is valuable to calculate the frequencies of the levels of a categorical variable, or the frequencies of level combinations of multiple categorical variables.</target>
          </trans-unit>
          <trans-unit id="203" xml:space="preserve">
            <source>Users can use the following script to calculate the frequencies:</source>
            <target state="new">Users can use the following script to calculate the frequencies:</target>
          </trans-unit>
          <trans-unit id="204" xml:space="preserve">
            <source><bpt id="1">&lt;html&gt;</bpt><ept id="1">&lt;/html&gt;</ept><bpt id="2">&lt;html&gt;</bpt><ept id="2">&lt;/html&gt;</ept> Risks of Categorical Variables in Binary Classification</source>
            <target state="new"><bpt id="1">&lt;html&gt;</bpt><ept id="1">&lt;/html&gt;</ept><bpt id="2">&lt;html&gt;</bpt><ept id="2">&lt;/html&gt;</ept> Risks of Categorical Variables in Binary Classification</target>
          </trans-unit>
          <trans-unit id="205" xml:space="preserve">
            <source>In binary classification, sometimes we need to convert non-numeric categorical variables into numeric features by replacing the non-numeric levels with numeric risks, since some models might only take numeric features.</source>
            <target state="new">In binary classification, sometimes we need to convert non-numeric categorical variables into numeric features by replacing the non-numeric levels with numeric risks, since some models might only take numeric features.</target>
          </trans-unit>
          <trans-unit id="206" xml:space="preserve">
            <source>In this section, we show some generic Hive queries of calculating the risk values (log odds) of a categorical variable.</source>
            <target state="new">In this section, we show some generic Hive queries of calculating the risk values (log odds) of a categorical variable.</target>
          </trans-unit>
          <trans-unit id="207" xml:space="preserve">
            <source>In this example, variables <bpt id="2">&lt;code&gt;</bpt>smooth_param1<ept id="2">&lt;/code&gt;</ept> and <bpt id="4">&lt;code&gt;</bpt>smooth_param2<ept id="4">&lt;/code&gt;</ept> are set to smooth the risk values calculated from data.</source>
            <target state="new">In this example, variables <bpt id="2">&lt;code&gt;</bpt>smooth_param1<ept id="2">&lt;/code&gt;</ept> and <bpt id="4">&lt;code&gt;</bpt>smooth_param2<ept id="4">&lt;/code&gt;</ept> are set to smooth the risk values calculated from data.</target>
          </trans-unit>
          <trans-unit id="208" xml:space="preserve">
            <source>Risks are ranged between -Inf and Inf.</source>
            <target state="new">Risks are ranged between -Inf and Inf.</target>
          </trans-unit>
          <trans-unit id="209" xml:space="preserve">
            <source>Risks&gt;0 stands for the probability that the target equals 1 is greater than 0.5.</source>
            <target state="new">Risks&gt;0 stands for the probability that the target equals 1 is greater than 0.5.</target>
          </trans-unit>
          <trans-unit id="210" xml:space="preserve">
            <source>After the risk table is calculated, users can assign risk values to a table by joining it with the risk table.</source>
            <target state="new">After the risk table is calculated, users can assign risk values to a table by joining it with the risk table.</target>
          </trans-unit>
          <trans-unit id="211" xml:space="preserve">
            <source>The Hive joining query has been given in previous section.</source>
            <target state="new">The Hive joining query has been given in previous section.</target>
          </trans-unit>
          <trans-unit id="212" xml:space="preserve">
            <source><bpt id="1">&lt;html&gt;</bpt><ept id="1">&lt;/html&gt;</ept><bpt id="2">&lt;html&gt;</bpt><ept id="2">&lt;/html&gt;</ept> Extract features from Datetime Fields</source>
            <target state="new"><bpt id="1">&lt;html&gt;</bpt><ept id="1">&lt;/html&gt;</ept><bpt id="2">&lt;html&gt;</bpt><ept id="2">&lt;/html&gt;</ept> Extract features from Datetime Fields</target>
          </trans-unit>
          <trans-unit id="213" xml:space="preserve">
            <source>Hive comes along with a set of UDFs for processing datetime fields.</source>
            <target state="new">Hive comes along with a set of UDFs for processing datetime fields.</target>
          </trans-unit>
          <trans-unit id="214" xml:space="preserve">
            <source>In Hive, the default datetime format is 'yyyy-MM-dd 00:00:00' (like '1970-01-01 12:21:32').</source>
            <target state="new">In Hive, the default datetime format is 'yyyy-MM-dd 00:00:00' (like '1970-01-01 12:21:32').</target>
          </trans-unit>
          <trans-unit id="215" xml:space="preserve">
            <source>In this section, we show examples of extracting the day of a month, and the month from a datetime field, and examples of converting a datetime string in a format other than the default format to a datetime string in default format.</source>
            <target state="new">In this section, we show examples of extracting the day of a month, and the month from a datetime field, and examples of converting a datetime string in a format other than the default format to a datetime string in default format.</target>
          </trans-unit>
          <trans-unit id="216" xml:space="preserve">
            <source>This Hive query assumes that the <bpt id="2">&lt;code&gt;</bpt>&lt;datetime field&gt;<ept id="2">&lt;/code&gt;</ept> is in the default datetime format.</source>
            <target state="new">This Hive query assumes that the <bpt id="2">&lt;code&gt;</bpt>&lt;datetime field&gt;<ept id="2">&lt;/code&gt;</ept> is in the default datetime format.</target>
          </trans-unit>
          <trans-unit id="217" xml:space="preserve">
            <source>If a datetime field is not in the default format, we need to first convert the datetile field into Unix time stamp, and then convert the Unix time stamp to a datetime string in the default format.</source>
            <target state="new">If a datetime field is not in the default format, we need to first convert the datetile field into Unix time stamp, and then convert the Unix time stamp to a datetime string in the default format.</target>
          </trans-unit>
          <trans-unit id="218" xml:space="preserve">
            <source>After the datetime is in default format, users can apply the embedded datetime UDFs to extract features.</source>
            <target state="new">After the datetime is in default format, users can apply the embedded datetime UDFs to extract features.</target>
          </trans-unit>
          <trans-unit id="219" xml:space="preserve">
            <source>In this query, if the <bpt id="2">&lt;code&gt;</bpt>&lt;datetime field&gt;<ept id="2">&lt;/code&gt;</ept> has the pattern like <bpt id="4">&lt;code&gt;</bpt>03/26/2015 12:04:39<ept id="4">&lt;/code&gt;</ept>, the <bpt id="6">&lt;code&gt;</bpt>'&lt;pattern of the datetime field&gt;'<ept id="6">&lt;/code&gt;</ept> should be <bpt id="8">&lt;code&gt;</bpt>'MM/dd/yyyy HH:mm:ss'<ept id="8">&lt;/code&gt;</ept>. To test it, users can run</source>
            <target state="new">In this query, if the <bpt id="2">&lt;code&gt;</bpt>&lt;datetime field&gt;<ept id="2">&lt;/code&gt;</ept> has the pattern like <bpt id="4">&lt;code&gt;</bpt>03/26/2015 12:04:39<ept id="4">&lt;/code&gt;</ept>, the <bpt id="6">&lt;code&gt;</bpt>'&lt;pattern of the datetime field&gt;'<ept id="6">&lt;/code&gt;</ept> should be <bpt id="8">&lt;code&gt;</bpt>'MM/dd/yyyy HH:mm:ss'<ept id="8">&lt;/code&gt;</ept>. To test it, users can run</target>
          </trans-unit>
          <trans-unit id="220" xml:space="preserve">
            <source>In this query, <bpt id="2">&lt;code&gt;</bpt>hivesampletable<ept id="2">&lt;/code&gt;</ept> comes with all Azure HDInsight Hadoop clusters by default when the clusters are provisioned.</source>
            <target state="new">In this query, <bpt id="2">&lt;code&gt;</bpt>hivesampletable<ept id="2">&lt;/code&gt;</ept> comes with all Azure HDInsight Hadoop clusters by default when the clusters are provisioned.</target>
          </trans-unit>
          <trans-unit id="221" xml:space="preserve">
            <source><bpt id="1">&lt;html&gt;</bpt><ept id="1">&lt;/html&gt;</ept><bpt id="2">&lt;html&gt;</bpt><ept id="2">&lt;/html&gt;</ept> Extract features from Text Fields</source>
            <target state="new"><bpt id="1">&lt;html&gt;</bpt><ept id="1">&lt;/html&gt;</ept><bpt id="2">&lt;html&gt;</bpt><ept id="2">&lt;/html&gt;</ept> Extract features from Text Fields</target>
          </trans-unit>
          <trans-unit id="222" xml:space="preserve">
            <source>Assume that the Hive table has a text field, which is a string of words separated by space, the following query extract the length of the string, and the number of words in the string.</source>
            <target state="new">Assume that the Hive table has a text field, which is a string of words separated by space, the following query extract the length of the string, and the number of words in the string.</target>
          </trans-unit>
          <trans-unit id="223" xml:space="preserve">
            <source><bpt id="1">&lt;html&gt;</bpt><ept id="1">&lt;/html&gt;</ept><bpt id="2">&lt;html&gt;</bpt><ept id="2">&lt;/html&gt;</ept> Calculate distance between GPS coordinates</source>
            <target state="new"><bpt id="1">&lt;html&gt;</bpt><ept id="1">&lt;/html&gt;</ept><bpt id="2">&lt;html&gt;</bpt><ept id="2">&lt;/html&gt;</ept> Calculate distance between GPS coordinates</target>
          </trans-unit>
          <trans-unit id="224" xml:space="preserve">
            <source>The query given in this section can be directly applied on the NYC Taxi Trip Data.</source>
            <target state="new">The query given in this section can be directly applied on the NYC Taxi Trip Data.</target>
          </trans-unit>
          <trans-unit id="225" xml:space="preserve">
            <source>The purpose of this query is to show how to apply the embedded mathematical functions in Hive to generate features.</source>
            <target state="new">The purpose of this query is to show how to apply the embedded mathematical functions in Hive to generate features.</target>
          </trans-unit>
          <trans-unit id="226" xml:space="preserve">
            <source>The fields that are used in this query are GPS coordinates of pickup and dropoff locations, named pickup\_longitude, pickup\_latitude, dropoff\_longitude, and dropoff\_latitude.</source>
            <target state="new">The fields that are used in this query are GPS coordinates of pickup and dropoff locations, named pickup\_longitude, pickup\_latitude, dropoff\_longitude, and dropoff\_latitude.</target>
          </trans-unit>
          <trans-unit id="227" xml:space="preserve">
            <source>The queries to calculate the direct distance between the pickup and dropoff coordinates are:</source>
            <target state="new">The queries to calculate the direct distance between the pickup and dropoff coordinates are:</target>
          </trans-unit>
          <trans-unit id="228" xml:space="preserve">
            <source>The mathematical equations of calculating distance between two GPS coordinates can be found at <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>here<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept>, authored by Peter Lapisu.</source>
            <target state="new">The mathematical equations of calculating distance between two GPS coordinates can be found at <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>here<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept>, authored by Peter Lapisu.</target>
          </trans-unit>
          <trans-unit id="229" xml:space="preserve">
            <source>In his Javascript, the function toRad() is just <bpt id="2">&lt;code&gt;</bpt>lat_or_lon*pi/180<ept id="2">&lt;/code&gt;</ept>, which converts degrees to radians.</source>
            <target state="new">In his Javascript, the function toRad() is just <bpt id="2">&lt;code&gt;</bpt>lat_or_lon*pi/180<ept id="2">&lt;/code&gt;</ept>, which converts degrees to radians.</target>
          </trans-unit>
          <trans-unit id="230" xml:space="preserve">
            <source>Here, <bpt id="2">&lt;code&gt;</bpt>lat_or_lon<ept id="2">&lt;/code&gt;</ept> is the latitude or longitude.</source>
            <target state="new">Here, <bpt id="2">&lt;code&gt;</bpt>lat_or_lon<ept id="2">&lt;/code&gt;</ept> is the latitude or longitude.</target>
          </trans-unit>
          <trans-unit id="231" xml:space="preserve">
            <source>Since Hive does not provide function <bpt id="2">&lt;code&gt;</bpt>atan2<ept id="2">&lt;/code&gt;</ept>, but provides function <bpt id="4">&lt;code&gt;</bpt>atan<ept id="4">&lt;/code&gt;</ept>, the <bpt id="6">&lt;code&gt;</bpt>atan2<ept id="6">&lt;/code&gt;</ept> function is implemented by <bpt id="8">&lt;code&gt;</bpt>atan<ept id="8">&lt;/code&gt;</ept> function in the above Hive query, based on its definition in <bpt id="10CapsExtId1">&lt;link&gt;</bpt><bpt id="10CapsExtId2">&lt;linkText&gt;</bpt>Wikipedia<ept id="10CapsExtId2">&lt;/linkText&gt;</ept><bpt id="10CapsExtId3">&lt;title&gt;</bpt><ept id="10CapsExtId3">&lt;/title&gt;</ept><ept id="10CapsExtId1">&lt;/link&gt;</ept>.</source>
            <target state="new">Since Hive does not provide function <bpt id="2">&lt;code&gt;</bpt>atan2<ept id="2">&lt;/code&gt;</ept>, but provides function <bpt id="4">&lt;code&gt;</bpt>atan<ept id="4">&lt;/code&gt;</ept>, the <bpt id="6">&lt;code&gt;</bpt>atan2<ept id="6">&lt;/code&gt;</ept> function is implemented by <bpt id="8">&lt;code&gt;</bpt>atan<ept id="8">&lt;/code&gt;</ept> function in the above Hive query, based on its definition in <bpt id="10CapsExtId1">&lt;link&gt;</bpt><bpt id="10CapsExtId2">&lt;linkText&gt;</bpt>Wikipedia<ept id="10CapsExtId2">&lt;/linkText&gt;</ept><bpt id="10CapsExtId3">&lt;title&gt;</bpt><ept id="10CapsExtId3">&lt;/title&gt;</ept><ept id="10CapsExtId1">&lt;/link&gt;</ept>.</target>
          </trans-unit>
          <trans-unit id="232" xml:space="preserve">
            <source><bpt id="1">&lt;linkText&gt;</bpt>Create workspace<ept id="1">&lt;/linkText&gt;</ept></source>
            <target state="new"><bpt id="1">&lt;linkText&gt;</bpt>Create workspace<ept id="1">&lt;/linkText&gt;</ept></target>
          </trans-unit>
          <trans-unit id="233" xml:space="preserve">
            <source>A full list of Hive embedded UDFs can be found <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>here<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept>.</source>
            <target state="new">A full list of Hive embedded UDFs can be found <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>here<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept>.</target>
          </trans-unit>
          <trans-unit id="234" xml:space="preserve">
            <source><bpt id="1">&lt;html&gt;</bpt><ept id="1">&lt;/html&gt;</ept><bpt id="2">&lt;html&gt;</bpt><ept id="2">&lt;/html&gt;</ept> Advanced topics: Tune Hive Parameters to Improve Query Speed</source>
            <target state="new"><bpt id="1">&lt;html&gt;</bpt><ept id="1">&lt;/html&gt;</ept><bpt id="2">&lt;html&gt;</bpt><ept id="2">&lt;/html&gt;</ept> Advanced topics: Tune Hive Parameters to Improve Query Speed</target>
          </trans-unit>
          <trans-unit id="235" xml:space="preserve">
            <source>The default parameter settings of Hive cluster might not be suitable for the Hive queries and the data the queries are processing.</source>
            <target state="new">The default parameter settings of Hive cluster might not be suitable for the Hive queries and the data the queries are processing.</target>
          </trans-unit>
          <trans-unit id="236" xml:space="preserve">
            <source>In this section, we discuss some parameters that users can tune so that the performance of Hive queries can be improved.</source>
            <target state="new">In this section, we discuss some parameters that users can tune so that the performance of Hive queries can be improved.</target>
          </trans-unit>
          <trans-unit id="237" xml:space="preserve">
            <source>Users need to add the parameter tuning queries before the queries of processing data.</source>
            <target state="new">Users need to add the parameter tuning queries before the queries of processing data.</target>
          </trans-unit>
          <trans-unit id="238" xml:space="preserve">
            <source>Java heap space : For queries involving joining large datasets, or processing long records, a typical error is <bpt id="2">&lt;strong&gt;</bpt>running out of heap space<ept id="2">&lt;/strong&gt;</ept>.</source>
            <target state="new">Java heap space : For queries involving joining large datasets, or processing long records, a typical error is <bpt id="2">&lt;strong&gt;</bpt>running out of heap space<ept id="2">&lt;/strong&gt;</ept>.</target>
          </trans-unit>
          <trans-unit id="239" xml:space="preserve">
            <source>This can be tuned by setting parameters <bpt id="2">&lt;code&gt;</bpt>mapreduce.map.java.opts<ept id="2">&lt;/code&gt;</ept> and <bpt id="4">&lt;code&gt;</bpt>mapreduce.task.io.sort.mb<ept id="4">&lt;/code&gt;</ept> to desired values.</source>
            <target state="new">This can be tuned by setting parameters <bpt id="2">&lt;code&gt;</bpt>mapreduce.map.java.opts<ept id="2">&lt;/code&gt;</ept> and <bpt id="4">&lt;code&gt;</bpt>mapreduce.task.io.sort.mb<ept id="4">&lt;/code&gt;</ept> to desired values.</target>
          </trans-unit>
          <trans-unit id="240" xml:space="preserve">
            <source>Here is an example:</source>
            <target state="new">Here is an example:</target>
          </trans-unit>
          <trans-unit id="241" xml:space="preserve">
            <source>This parameter allocates 4GB memory to Java heap space and also makes sorting more efficient by allocating more memory for it.</source>
            <target state="new">This parameter allocates 4GB memory to Java heap space and also makes sorting more efficient by allocating more memory for it.</target>
          </trans-unit>
          <trans-unit id="242" xml:space="preserve">
            <source>It is a good idea to play with it if there are any job failure errors related to heap space.</source>
            <target state="new">It is a good idea to play with it if there are any job failure errors related to heap space.</target>
          </trans-unit>
          <trans-unit id="243" xml:space="preserve">
            <source>DFS block size : This parameter sets the smallest unit of data that the file system stores.</source>
            <target state="new">DFS block size : This parameter sets the smallest unit of data that the file system stores.</target>
          </trans-unit>
          <trans-unit id="244" xml:space="preserve">
            <source>As an example, if the DFS block size is 128MB, then any data of size less than and up to 128MB is stored in a single block, while data that is larger than 128MB is allotted extra blocks.</source>
            <target state="new">As an example, if the DFS block size is 128MB, then any data of size less than and up to 128MB is stored in a single block, while data that is larger than 128MB is allotted extra blocks.</target>
          </trans-unit>
          <trans-unit id="245" xml:space="preserve">
            <source>Choosing a very small block size causes large overheads in Hadoop since the name node has to process many more requests to find the relevant block pertaining to the file.</source>
            <target state="new">Choosing a very small block size causes large overheads in Hadoop since the name node has to process many more requests to find the relevant block pertaining to the file.</target>
          </trans-unit>
          <trans-unit id="246" xml:space="preserve">
            <source>A recommended setting when dealing with gigabytes (or larger) data is :</source>
            <target state="new">A recommended setting when dealing with gigabytes (or larger) data is :</target>
          </trans-unit>
          <trans-unit id="247" xml:space="preserve">
            <source>Optimizing join operation in Hive : While join operations in the map/reduce framework typically take place in the reduce phase, some times, enormous gains can be achieved by scheduling joins in the map phase (also called "mapjoins").</source>
            <target state="new">Optimizing join operation in Hive : While join operations in the map/reduce framework typically take place in the reduce phase, some times, enormous gains can be achieved by scheduling joins in the map phase (also called "mapjoins").</target>
          </trans-unit>
          <trans-unit id="248" xml:space="preserve">
            <source>To direct Hive to do this whenever possible, we can set :</source>
            <target state="new">To direct Hive to do this whenever possible, we can set :</target>
          </trans-unit>
          <trans-unit id="249" xml:space="preserve">
            <source>Specifying the number of mappers to Hive : While Hadoop allows the user to set the number of reducers, the number of mappers may typically not be set by the user.</source>
            <target state="new">Specifying the number of mappers to Hive : While Hadoop allows the user to set the number of reducers, the number of mappers may typically not be set by the user.</target>
          </trans-unit>
          <trans-unit id="250" xml:space="preserve">
            <source>A trick that allows some degree of control on this number is to choose the hadoop variables, mapred.min.split.size and mapred.max.split.size.</source>
            <target state="new">A trick that allows some degree of control on this number is to choose the hadoop variables, mapred.min.split.size and mapred.max.split.size.</target>
          </trans-unit>
          <trans-unit id="251" xml:space="preserve">
            <source>The size of each map task is determined by :</source>
            <target state="new">The size of each map task is determined by :</target>
          </trans-unit>
          <trans-unit id="252" xml:space="preserve">
            <source>Typically, the default value of mapred.min.split.size is 0, that of mapred.max.split.size is Long.MAX and that of dfs.block.size is 64MB.</source>
            <target state="new">Typically, the default value of mapred.min.split.size is 0, that of mapred.max.split.size is Long.MAX and that of dfs.block.size is 64MB.</target>
          </trans-unit>
          <trans-unit id="253" xml:space="preserve">
            <source>As we can see, given the data size, tuning these parameters by "setting" them allows us to tune the number of mappers used.</source>
            <target state="new">As we can see, given the data size, tuning these parameters by "setting" them allows us to tune the number of mappers used.</target>
          </trans-unit>
          <trans-unit id="254" xml:space="preserve">
            <source>A few other more advanced options for optimizing Hive performance are mentioned below.</source>
            <target state="new">A few other more advanced options for optimizing Hive performance are mentioned below.</target>
          </trans-unit>
          <trans-unit id="255" xml:space="preserve">
            <source>These allow for the setting of memory allocated to map and reduce tasks, and can be useful in tweaking performance.</source>
            <target state="new">These allow for the setting of memory allocated to map and reduce tasks, and can be useful in tweaking performance.</target>
          </trans-unit>
          <trans-unit id="256" xml:space="preserve">
            <source>Please keep in mind that the <bpt id="2">&lt;code&gt;</bpt>mapreduce.reduce.memory.mb<ept id="2">&lt;/code&gt;</ept> cannot be greater than the physical memory size of each worker node in the Hadoop cluster.</source>
            <target state="new">Please keep in mind that the <bpt id="2">&lt;code&gt;</bpt>mapreduce.reduce.memory.mb<ept id="2">&lt;/code&gt;</ept> cannot be greater than the physical memory size of each worker node in the Hadoop cluster.</target>
          </trans-unit>
        </group>
      </group>
    </body>
  </file>
</xliff>