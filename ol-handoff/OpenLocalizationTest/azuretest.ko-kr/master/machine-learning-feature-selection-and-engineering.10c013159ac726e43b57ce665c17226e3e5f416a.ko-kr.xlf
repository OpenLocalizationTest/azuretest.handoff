<?xml version="1.0" encoding="utf-8"?>
<xliff version="1.2" xmlns="urn:oasis:names:tc:xliff:document:1.2" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="urn:oasis:names:tc:xliff:document:1.2 xliff-core-1.2-transitional.xsd">
  <file datatype="xml" source-language="en-us" target-language="ko-kr" original="2/21/2016 3:30:10 AM" tool-id="MarkdownTransformer" product-name="N/A" product-version="N/A" build-num="1">
    <header>
      <xliffext:olfilehash xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">45641793fb9873ceec8df0d7a1052ec44fc14334</xliffext:olfilehash>
      <tool tool-id="MarkdownTransformer" tool-name="MarkdownToXliff" tool-version="1.0" tool-company="Microsoft" />
    </header>
    <body>
      <group extype="content">
        <group id="101">
          <trans-unit id="101" xml:space="preserve">
            <source>Feature Engineering and Selection in Azure Machine Learning | Microsoft Azure</source>
            <target state="new">Feature Engineering and Selection in Azure Machine Learning | Microsoft Azure</target>
          </trans-unit>
          <trans-unit id="102" xml:space="preserve">
            <source>Explains the purposes of feature selection and feature engineering and provides examples of their role in the data enhancement process of machine learning.</source>
            <target state="new">Explains the purposes of feature selection and feature engineering and provides examples of their role in the data enhancement process of machine learning.</target>
          </trans-unit>
          <trans-unit id="103" xml:space="preserve">
            <source>Feature engineering and selection in Azure Machine Learning</source>
            <target state="new">Feature engineering and selection in Azure Machine Learning</target>
          </trans-unit>
          <trans-unit id="104" xml:space="preserve">
            <source>This topic explains the purposes of feature engineering and feature selection in the data enhancement process of machine learning.</source>
            <target state="new">This topic explains the purposes of feature engineering and feature selection in the data enhancement process of machine learning.</target>
          </trans-unit>
          <trans-unit id="105" xml:space="preserve">
            <source>It illustrates what these processes involve using examples provided by Azure Machine Learning Studio.</source>
            <target state="new">It illustrates what these processes involve using examples provided by Azure Machine Learning Studio.</target>
          </trans-unit>
          <trans-unit id="106" xml:space="preserve">
            <source><ph id="1">&lt;token href="../../includes/machine-learning-free-trial.md"/&gt;</ph></source>
            <target state="new"><ph id="1">&lt;token href="../../includes/machine-learning-free-trial.md"/&gt;</ph></target>
          </trans-unit>
          <trans-unit id="107" xml:space="preserve">
            <source>The training data used in machine learning can often be enhanced by the selection or extraction of features from the raw data collected.</source>
            <target state="new">The training data used in machine learning can often be enhanced by the selection or extraction of features from the raw data collected.</target>
          </trans-unit>
          <trans-unit id="108" xml:space="preserve">
            <source>A  example of an engineered feature in the context of learning how to classify the images of handwritten characters is a bit density map constructed from the raw bit distribution data.</source>
            <target state="new">A  example of an engineered feature in the context of learning how to classify the images of handwritten characters is a bit density map constructed from the raw bit distribution data.</target>
          </trans-unit>
          <trans-unit id="109" xml:space="preserve">
            <source>This map can help locate the edges of the characters more efficiently than the raw distribution.</source>
            <target state="new">This map can help locate the edges of the characters more efficiently than the raw distribution.</target>
          </trans-unit>
          <trans-unit id="110" xml:space="preserve">
            <source>Engineered and selected features increase the efficiency of the training process which attempts to extract the key information contained in the data.</source>
            <target state="new">Engineered and selected features increase the efficiency of the training process which attempts to extract the key information contained in the data.</target>
          </trans-unit>
          <trans-unit id="111" xml:space="preserve">
            <source>They also improve the power of these models to classify the input data accurately and to predict outcomes of interest more robustly.</source>
            <target state="new">They also improve the power of these models to classify the input data accurately and to predict outcomes of interest more robustly.</target>
          </trans-unit>
          <trans-unit id="112" xml:space="preserve">
            <source>Feature engineering and selection can also combine to make the learning more computationally tractable.</source>
            <target state="new">Feature engineering and selection can also combine to make the learning more computationally tractable.</target>
          </trans-unit>
          <trans-unit id="113" xml:space="preserve">
            <source>It does so by enhancing and then reducing the number of features needed to calibrate or train a model.</source>
            <target state="new">It does so by enhancing and then reducing the number of features needed to calibrate or train a model.</target>
          </trans-unit>
          <trans-unit id="114" xml:space="preserve">
            <source>Mathematically speaking, the features selected to train the model are a minimal set of independent variables that explain the patterns in the data and then predict outcomes successfully.</source>
            <target state="new">Mathematically speaking, the features selected to train the model are a minimal set of independent variables that explain the patterns in the data and then predict outcomes successfully.</target>
          </trans-unit>
          <trans-unit id="115" xml:space="preserve">
            <source>The engineering and selection of features is one part of a larger process, which typically consists of four steps:</source>
            <target state="new">The engineering and selection of features is one part of a larger process, which typically consists of four steps:</target>
          </trans-unit>
          <trans-unit id="116" xml:space="preserve">
            <source>data collection</source>
            <target state="new">data collection</target>
          </trans-unit>
          <trans-unit id="117" xml:space="preserve">
            <source>data enhancement</source>
            <target state="new">data enhancement</target>
          </trans-unit>
          <trans-unit id="118" xml:space="preserve">
            <source>model construction</source>
            <target state="new">model construction</target>
          </trans-unit>
          <trans-unit id="119" xml:space="preserve">
            <source>post-processing</source>
            <target state="new">post-processing</target>
          </trans-unit>
          <trans-unit id="120" xml:space="preserve">
            <source>Engineering and selection are the <bpt id="2">&lt;strong&gt;</bpt>data enhancement<ept id="2">&lt;/strong&gt;</ept> step of machine learning.</source>
            <target state="new">Engineering and selection are the <bpt id="2">&lt;strong&gt;</bpt>data enhancement<ept id="2">&lt;/strong&gt;</ept> step of machine learning.</target>
          </trans-unit>
          <trans-unit id="121" xml:space="preserve">
            <source>Three aspects of this process may be distinguished for our purposes:</source>
            <target state="new">Three aspects of this process may be distinguished for our purposes:</target>
          </trans-unit>
          <trans-unit id="122" xml:space="preserve">
            <source><bpt id="1">&lt;strong&gt;</bpt>data pre-processing<ept id="1">&lt;/strong&gt;</ept>: This process tries to insure that the collected data is clean and consistent.</source>
            <target state="new"><bpt id="1">&lt;strong&gt;</bpt>data pre-processing<ept id="1">&lt;/strong&gt;</ept>: This process tries to insure that the collected data is clean and consistent.</target>
          </trans-unit>
          <trans-unit id="123" xml:space="preserve">
            <source>It includes tasks such as integrating multiple datasets, handling missing data, handling inconsistent data, and converting data types.</source>
            <target state="new">It includes tasks such as integrating multiple datasets, handling missing data, handling inconsistent data, and converting data types.</target>
          </trans-unit>
          <trans-unit id="124" xml:space="preserve">
            <source><bpt id="1">&lt;strong&gt;</bpt>feature engineering<ept id="1">&lt;/strong&gt;</ept>: This process attempts to create additional relevant features from the existing raw features in the data, and to increase predictive power to the learning algorithm.</source>
            <target state="new"><bpt id="1">&lt;strong&gt;</bpt>feature engineering<ept id="1">&lt;/strong&gt;</ept>: This process attempts to create additional relevant features from the existing raw features in the data, and to increase predictive power to the learning algorithm.</target>
          </trans-unit>
          <trans-unit id="125" xml:space="preserve">
            <source><bpt id="1">&lt;strong&gt;</bpt>feature selection<ept id="1">&lt;/strong&gt;</ept>: This process selects the key subset of original data features in an attempt to reduce the dimensionality of the training problem.</source>
            <target state="new"><bpt id="1">&lt;strong&gt;</bpt>feature selection<ept id="1">&lt;/strong&gt;</ept>: This process selects the key subset of original data features in an attempt to reduce the dimensionality of the training problem.</target>
          </trans-unit>
          <trans-unit id="126" xml:space="preserve">
            <source>This topic only covers the feature engineering and feature selection aspects of the data enhancement process.</source>
            <target state="new">This topic only covers the feature engineering and feature selection aspects of the data enhancement process.</target>
          </trans-unit>
          <trans-unit id="127" xml:space="preserve">
            <source>For additional information on the data pre-processing step, see the <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>Pre-processing Data in Azure ML Studio<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept> video.</source>
            <target state="new">For additional information on the data pre-processing step, see the <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>Pre-processing Data in Azure ML Studio<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept> video.</target>
          </trans-unit>
          <trans-unit id="128" xml:space="preserve">
            <source>Creating Features from Your Data - Feature Engineering</source>
            <target state="new">Creating Features from Your Data - Feature Engineering</target>
          </trans-unit>
          <trans-unit id="129" xml:space="preserve">
            <source>The training data consists of a matrix composed of examples (records or observations stored in rows), each of which has a set of features (variables or fields stored in columns).</source>
            <target state="new">The training data consists of a matrix composed of examples (records or observations stored in rows), each of which has a set of features (variables or fields stored in columns).</target>
          </trans-unit>
          <trans-unit id="130" xml:space="preserve">
            <source>The features specified in the experimental design are expected to characterize the patterns in the data.</source>
            <target state="new">The features specified in the experimental design are expected to characterize the patterns in the data.</target>
          </trans-unit>
          <trans-unit id="131" xml:space="preserve">
            <source>Although many of the raw data fields can be directly included in the selected feature set used to train a model, it is often the case that additional (engineered) features need to be constructed from the features in the raw data to generate an enhanced training dataset.</source>
            <target state="new">Although many of the raw data fields can be directly included in the selected feature set used to train a model, it is often the case that additional (engineered) features need to be constructed from the features in the raw data to generate an enhanced training dataset.</target>
          </trans-unit>
          <trans-unit id="132" xml:space="preserve">
            <source>What kind of features should be created to enhance the dataset when training a model?</source>
            <target state="new">What kind of features should be created to enhance the dataset when training a model?</target>
          </trans-unit>
          <trans-unit id="133" xml:space="preserve">
            <source>Engineered features that enhance the training provide information that better differentiates the patterns in the data.</source>
            <target state="new">Engineered features that enhance the training provide information that better differentiates the patterns in the data.</target>
          </trans-unit>
          <trans-unit id="134" xml:space="preserve">
            <source>We expect the new features to provide additional information that is not clearly captured or easily apparent in the original or existing feature set.</source>
            <target state="new">We expect the new features to provide additional information that is not clearly captured or easily apparent in the original or existing feature set.</target>
          </trans-unit>
          <trans-unit id="135" xml:space="preserve">
            <source>But this process is something of an art.</source>
            <target state="new">But this process is something of an art.</target>
          </trans-unit>
          <trans-unit id="136" xml:space="preserve">
            <source>Sound and productive decisions often require some domain expertise.</source>
            <target state="new">Sound and productive decisions often require some domain expertise.</target>
          </trans-unit>
          <trans-unit id="137" xml:space="preserve">
            <source>When starting with Azure Machine Learning, it is easiest to grasp this process concretely using samples provided in the Studio.</source>
            <target state="new">When starting with Azure Machine Learning, it is easiest to grasp this process concretely using samples provided in the Studio.</target>
          </trans-unit>
          <trans-unit id="138" xml:space="preserve">
            <source>Two examples are presented here:</source>
            <target state="new">Two examples are presented here:</target>
          </trans-unit>
          <trans-unit id="139" xml:space="preserve">
            <source>A regression example <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>Prediction of the number of bike rentals<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept> in a supervised experiment where the target values are known</source>
            <target state="new">A regression example <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>Prediction of the number of bike rentals<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept> in a supervised experiment where the target values are known</target>
          </trans-unit>
          <trans-unit id="140" xml:space="preserve">
            <source>A text mining classification example using <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>Feature Hashing[feature-hashing]<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept></source>
            <target state="new">A text mining classification example using <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>Feature Hashing[feature-hashing]<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept></target>
          </trans-unit>
          <trans-unit id="141" xml:space="preserve">
            <source>Example 1: Adding Temporal Features for Regression Model</source>
            <target state="new">Example 1: Adding Temporal Features for Regression Model</target>
          </trans-unit>
          <trans-unit id="142" xml:space="preserve">
            <source>Let's use the experiment "Demand forecasting of bikes" in Azure Machine Learning Studio to demonstrate how to engineer features for a regression task.</source>
            <target state="new">Let's use the experiment "Demand forecasting of bikes" in Azure Machine Learning Studio to demonstrate how to engineer features for a regression task.</target>
          </trans-unit>
          <trans-unit id="143" xml:space="preserve">
            <source>The objective of this experiment is to predict the demand for the bikes, that is, the number of bike rentals within a specific month/day/hour.</source>
            <target state="new">The objective of this experiment is to predict the demand for the bikes, that is, the number of bike rentals within a specific month/day/hour.</target>
          </trans-unit>
          <trans-unit id="144" xml:space="preserve">
            <source>The dataset "Bike Rental UCI dataset" is used as the raw input data.</source>
            <target state="new">The dataset "Bike Rental UCI dataset" is used as the raw input data.</target>
          </trans-unit>
          <trans-unit id="145" xml:space="preserve">
            <source>This dataset is based on real data from the Capital Bikeshare company that maintains a bike rental network in Washington DC in the United States.</source>
            <target state="new">This dataset is based on real data from the Capital Bikeshare company that maintains a bike rental network in Washington DC in the United States.</target>
          </trans-unit>
          <trans-unit id="146" xml:space="preserve">
            <source>The dataset represents the number of bike rentals within a specific hour of a day in the years 2011 and year 2012 and contains 17379 rows and 17 columns.</source>
            <target state="new">The dataset represents the number of bike rentals within a specific hour of a day in the years 2011 and year 2012 and contains 17379 rows and 17 columns.</target>
          </trans-unit>
          <trans-unit id="147" xml:space="preserve">
            <source>The raw feature set contains weather conditions (temperature/humidity/wind speed) and the type of the day (holiday/weekday).</source>
            <target state="new">The raw feature set contains weather conditions (temperature/humidity/wind speed) and the type of the day (holiday/weekday).</target>
          </trans-unit>
          <trans-unit id="148" xml:space="preserve">
            <source>The field to predict is "cnt", a count which represents the bike rentals within a specific hour and which ranges ranges from 1 to 977.</source>
            <target state="new">The field to predict is "cnt", a count which represents the bike rentals within a specific hour and which ranges ranges from 1 to 977.</target>
          </trans-unit>
          <trans-unit id="149" xml:space="preserve">
            <source>With the goal of constructing effective features in the training data, four regression models are built using the same algorithm but with four different training datasets.</source>
            <target state="new">With the goal of constructing effective features in the training data, four regression models are built using the same algorithm but with four different training datasets.</target>
          </trans-unit>
          <trans-unit id="150" xml:space="preserve">
            <source>The four datasets represent the same raw input data, but with an increasing number of features set.</source>
            <target state="new">The four datasets represent the same raw input data, but with an increasing number of features set.</target>
          </trans-unit>
          <trans-unit id="151" xml:space="preserve">
            <source>These features are grouped into four categories:</source>
            <target state="new">These features are grouped into four categories:</target>
          </trans-unit>
          <trans-unit id="152" xml:space="preserve">
            <source>A = weather + holiday + weekday + weekend features for the predicted day</source>
            <target state="new">A = weather + holiday + weekday + weekend features for the predicted day</target>
          </trans-unit>
          <trans-unit id="153" xml:space="preserve">
            <source>B = number of bikes that were rented in each of the previous 12 hours</source>
            <target state="new">B = number of bikes that were rented in each of the previous 12 hours</target>
          </trans-unit>
          <trans-unit id="154" xml:space="preserve">
            <source>C = number of bikes that were rented in each of the previous 12 days at the same hour</source>
            <target state="new">C = number of bikes that were rented in each of the previous 12 days at the same hour</target>
          </trans-unit>
          <trans-unit id="155" xml:space="preserve">
            <source>D = number of bikes that were rented in each of the previous 12 weeks at the same hour and the same day</source>
            <target state="new">D = number of bikes that were rented in each of the previous 12 weeks at the same hour and the same day</target>
          </trans-unit>
          <trans-unit id="156" xml:space="preserve">
            <source>Besides feature set A, which already exist in the original raw data, the other three sets of features are created through the feature engineering process.</source>
            <target state="new">Besides feature set A, which already exist in the original raw data, the other three sets of features are created through the feature engineering process.</target>
          </trans-unit>
          <trans-unit id="157" xml:space="preserve">
            <source>Feature set B captures very recent demand for the bikes.</source>
            <target state="new">Feature set B captures very recent demand for the bikes.</target>
          </trans-unit>
          <trans-unit id="158" xml:space="preserve">
            <source>Feature set C captures the demand for bikes at a particular hour.</source>
            <target state="new">Feature set C captures the demand for bikes at a particular hour.</target>
          </trans-unit>
          <trans-unit id="159" xml:space="preserve">
            <source>Feature set D captures demand for bikes at particular hour and particular day of the week.</source>
            <target state="new">Feature set D captures demand for bikes at particular hour and particular day of the week.</target>
          </trans-unit>
          <trans-unit id="160" xml:space="preserve">
            <source>The four training datasets each includes feature set A, A+B, A+B+C, and A+B+C+D, respectively.</source>
            <target state="new">The four training datasets each includes feature set A, A+B, A+B+C, and A+B+C+D, respectively.</target>
          </trans-unit>
          <trans-unit id="161" xml:space="preserve">
            <source>In the Azure Machine Learning experiment, these four training datasets are formed via four branches from the pre-processed input dataset.</source>
            <target state="new">In the Azure Machine Learning experiment, these four training datasets are formed via four branches from the pre-processed input dataset.</target>
          </trans-unit>
          <trans-unit id="162" xml:space="preserve">
            <source>Except the left most branch, each of these branches contains an <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>Execute R Script[execute-r-script]<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept> module, in which a set of derived features (feature set B, C, and D) are respectively constructed and appended to the imported dataset.</source>
            <target state="new">Except the left most branch, each of these branches contains an <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>Execute R Script[execute-r-script]<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept> module, in which a set of derived features (feature set B, C, and D) are respectively constructed and appended to the imported dataset.</target>
          </trans-unit>
          <trans-unit id="163" xml:space="preserve">
            <source>The following figure demonstrates the R script used to create feature set B in the second left branch.</source>
            <target state="new">The following figure demonstrates the R script used to create feature set B in the second left branch.</target>
          </trans-unit>
          <trans-unit id="164" xml:space="preserve">
            <source><bpt id="1">&lt;linkText&gt;</bpt>create features<ept id="1">&lt;/linkText&gt;</ept></source>
            <target state="new"><bpt id="1">&lt;linkText&gt;</bpt>create features<ept id="1">&lt;/linkText&gt;</ept></target>
          </trans-unit>
          <trans-unit id="165" xml:space="preserve">
            <source>The comparison of the performance results of the four models are summarized in the following table.</source>
            <target state="new">The comparison of the performance results of the four models are summarized in the following table.</target>
          </trans-unit>
          <trans-unit id="166" xml:space="preserve">
            <source>The best results are shown by features A+B+C.</source>
            <target state="new">The best results are shown by features A+B+C.</target>
          </trans-unit>
          <trans-unit id="167" xml:space="preserve">
            <source>Note that the error rate decreases when additional feature set are included in the training data.</source>
            <target state="new">Note that the error rate decreases when additional feature set are included in the training data.</target>
          </trans-unit>
          <trans-unit id="168" xml:space="preserve">
            <source>It verifies our presumption that the feature set B, C provide additional relevant information for the regression task.</source>
            <target state="new">It verifies our presumption that the feature set B, C provide additional relevant information for the regression task.</target>
          </trans-unit>
          <trans-unit id="169" xml:space="preserve">
            <source>But adding the D feature does not seem to provide any additional reduction in the error rate.</source>
            <target state="new">But adding the D feature does not seem to provide any additional reduction in the error rate.</target>
          </trans-unit>
          <trans-unit id="170" xml:space="preserve">
            <source><bpt id="1">&lt;linkText&gt;</bpt>result comparison<ept id="1">&lt;/linkText&gt;</ept></source>
            <target state="new"><bpt id="1">&lt;linkText&gt;</bpt>result comparison<ept id="1">&lt;/linkText&gt;</ept></target>
          </trans-unit>
          <trans-unit id="171" xml:space="preserve">
            <source><bpt id="1">&lt;html&gt;</bpt><ept id="1">&lt;/html&gt;</ept><bpt id="2">&lt;html&gt;</bpt><ept id="2">&lt;/html&gt;</ept> Example 2: Creating Features in Text Mining</source>
            <target state="new"><bpt id="1">&lt;html&gt;</bpt><ept id="1">&lt;/html&gt;</ept><bpt id="2">&lt;html&gt;</bpt><ept id="2">&lt;/html&gt;</ept> Example 2: Creating Features in Text Mining</target>
          </trans-unit>
          <trans-unit id="172" xml:space="preserve">
            <source>Feature engineering is widely applied in tasks related to text mining, such as document classification and sentiment analysis.</source>
            <target state="new">Feature engineering is widely applied in tasks related to text mining, such as document classification and sentiment analysis.</target>
          </trans-unit>
          <trans-unit id="173" xml:space="preserve">
            <source>For example, when we want to classify documents into several categories, a typical assumption is that the word/phrases included in one doc category are less likely to occur in another doc category.</source>
            <target state="new">For example, when we want to classify documents into several categories, a typical assumption is that the word/phrases included in one doc category are less likely to occur in another doc category.</target>
          </trans-unit>
          <trans-unit id="174" xml:space="preserve">
            <source>In another words, the frequency of the words/phrases distribution is able to characterize different document categories.</source>
            <target state="new">In another words, the frequency of the words/phrases distribution is able to characterize different document categories.</target>
          </trans-unit>
          <trans-unit id="175" xml:space="preserve">
            <source>In text mining applications, because individual pieces of text-contents usually serve as the input data, the feature engineering process is needed to create the features involving word/phrase frequencies.</source>
            <target state="new">In text mining applications, because individual pieces of text-contents usually serve as the input data, the feature engineering process is needed to create the features involving word/phrase frequencies.</target>
          </trans-unit>
          <trans-unit id="176" xml:space="preserve">
            <source>To achieve this task, a technique called <bpt id="2">&lt;strong&gt;</bpt>feature hashing<ept id="2">&lt;/strong&gt;</ept> is applied to efficiently turn arbitrary text features into indices.</source>
            <target state="new">To achieve this task, a technique called <bpt id="2">&lt;strong&gt;</bpt>feature hashing<ept id="2">&lt;/strong&gt;</ept> is applied to efficiently turn arbitrary text features into indices.</target>
          </trans-unit>
          <trans-unit id="177" xml:space="preserve">
            <source>Instead of associating each text feature (words/phrases) to a particular index, this method functions by applying a hash function to the features and using their hash values as indices directly.</source>
            <target state="new">Instead of associating each text feature (words/phrases) to a particular index, this method functions by applying a hash function to the features and using their hash values as indices directly.</target>
          </trans-unit>
          <trans-unit id="178" xml:space="preserve">
            <source>In Azure Machine Learning, there is a <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>Feature Hashing[feature-hashing]<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept> module that creates these word/phrase features conveniently.</source>
            <target state="new">In Azure Machine Learning, there is a <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>Feature Hashing[feature-hashing]<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept> module that creates these word/phrase features conveniently.</target>
          </trans-unit>
          <trans-unit id="179" xml:space="preserve">
            <source>Following figure shows an example of using this module.</source>
            <target state="new">Following figure shows an example of using this module.</target>
          </trans-unit>
          <trans-unit id="180" xml:space="preserve">
            <source>The input dataset contains two columns: the book rating ranging from 1 to 5, and the actual review content.</source>
            <target state="new">The input dataset contains two columns: the book rating ranging from 1 to 5, and the actual review content.</target>
          </trans-unit>
          <trans-unit id="181" xml:space="preserve">
            <source>The goal of this <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>Feature Hashing[feature-hashing]<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept> module is to retrieve a bunch of new features that show the occurrence frequency of the corresponding word(s)/phrase(s) within the particular book review.</source>
            <target state="new">The goal of this <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>Feature Hashing[feature-hashing]<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept> module is to retrieve a bunch of new features that show the occurrence frequency of the corresponding word(s)/phrase(s) within the particular book review.</target>
          </trans-unit>
          <trans-unit id="182" xml:space="preserve">
            <source>To use this module, we need to complete the following steps:</source>
            <target state="new">To use this module, we need to complete the following steps:</target>
          </trans-unit>
          <trans-unit id="183" xml:space="preserve">
            <source>First, select the column that contains the input text ("Col2" in this example).</source>
            <target state="new">First, select the column that contains the input text ("Col2" in this example).</target>
          </trans-unit>
          <trans-unit id="184" xml:space="preserve">
            <source>Second, set the "Hashing bitsize" to 8, which means 2^8=256 features will be created.</source>
            <target state="new">Second, set the "Hashing bitsize" to 8, which means 2^8=256 features will be created.</target>
          </trans-unit>
          <trans-unit id="185" xml:space="preserve">
            <source>The word/phase in all the text will be hashed to 256 indices.</source>
            <target state="new">The word/phase in all the text will be hashed to 256 indices.</target>
          </trans-unit>
          <trans-unit id="186" xml:space="preserve">
            <source>The parameter "Hashing bitsize" ranges from 1 to 31.</source>
            <target state="new">The parameter "Hashing bitsize" ranges from 1 to 31.</target>
          </trans-unit>
          <trans-unit id="187" xml:space="preserve">
            <source>The word(s)/phrase(s) are less likely to be hashed into the same index if setting it to be a larger number.</source>
            <target state="new">The word(s)/phrase(s) are less likely to be hashed into the same index if setting it to be a larger number.</target>
          </trans-unit>
          <trans-unit id="188" xml:space="preserve">
            <source>Third, set the parameter "N-grams" to 2.</source>
            <target state="new">Third, set the parameter "N-grams" to 2.</target>
          </trans-unit>
          <trans-unit id="189" xml:space="preserve">
            <source>This gets the occurrence frequency of unigrams (a feature for every single word) and bigrams (a feature for every pair of adjacent words) from the input text.</source>
            <target state="new">This gets the occurrence frequency of unigrams (a feature for every single word) and bigrams (a feature for every pair of adjacent words) from the input text.</target>
          </trans-unit>
          <trans-unit id="190" xml:space="preserve">
            <source>The parameter "N-grams" ranges from 0 to 10, which indicates the maximum number of sequential words to be included in a feature.</source>
            <target state="new">The parameter "N-grams" ranges from 0 to 10, which indicates the maximum number of sequential words to be included in a feature.</target>
          </trans-unit>
          <trans-unit id="191" xml:space="preserve">
            <source><bpt id="1">&lt;linkText&gt;</bpt>"Feature Hashing" module<ept id="1">&lt;/linkText&gt;</ept></source>
            <target state="new"><bpt id="1">&lt;linkText&gt;</bpt>"Feature Hashing" module<ept id="1">&lt;/linkText&gt;</ept></target>
          </trans-unit>
          <trans-unit id="192" xml:space="preserve">
            <source>The following figure shows what the these new feature look like.</source>
            <target state="new">The following figure shows what the these new feature look like.</target>
          </trans-unit>
          <trans-unit id="193" xml:space="preserve">
            <source><bpt id="1">&lt;linkText&gt;</bpt>"Feature Hashing" example<ept id="1">&lt;/linkText&gt;</ept></source>
            <target state="new"><bpt id="1">&lt;linkText&gt;</bpt>"Feature Hashing" example<ept id="1">&lt;/linkText&gt;</ept></target>
          </trans-unit>
          <trans-unit id="194" xml:space="preserve">
            <source>Filtering Features from Your Data - Feature Selection</source>
            <target state="new">Filtering Features from Your Data - Feature Selection</target>
          </trans-unit>
          <trans-unit id="195" xml:space="preserve">
            <source>Feature selection is a process that is commonly applied for the construction of training datasets for predictive modeling tasks such as classification or regression tasks.</source>
            <target state="new">Feature selection is a process that is commonly applied for the construction of training datasets for predictive modeling tasks such as classification or regression tasks.</target>
          </trans-unit>
          <trans-unit id="196" xml:space="preserve">
            <source>The goal is to select a subset of the features from the original dataset that reduce its dimensions by using a minimal set of features to represent the maximum amount of variance in the data.</source>
            <target state="new">The goal is to select a subset of the features from the original dataset that reduce its dimensions by using a minimal set of features to represent the maximum amount of variance in the data.</target>
          </trans-unit>
          <trans-unit id="197" xml:space="preserve">
            <source>This subset of features are, then, the only features to be included to train the model.</source>
            <target state="new">This subset of features are, then, the only features to be included to train the model.</target>
          </trans-unit>
          <trans-unit id="198" xml:space="preserve">
            <source>Feature selection serves two main purposes.</source>
            <target state="new">Feature selection serves two main purposes.</target>
          </trans-unit>
          <trans-unit id="199" xml:space="preserve">
            <source>First, feature selection often increases classification accuracy by eliminating irrelevant, redundant, or highly correlated features.</source>
            <target state="new">First, feature selection often increases classification accuracy by eliminating irrelevant, redundant, or highly correlated features.</target>
          </trans-unit>
          <trans-unit id="200" xml:space="preserve">
            <source>Second, it decreases the number of features which makes model training process more efficient.</source>
            <target state="new">Second, it decreases the number of features which makes model training process more efficient.</target>
          </trans-unit>
          <trans-unit id="201" xml:space="preserve">
            <source>This is particularly important for learners that are expensive to train such as support vector machines.</source>
            <target state="new">This is particularly important for learners that are expensive to train such as support vector machines.</target>
          </trans-unit>
          <trans-unit id="202" xml:space="preserve">
            <source>Although feature selection does seek to reduce the number of features in the dataset used to train the model, it is not usually referred to by the term "dimensionality reduction".</source>
            <target state="new">Although feature selection does seek to reduce the number of features in the dataset used to train the model, it is not usually referred to by the term "dimensionality reduction".</target>
          </trans-unit>
          <trans-unit id="203" xml:space="preserve">
            <source>Feature selection methods extract a subset of original features in the data without changing them.</source>
            <target state="new">Feature selection methods extract a subset of original features in the data without changing them.</target>
          </trans-unit>
          <trans-unit id="204" xml:space="preserve">
            <source>Dimensionality reduction methods employ engineered features that can transform the original features and thus modify them.</source>
            <target state="new">Dimensionality reduction methods employ engineered features that can transform the original features and thus modify them.</target>
          </trans-unit>
          <trans-unit id="205" xml:space="preserve">
            <source>Examples of dimensionality reduction methods include Principal Component Analysis, canonical correlation analysis, and Singular Value Decomposition.</source>
            <target state="new">Examples of dimensionality reduction methods include Principal Component Analysis, canonical correlation analysis, and Singular Value Decomposition.</target>
          </trans-unit>
          <trans-unit id="206" xml:space="preserve">
            <source>Among others, one widely applied category of feature selection methods in a supervised context is called "filter based feature selection".</source>
            <target state="new">Among others, one widely applied category of feature selection methods in a supervised context is called "filter based feature selection".</target>
          </trans-unit>
          <trans-unit id="207" xml:space="preserve">
            <source>By evaluating the correlation between each feature and the target attribute, these methods apply a statistical measure to assign a score to each feature.</source>
            <target state="new">By evaluating the correlation between each feature and the target attribute, these methods apply a statistical measure to assign a score to each feature.</target>
          </trans-unit>
          <trans-unit id="208" xml:space="preserve">
            <source>The features are then ranked by the score, which may be used to help set the threshold for keeping or eliminating a specific feature.</source>
            <target state="new">The features are then ranked by the score, which may be used to help set the threshold for keeping or eliminating a specific feature.</target>
          </trans-unit>
          <trans-unit id="209" xml:space="preserve">
            <source>Examples of the statistical measures used in these methods include Person correlation, mutual information, and the Chi squared test.</source>
            <target state="new">Examples of the statistical measures used in these methods include Person correlation, mutual information, and the Chi squared test.</target>
          </trans-unit>
          <trans-unit id="210" xml:space="preserve">
            <source>In Azure Machine Learning Studio, there are modules provided for feature selection.</source>
            <target state="new">In Azure Machine Learning Studio, there are modules provided for feature selection.</target>
          </trans-unit>
          <trans-unit id="211" xml:space="preserve">
            <source>As shown in the following figure, these modules include <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>Filter-Based Feature Selection[filter-based-feature-selection]<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept> and <bpt id="4CapsExtId1">&lt;link&gt;</bpt><bpt id="4CapsExtId2">&lt;linkText&gt;</bpt>Fisher Linear Discriminant Analysis[fisher-linear-discriminant-analysis]<ept id="4CapsExtId2">&lt;/linkText&gt;</ept><bpt id="4CapsExtId3">&lt;title&gt;</bpt><ept id="4CapsExtId3">&lt;/title&gt;</ept><ept id="4CapsExtId1">&lt;/link&gt;</ept>.</source>
            <target state="new">As shown in the following figure, these modules include <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>Filter-Based Feature Selection[filter-based-feature-selection]<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept> and <bpt id="4CapsExtId1">&lt;link&gt;</bpt><bpt id="4CapsExtId2">&lt;linkText&gt;</bpt>Fisher Linear Discriminant Analysis[fisher-linear-discriminant-analysis]<ept id="4CapsExtId2">&lt;/linkText&gt;</ept><bpt id="4CapsExtId3">&lt;title&gt;</bpt><ept id="4CapsExtId3">&lt;/title&gt;</ept><ept id="4CapsExtId1">&lt;/link&gt;</ept>.</target>
          </trans-unit>
          <trans-unit id="212" xml:space="preserve">
            <source><bpt id="1">&lt;linkText&gt;</bpt>Feature selection example<ept id="1">&lt;/linkText&gt;</ept></source>
            <target state="new"><bpt id="1">&lt;linkText&gt;</bpt>Feature selection example<ept id="1">&lt;/linkText&gt;</ept></target>
          </trans-unit>
          <trans-unit id="213" xml:space="preserve">
            <source>Consider, for example, the use of the <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>Filter-Based Feature Selection[filter-based-feature-selection]<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept> module.</source>
            <target state="new">Consider, for example, the use of the <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>Filter-Based Feature Selection[filter-based-feature-selection]<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept> module.</target>
          </trans-unit>
          <trans-unit id="214" xml:space="preserve">
            <source>For the purpose of convenience, we continue to use the text mining example outlined above.</source>
            <target state="new">For the purpose of convenience, we continue to use the text mining example outlined above.</target>
          </trans-unit>
          <trans-unit id="215" xml:space="preserve">
            <source>Assume that we want to build a regression model after a set of 256 features are created through the <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>Feature Hashing[feature-hashing]<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept> module, and that the response variable is the "Col1" and represents a book review ratings ranging from 1 to 5.</source>
            <target state="new">Assume that we want to build a regression model after a set of 256 features are created through the <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>Feature Hashing[feature-hashing]<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept> module, and that the response variable is the "Col1" and represents a book review ratings ranging from 1 to 5.</target>
          </trans-unit>
          <trans-unit id="216" xml:space="preserve">
            <source>By setting "Feature scoring method" to be "Pearson Correlation", the "Target column" to be "Col1", and the "Number of desired features" to 50.</source>
            <target state="new">By setting "Feature scoring method" to be "Pearson Correlation", the "Target column" to be "Col1", and the "Number of desired features" to 50.</target>
          </trans-unit>
          <trans-unit id="217" xml:space="preserve">
            <source>Then the module <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>Filter-Based Feature Selection[filter-based-feature-selection]<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept> will produce a dataset containing 50 features together with the target attribute "Col1".</source>
            <target state="new">Then the module <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>Filter-Based Feature Selection[filter-based-feature-selection]<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept> will produce a dataset containing 50 features together with the target attribute "Col1".</target>
          </trans-unit>
          <trans-unit id="218" xml:space="preserve">
            <source>The following figure shows the flow of this experiment and the input parameters we just described.</source>
            <target state="new">The following figure shows the flow of this experiment and the input parameters we just described.</target>
          </trans-unit>
          <trans-unit id="219" xml:space="preserve">
            <source><bpt id="1">&lt;linkText&gt;</bpt>Feature selection example<ept id="1">&lt;/linkText&gt;</ept></source>
            <target state="new"><bpt id="1">&lt;linkText&gt;</bpt>Feature selection example<ept id="1">&lt;/linkText&gt;</ept></target>
          </trans-unit>
          <trans-unit id="220" xml:space="preserve">
            <source>The following figure shows the resulting datasets.</source>
            <target state="new">The following figure shows the resulting datasets.</target>
          </trans-unit>
          <trans-unit id="221" xml:space="preserve">
            <source>Each feature is scored based on the Pearson Correlation between itself and the target attribute "Col1".</source>
            <target state="new">Each feature is scored based on the Pearson Correlation between itself and the target attribute "Col1".</target>
          </trans-unit>
          <trans-unit id="222" xml:space="preserve">
            <source>The features with top scores are kept.</source>
            <target state="new">The features with top scores are kept.</target>
          </trans-unit>
          <trans-unit id="223" xml:space="preserve">
            <source><bpt id="1">&lt;linkText&gt;</bpt>Feature selection example<ept id="1">&lt;/linkText&gt;</ept></source>
            <target state="new"><bpt id="1">&lt;linkText&gt;</bpt>Feature selection example<ept id="1">&lt;/linkText&gt;</ept></target>
          </trans-unit>
          <trans-unit id="224" xml:space="preserve">
            <source>The corresponding scores of the selected features are shown in the following figure.</source>
            <target state="new">The corresponding scores of the selected features are shown in the following figure.</target>
          </trans-unit>
          <trans-unit id="225" xml:space="preserve">
            <source><bpt id="1">&lt;linkText&gt;</bpt>Feature selection example<ept id="1">&lt;/linkText&gt;</ept></source>
            <target state="new"><bpt id="1">&lt;linkText&gt;</bpt>Feature selection example<ept id="1">&lt;/linkText&gt;</ept></target>
          </trans-unit>
          <trans-unit id="226" xml:space="preserve">
            <source>By applying this <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>Filter-Based Feature Selection[filter-based-feature-selection]<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept> module, 50 out of 256 features are selected because they have the most correlated features with the target variable "Col1", based on the scoring method "Pearson Correlation".</source>
            <target state="new">By applying this <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>Filter-Based Feature Selection[filter-based-feature-selection]<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept> module, 50 out of 256 features are selected because they have the most correlated features with the target variable "Col1", based on the scoring method "Pearson Correlation".</target>
          </trans-unit>
          <trans-unit id="227" xml:space="preserve">
            <source>Conclusion</source>
            <target state="new">Conclusion</target>
          </trans-unit>
          <trans-unit id="228" xml:space="preserve">
            <source>Feature engineering and feature selection are two commonly performed steps to prepare the training data when building a machine learning model.</source>
            <target state="new">Feature engineering and feature selection are two commonly performed steps to prepare the training data when building a machine learning model.</target>
          </trans-unit>
          <trans-unit id="229" xml:space="preserve">
            <source>Normally feature engineering is applied first to generate additional features, and then the feature selection step is performed to eliminate irrelevant, redundant, or highly correlated features.</source>
            <target state="new">Normally feature engineering is applied first to generate additional features, and then the feature selection step is performed to eliminate irrelevant, redundant, or highly correlated features.</target>
          </trans-unit>
          <trans-unit id="230" xml:space="preserve">
            <source>Note that it is not always necessarily to perform feature engineering or feature selection.</source>
            <target state="new">Note that it is not always necessarily to perform feature engineering or feature selection.</target>
          </trans-unit>
          <trans-unit id="231" xml:space="preserve">
            <source>Whether it is needed or not depends on the data we have or collect, the algorithm we pick, and the objective of the experiment.</source>
            <target state="new">Whether it is needed or not depends on the data we have or collect, the algorithm we pick, and the objective of the experiment.</target>
          </trans-unit>
        </group>
      </group>
    </body>
  </file>
</xliff>