<?xml version="1.0" encoding="utf-8"?>
<xliff version="1.2" xmlns="urn:oasis:names:tc:xliff:document:1.2" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="urn:oasis:names:tc:xliff:document:1.2 xliff-core-1.2-transitional.xsd">
  <file datatype="xml" source-language="en-us" target-language="ko-kr" original="2/21/2016 3:24:45 AM" tool-id="MarkdownTransformer" product-name="N/A" product-version="N/A" build-num="1">
    <header>
      <xliffext:olfilehash xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">4491b823b7fd212bd22be17d9064bba44b74052d</xliffext:olfilehash>
      <tool tool-id="MarkdownTransformer" tool-name="MarkdownToXliff" tool-version="1.0" tool-company="Microsoft" />
    </header>
    <body>
      <group extype="content">
        <group id="101">
          <trans-unit id="101" xml:space="preserve">
            <source>Guide to the Net# Neural Networks Specification Language | Microsoft Azure</source>
            <target state="new">Guide to the Net# Neural Networks Specification Language | Microsoft Azure</target>
          </trans-unit>
          <trans-unit id="102" xml:space="preserve">
            <source>Syntax for the Net# neural networks specification language, together with examples of how to create a custom neural network model in Microsoft Azure ML using Net#</source>
            <target state="new">Syntax for the Net# neural networks specification language, together with examples of how to create a custom neural network model in Microsoft Azure ML using Net#</target>
          </trans-unit>
          <trans-unit id="103" xml:space="preserve">
            <source>Guide to Net# neural network specification language for Azure Machine Learning</source>
            <target state="new">Guide to Net# neural network specification language for Azure Machine Learning</target>
          </trans-unit>
          <trans-unit id="104" xml:space="preserve">
            <source>Overview</source>
            <target state="new">Overview</target>
          </trans-unit>
          <trans-unit id="105" xml:space="preserve">
            <source>Net# is a language developed by Microsoft that is used to define neural network architectures for neural network modules in Microsoft Azure Machine Learning.</source>
            <target state="new">Net# is a language developed by Microsoft that is used to define neural network architectures for neural network modules in Microsoft Azure Machine Learning.</target>
          </trans-unit>
          <trans-unit id="106" xml:space="preserve">
            <source>In this article, you will learn:</source>
            <target state="new">In this article, you will learn:</target>
          </trans-unit>
          <trans-unit id="107" xml:space="preserve">
            <source>Basic concepts related to neural networks</source>
            <target state="new">Basic concepts related to neural networks</target>
          </trans-unit>
          <trans-unit id="108" xml:space="preserve">
            <source>Neural network requirements and how to define the primary components</source>
            <target state="new">Neural network requirements and how to define the primary components</target>
          </trans-unit>
          <trans-unit id="109" xml:space="preserve">
            <source>The syntax and keywords of the Net# specification language</source>
            <target state="new">The syntax and keywords of the Net# specification language</target>
          </trans-unit>
          <trans-unit id="110" xml:space="preserve">
            <source>Examples of custom neural networks created using Net#</source>
            <target state="new">Examples of custom neural networks created using Net#</target>
          </trans-unit>
          <trans-unit id="111" xml:space="preserve">
            <source><ph id="1">&lt;token href="../../includes/machine-learning-free-trial.md"/&gt;</ph></source>
            <target state="new"><ph id="1">&lt;token href="../../includes/machine-learning-free-trial.md"/&gt;</ph></target>
          </trans-unit>
          <trans-unit id="112" xml:space="preserve">
            <source>Neural network basics</source>
            <target state="new">Neural network basics</target>
          </trans-unit>
          <trans-unit id="113" xml:space="preserve">
            <source>A neural network structure consists of <bpt id="2">&lt;strong&gt;</bpt>*nodes*<ept id="2">&lt;/strong&gt;</ept> that are organized in <bpt id="4">&lt;strong&gt;</bpt>*layers*<ept id="4">&lt;/strong&gt;</ept>, and weighted <bpt id="6">&lt;strong&gt;</bpt>*connections*<ept id="6">&lt;/strong&gt;</ept> (or <bpt id="8">&lt;strong&gt;</bpt>*edges*<ept id="8">&lt;/strong&gt;</ept>) between the nodes.</source>
            <target state="new">A neural network structure consists of <bpt id="2">&lt;strong&gt;</bpt>*nodes*<ept id="2">&lt;/strong&gt;</ept> that are organized in <bpt id="4">&lt;strong&gt;</bpt>*layers*<ept id="4">&lt;/strong&gt;</ept>, and weighted <bpt id="6">&lt;strong&gt;</bpt>*connections*<ept id="6">&lt;/strong&gt;</ept> (or <bpt id="8">&lt;strong&gt;</bpt>*edges*<ept id="8">&lt;/strong&gt;</ept>) between the nodes.</target>
          </trans-unit>
          <trans-unit id="114" xml:space="preserve">
            <source>The connections are directional, and each connection has a <bpt id="2">&lt;strong&gt;</bpt>*source*<ept id="2">&lt;/strong&gt;</ept> node and a <bpt id="4">&lt;strong&gt;</bpt>*destination*<ept id="4">&lt;/strong&gt;</ept> node.</source>
            <target state="new">The connections are directional, and each connection has a <bpt id="2">&lt;strong&gt;</bpt>*source*<ept id="2">&lt;/strong&gt;</ept> node and a <bpt id="4">&lt;strong&gt;</bpt>*destination*<ept id="4">&lt;/strong&gt;</ept> node.</target>
          </trans-unit>
          <trans-unit id="115" xml:space="preserve">
            <source>Each <bpt id="2">&lt;strong&gt;</bpt>*trainable layer*<ept id="2">&lt;/strong&gt;</ept> (a hidden or an output layer) has one or more <bpt id="4">&lt;strong&gt;</bpt>*connection bundles*<ept id="4">&lt;/strong&gt;</ept>.</source>
            <target state="new">Each <bpt id="2">&lt;strong&gt;</bpt>*trainable layer*<ept id="2">&lt;/strong&gt;</ept> (a hidden or an output layer) has one or more <bpt id="4">&lt;strong&gt;</bpt>*connection bundles*<ept id="4">&lt;/strong&gt;</ept>.</target>
          </trans-unit>
          <trans-unit id="116" xml:space="preserve">
            <source>A connection bundle consists of a source layer and a specification of the connections from that source layer.</source>
            <target state="new">A connection bundle consists of a source layer and a specification of the connections from that source layer.</target>
          </trans-unit>
          <trans-unit id="117" xml:space="preserve">
            <source>All the connections in a given bundle share the same <bpt id="2">&lt;strong&gt;</bpt>*source layer*<ept id="2">&lt;/strong&gt;</ept> and the same <bpt id="4">&lt;strong&gt;</bpt>*destination layer*<ept id="4">&lt;/strong&gt;</ept>.</source>
            <target state="new">All the connections in a given bundle share the same <bpt id="2">&lt;strong&gt;</bpt>*source layer*<ept id="2">&lt;/strong&gt;</ept> and the same <bpt id="4">&lt;strong&gt;</bpt>*destination layer*<ept id="4">&lt;/strong&gt;</ept>.</target>
          </trans-unit>
          <trans-unit id="118" xml:space="preserve">
            <source>In Net#, a connection bundle is considered as belonging to the bundle's destination layer.</source>
            <target state="new">In Net#, a connection bundle is considered as belonging to the bundle's destination layer.</target>
          </trans-unit>
          <trans-unit id="119" xml:space="preserve">
            <source>Net# supports various kinds of connection bundles, which lets you customize the way inputs are mapped to hidden layers and mapped to the outputs.</source>
            <target state="new">Net# supports various kinds of connection bundles, which lets you customize the way inputs are mapped to hidden layers and mapped to the outputs.</target>
          </trans-unit>
          <trans-unit id="120" xml:space="preserve">
            <source>The default or standard bundle is a <bpt id="2">&lt;strong&gt;</bpt>full bundle<ept id="2">&lt;/strong&gt;</ept>, in which each node in the source layer is connected to every node in the destination layer.</source>
            <target state="new">The default or standard bundle is a <bpt id="2">&lt;strong&gt;</bpt>full bundle<ept id="2">&lt;/strong&gt;</ept>, in which each node in the source layer is connected to every node in the destination layer.</target>
          </trans-unit>
          <trans-unit id="121" xml:space="preserve">
            <source>Additionally, Net# supports the following four kinds of advanced connection bundles:</source>
            <target state="new">Additionally, Net# supports the following four kinds of advanced connection bundles:</target>
          </trans-unit>
          <trans-unit id="122" xml:space="preserve">
            <source><bpt id="1">&lt;strong&gt;</bpt>Filtered bundles<ept id="1">&lt;/strong&gt;</ept>.</source>
            <target state="new"><bpt id="1">&lt;strong&gt;</bpt>Filtered bundles<ept id="1">&lt;/strong&gt;</ept>.</target>
          </trans-unit>
          <trans-unit id="123" xml:space="preserve">
            <source>The user can define a predicate by using the locations of the source layer node and the destination layer node.</source>
            <target state="new">The user can define a predicate by using the locations of the source layer node and the destination layer node.</target>
          </trans-unit>
          <trans-unit id="124" xml:space="preserve">
            <source>Nodes are connected whenever the predicate is True.</source>
            <target state="new">Nodes are connected whenever the predicate is True.</target>
          </trans-unit>
          <trans-unit id="125" xml:space="preserve">
            <source><bpt id="1">&lt;strong&gt;</bpt>Convolutional bundles<ept id="1">&lt;/strong&gt;</ept>.</source>
            <target state="new"><bpt id="1">&lt;strong&gt;</bpt>Convolutional bundles<ept id="1">&lt;/strong&gt;</ept>.</target>
          </trans-unit>
          <trans-unit id="126" xml:space="preserve">
            <source>The user can define small neighborhoods of nodes in the source layer.</source>
            <target state="new">The user can define small neighborhoods of nodes in the source layer.</target>
          </trans-unit>
          <trans-unit id="127" xml:space="preserve">
            <source>Each node in the destination layer is connected to one neighborhood of nodes in the source layer.</source>
            <target state="new">Each node in the destination layer is connected to one neighborhood of nodes in the source layer.</target>
          </trans-unit>
          <trans-unit id="128" xml:space="preserve">
            <source><bpt id="1">&lt;strong&gt;</bpt>Pooling bundles<ept id="1">&lt;/strong&gt;</ept> and <bpt id="3">&lt;strong&gt;</bpt>Response normalization bundles<ept id="3">&lt;/strong&gt;</ept>.</source>
            <target state="new"><bpt id="1">&lt;strong&gt;</bpt>Pooling bundles<ept id="1">&lt;/strong&gt;</ept> and <bpt id="3">&lt;strong&gt;</bpt>Response normalization bundles<ept id="3">&lt;/strong&gt;</ept>.</target>
          </trans-unit>
          <trans-unit id="129" xml:space="preserve">
            <source>These are similar to convolutional bundles in that the user defines small neighborhoods of nodes in the source layer.</source>
            <target state="new">These are similar to convolutional bundles in that the user defines small neighborhoods of nodes in the source layer.</target>
          </trans-unit>
          <trans-unit id="130" xml:space="preserve">
            <source>The difference is that the weights of the edges in these bundles are not trainable.</source>
            <target state="new">The difference is that the weights of the edges in these bundles are not trainable.</target>
          </trans-unit>
          <trans-unit id="131" xml:space="preserve">
            <source>Instead, a predefined function is applied to the source node values to determine the destination node value.</source>
            <target state="new">Instead, a predefined function is applied to the source node values to determine the destination node value.</target>
          </trans-unit>
          <trans-unit id="132" xml:space="preserve">
            <source>Using Net# to define the structure of a neural network makes it possible to define complex structures such as deep neural networks or convolutions of arbitrary dimensions, which are known to improve learning on data such as image, audio, or video.</source>
            <target state="new">Using Net# to define the structure of a neural network makes it possible to define complex structures such as deep neural networks or convolutions of arbitrary dimensions, which are known to improve learning on data such as image, audio, or video.</target>
          </trans-unit>
          <trans-unit id="133" xml:space="preserve">
            <source>Supported customizations</source>
            <target state="new">Supported customizations</target>
          </trans-unit>
          <trans-unit id="134" xml:space="preserve">
            <source>The architecture of neural network models that you create in Azure Machine Learning can be extensively customized by using Net#.</source>
            <target state="new">The architecture of neural network models that you create in Azure Machine Learning can be extensively customized by using Net#.</target>
          </trans-unit>
          <trans-unit id="135" xml:space="preserve">
            <source>You can:</source>
            <target state="new">You can:</target>
          </trans-unit>
          <trans-unit id="136" xml:space="preserve">
            <source>Create hidden layers and control the number of nodes in each layer.</source>
            <target state="new">Create hidden layers and control the number of nodes in each layer.</target>
          </trans-unit>
          <trans-unit id="137" xml:space="preserve">
            <source>Specify how layers are to be connected to each other.</source>
            <target state="new">Specify how layers are to be connected to each other.</target>
          </trans-unit>
          <trans-unit id="138" xml:space="preserve">
            <source>Define special connectivity structures, such as convolutions and weight sharing bundles.</source>
            <target state="new">Define special connectivity structures, such as convolutions and weight sharing bundles.</target>
          </trans-unit>
          <trans-unit id="139" xml:space="preserve">
            <source>Specify different activation functions.</source>
            <target state="new">Specify different activation functions.</target>
          </trans-unit>
          <trans-unit id="140" xml:space="preserve">
            <source>For details of the specification language syntax, see Structure Specification.</source>
            <target state="new">For details of the specification language syntax, see Structure Specification.</target>
          </trans-unit>
          <trans-unit id="141" xml:space="preserve">
            <source>For examples of defining neural networks for some common machine learning tasks, from simplex to complex, see Examples.</source>
            <target state="new">For examples of defining neural networks for some common machine learning tasks, from simplex to complex, see Examples.</target>
          </trans-unit>
          <trans-unit id="142" xml:space="preserve">
            <source>General requirements</source>
            <target state="new">General requirements</target>
          </trans-unit>
          <trans-unit id="143" xml:space="preserve">
            <source>There must be exactly one output layer, at least one input layer, and zero or more hidden layers.</source>
            <target state="new">There must be exactly one output layer, at least one input layer, and zero or more hidden layers.</target>
          </trans-unit>
          <trans-unit id="144" xml:space="preserve">
            <source>Each layer has a fixed number of nodes, conceptually arranged in a rectangular array of arbitrary dimensions.</source>
            <target state="new">Each layer has a fixed number of nodes, conceptually arranged in a rectangular array of arbitrary dimensions.</target>
          </trans-unit>
          <trans-unit id="145" xml:space="preserve">
            <source>Input layers have no associated trained parameters and represent the point where instance data enters the network.</source>
            <target state="new">Input layers have no associated trained parameters and represent the point where instance data enters the network.</target>
          </trans-unit>
          <trans-unit id="146" xml:space="preserve">
            <source>Trainable layers (the hidden and output layers) have associated trained parameters, known as weights and biases.</source>
            <target state="new">Trainable layers (the hidden and output layers) have associated trained parameters, known as weights and biases.</target>
          </trans-unit>
          <trans-unit id="147" xml:space="preserve">
            <source>The source and destination nodes must be in separate layers.</source>
            <target state="new">The source and destination nodes must be in separate layers.</target>
          </trans-unit>
          <trans-unit id="148" xml:space="preserve">
            <source>Connections must be acyclic; in other words, there cannot be a chain of connections leading back to the initial source node.</source>
            <target state="new">Connections must be acyclic; in other words, there cannot be a chain of connections leading back to the initial source node.</target>
          </trans-unit>
          <trans-unit id="149" xml:space="preserve">
            <source>The output layer cannot be a source layer of a connection bundle.</source>
            <target state="new">The output layer cannot be a source layer of a connection bundle.</target>
          </trans-unit>
          <trans-unit id="150" xml:space="preserve">
            <source>Structure specifications</source>
            <target state="new">Structure specifications</target>
          </trans-unit>
          <trans-unit id="151" xml:space="preserve">
            <source>A neural network structure specification is composed of three sections: the <bpt id="2">&lt;strong&gt;</bpt>constant declaration<ept id="2">&lt;/strong&gt;</ept>, the <bpt id="4">&lt;strong&gt;</bpt>layer declaration<ept id="4">&lt;/strong&gt;</ept>, the <bpt id="6">&lt;strong&gt;</bpt>connection declaration<ept id="6">&lt;/strong&gt;</ept>.</source>
            <target state="new">A neural network structure specification is composed of three sections: the <bpt id="2">&lt;strong&gt;</bpt>constant declaration<ept id="2">&lt;/strong&gt;</ept>, the <bpt id="4">&lt;strong&gt;</bpt>layer declaration<ept id="4">&lt;/strong&gt;</ept>, the <bpt id="6">&lt;strong&gt;</bpt>connection declaration<ept id="6">&lt;/strong&gt;</ept>.</target>
          </trans-unit>
          <trans-unit id="152" xml:space="preserve">
            <source>There is also an optional <bpt id="2">&lt;strong&gt;</bpt>share declaration<ept id="2">&lt;/strong&gt;</ept> section.</source>
            <target state="new">There is also an optional <bpt id="2">&lt;strong&gt;</bpt>share declaration<ept id="2">&lt;/strong&gt;</ept> section.</target>
          </trans-unit>
          <trans-unit id="153" xml:space="preserve">
            <source>The sections can be specified in any order.</source>
            <target state="new">The sections can be specified in any order.</target>
          </trans-unit>
          <trans-unit id="154" xml:space="preserve">
            <source>Constant declaration</source>
            <target state="new">Constant declaration</target>
          </trans-unit>
          <trans-unit id="155" xml:space="preserve">
            <source>A constant declaration is optional.</source>
            <target state="new">A constant declaration is optional.</target>
          </trans-unit>
          <trans-unit id="156" xml:space="preserve">
            <source>It provides a means to define values used elsewhere in the neural network definition.</source>
            <target state="new">It provides a means to define values used elsewhere in the neural network definition.</target>
          </trans-unit>
          <trans-unit id="157" xml:space="preserve">
            <source>The declaration statement consists of an identifier followed by an equal sign and a value expression.</source>
            <target state="new">The declaration statement consists of an identifier followed by an equal sign and a value expression.</target>
          </trans-unit>
          <trans-unit id="158" xml:space="preserve">
            <source>For example, the following statement defines a constant <bpt id="2">&lt;strong&gt;</bpt>x<ept id="2">&lt;/strong&gt;</ept>:</source>
            <target state="new">For example, the following statement defines a constant <bpt id="2">&lt;strong&gt;</bpt>x<ept id="2">&lt;/strong&gt;</ept>:</target>
          </trans-unit>
          <trans-unit id="159" xml:space="preserve">
            <source>To define two or more constants simultaneously, enclose the identifier names and values in braces, and separate them by using semicolons.</source>
            <target state="new">To define two or more constants simultaneously, enclose the identifier names and values in braces, and separate them by using semicolons.</target>
          </trans-unit>
          <trans-unit id="160" xml:space="preserve">
            <source>For example:</source>
            <target state="new">For example:</target>
          </trans-unit>
          <trans-unit id="161" xml:space="preserve">
            <source>The right-hand side of each assignment expression can be an integer, a real number, a Boolean value (True or False), or a mathematical expression.</source>
            <target state="new">The right-hand side of each assignment expression can be an integer, a real number, a Boolean value (True or False), or a mathematical expression.</target>
          </trans-unit>
          <trans-unit id="162" xml:space="preserve">
            <source>For example:</source>
            <target state="new">For example:</target>
          </trans-unit>
          <trans-unit id="163" xml:space="preserve">
            <source>Layer declaration</source>
            <target state="new">Layer declaration</target>
          </trans-unit>
          <trans-unit id="164" xml:space="preserve">
            <source>The layer declaration is required.</source>
            <target state="new">The layer declaration is required.</target>
          </trans-unit>
          <trans-unit id="165" xml:space="preserve">
            <source>It defines the size and source of the layer, including its connection bundles and attributes.</source>
            <target state="new">It defines the size and source of the layer, including its connection bundles and attributes.</target>
          </trans-unit>
          <trans-unit id="166" xml:space="preserve">
            <source>The declaration statement starts with the name of the layer (input, hidden, or output), followed by the dimensions of the layer (a tuple of positive integers).</source>
            <target state="new">The declaration statement starts with the name of the layer (input, hidden, or output), followed by the dimensions of the layer (a tuple of positive integers).</target>
          </trans-unit>
          <trans-unit id="167" xml:space="preserve">
            <source>For example:</source>
            <target state="new">For example:</target>
          </trans-unit>
          <trans-unit id="168" xml:space="preserve">
            <source>The product of the dimensions is the number of nodes in the layer.</source>
            <target state="new">The product of the dimensions is the number of nodes in the layer.</target>
          </trans-unit>
          <trans-unit id="169" xml:space="preserve">
            <source>In this example, there are two dimensions [5,20], which means there are  100 nodes in the layer.</source>
            <target state="new">In this example, there are two dimensions [5,20], which means there are  100 nodes in the layer.</target>
          </trans-unit>
          <trans-unit id="170" xml:space="preserve">
            <source>The layers can be declared in any order, with one exception: If more than one input layer is defined, the order in which they are declared must match the order of features in the input data.</source>
            <target state="new">The layers can be declared in any order, with one exception: If more than one input layer is defined, the order in which they are declared must match the order of features in the input data.</target>
          </trans-unit>
          <trans-unit id="171" xml:space="preserve">
            <source>A layer declaration for a trainable layer (the hidden or output layers) can optionally include the output function (also called an activation function), which defaults to <bpt id="2">&lt;strong&gt;</bpt>sigmoid<ept id="2">&lt;/strong&gt;</ept>.</source>
            <target state="new">A layer declaration for a trainable layer (the hidden or output layers) can optionally include the output function (also called an activation function), which defaults to <bpt id="2">&lt;strong&gt;</bpt>sigmoid<ept id="2">&lt;/strong&gt;</ept>.</target>
          </trans-unit>
          <trans-unit id="172" xml:space="preserve">
            <source>The following output functions are supported:</source>
            <target state="new">The following output functions are supported:</target>
          </trans-unit>
          <trans-unit id="173" xml:space="preserve">
            <source>sigmoid</source>
            <target state="new">sigmoid</target>
          </trans-unit>
          <trans-unit id="174" xml:space="preserve">
            <source>linear</source>
            <target state="new">linear</target>
          </trans-unit>
          <trans-unit id="175" xml:space="preserve">
            <source>softmax</source>
            <target state="new">softmax</target>
          </trans-unit>
          <trans-unit id="176" xml:space="preserve">
            <source>rlinear</source>
            <target state="new">rlinear</target>
          </trans-unit>
          <trans-unit id="177" xml:space="preserve">
            <source>square</source>
            <target state="new">square</target>
          </trans-unit>
          <trans-unit id="178" xml:space="preserve">
            <source>sqrt</source>
            <target state="new">sqrt</target>
          </trans-unit>
          <trans-unit id="179" xml:space="preserve">
            <source>srlinear</source>
            <target state="new">srlinear</target>
          </trans-unit>
          <trans-unit id="180" xml:space="preserve">
            <source>abs</source>
            <target state="new">abs</target>
          </trans-unit>
          <trans-unit id="181" xml:space="preserve">
            <source>tanh</source>
            <target state="new">tanh</target>
          </trans-unit>
          <trans-unit id="182" xml:space="preserve">
            <source>brlinear</source>
            <target state="new">brlinear</target>
          </trans-unit>
          <trans-unit id="183" xml:space="preserve">
            <source>For example, the following declaration uses the <bpt id="2">&lt;strong&gt;</bpt>softmax<ept id="2">&lt;/strong&gt;</ept> function:</source>
            <target state="new">For example, the following declaration uses the <bpt id="2">&lt;strong&gt;</bpt>softmax<ept id="2">&lt;/strong&gt;</ept> function:</target>
          </trans-unit>
          <trans-unit id="184" xml:space="preserve">
            <source>Connection declaration</source>
            <target state="new">Connection declaration</target>
          </trans-unit>
          <trans-unit id="185" xml:space="preserve">
            <source>Immediately after defining the trainable layer, you must declare connections among the layers you have defined.</source>
            <target state="new">Immediately after defining the trainable layer, you must declare connections among the layers you have defined.</target>
          </trans-unit>
          <trans-unit id="186" xml:space="preserve">
            <source>The connection bundle declaration starts with the keyword <bpt id="2">&lt;strong&gt;</bpt>from<ept id="2">&lt;/strong&gt;</ept>, followed by the name of the bundle's source layer and the kind of connection bundle to create.</source>
            <target state="new">The connection bundle declaration starts with the keyword <bpt id="2">&lt;strong&gt;</bpt>from<ept id="2">&lt;/strong&gt;</ept>, followed by the name of the bundle's source layer and the kind of connection bundle to create.</target>
          </trans-unit>
          <trans-unit id="187" xml:space="preserve">
            <source>Currently, five kinds of connection bundles are supported:</source>
            <target state="new">Currently, five kinds of connection bundles are supported:</target>
          </trans-unit>
          <trans-unit id="188" xml:space="preserve">
            <source><bpt id="1">&lt;strong&gt;</bpt>Full<ept id="1">&lt;/strong&gt;</ept> bundles, indicated by the keyword <bpt id="3">&lt;strong&gt;</bpt>all<ept id="3">&lt;/strong&gt;</ept></source>
            <target state="new"><bpt id="1">&lt;strong&gt;</bpt>Full<ept id="1">&lt;/strong&gt;</ept> bundles, indicated by the keyword <bpt id="3">&lt;strong&gt;</bpt>all<ept id="3">&lt;/strong&gt;</ept></target>
          </trans-unit>
          <trans-unit id="189" xml:space="preserve">
            <source><bpt id="1">&lt;strong&gt;</bpt>Filtered<ept id="1">&lt;/strong&gt;</ept> bundles, indicated by the keyword <bpt id="3">&lt;strong&gt;</bpt>where<ept id="3">&lt;/strong&gt;</ept>, followed by a predicate expression</source>
            <target state="new"><bpt id="1">&lt;strong&gt;</bpt>Filtered<ept id="1">&lt;/strong&gt;</ept> bundles, indicated by the keyword <bpt id="3">&lt;strong&gt;</bpt>where<ept id="3">&lt;/strong&gt;</ept>, followed by a predicate expression</target>
          </trans-unit>
          <trans-unit id="190" xml:space="preserve">
            <source><bpt id="1">&lt;strong&gt;</bpt>Convolutional<ept id="1">&lt;/strong&gt;</ept> bundles, indicated by the keyword <bpt id="3">&lt;strong&gt;</bpt>convolve<ept id="3">&lt;/strong&gt;</ept>, followed by the convolution attributes</source>
            <target state="new"><bpt id="1">&lt;strong&gt;</bpt>Convolutional<ept id="1">&lt;/strong&gt;</ept> bundles, indicated by the keyword <bpt id="3">&lt;strong&gt;</bpt>convolve<ept id="3">&lt;/strong&gt;</ept>, followed by the convolution attributes</target>
          </trans-unit>
          <trans-unit id="191" xml:space="preserve">
            <source><bpt id="1">&lt;strong&gt;</bpt>Pooling<ept id="1">&lt;/strong&gt;</ept> bundles, indicated by the keywords <bpt id="3">&lt;strong&gt;</bpt>max pool<ept id="3">&lt;/strong&gt;</ept> or <bpt id="5">&lt;strong&gt;</bpt>mean pool<ept id="5">&lt;/strong&gt;</ept></source>
            <target state="new"><bpt id="1">&lt;strong&gt;</bpt>Pooling<ept id="1">&lt;/strong&gt;</ept> bundles, indicated by the keywords <bpt id="3">&lt;strong&gt;</bpt>max pool<ept id="3">&lt;/strong&gt;</ept> or <bpt id="5">&lt;strong&gt;</bpt>mean pool<ept id="5">&lt;/strong&gt;</ept></target>
          </trans-unit>
          <trans-unit id="192" xml:space="preserve">
            <source><bpt id="1">&lt;strong&gt;</bpt>Response normalization<ept id="1">&lt;/strong&gt;</ept> bundles, indicated by the keyword <bpt id="3">&lt;strong&gt;</bpt>response norm<ept id="3">&lt;/strong&gt;</ept></source>
            <target state="new"><bpt id="1">&lt;strong&gt;</bpt>Response normalization<ept id="1">&lt;/strong&gt;</ept> bundles, indicated by the keyword <bpt id="3">&lt;strong&gt;</bpt>response norm<ept id="3">&lt;/strong&gt;</ept></target>
          </trans-unit>
          <trans-unit id="193" xml:space="preserve">
            <source>Full bundles</source>
            <target state="new">Full bundles</target>
          </trans-unit>
          <trans-unit id="194" xml:space="preserve">
            <source>A full connection bundle includes a connection from each node in the source layer to each node in the destination layer.</source>
            <target state="new">A full connection bundle includes a connection from each node in the source layer to each node in the destination layer.</target>
          </trans-unit>
          <trans-unit id="195" xml:space="preserve">
            <source>This is the default network connection type.</source>
            <target state="new">This is the default network connection type.</target>
          </trans-unit>
          <trans-unit id="196" xml:space="preserve">
            <source>Filtered bundles</source>
            <target state="new">Filtered bundles</target>
          </trans-unit>
          <trans-unit id="197" xml:space="preserve">
            <source>A filtered connection bundle specification includes a predicate, expressed syntactically, much like a C# lambda expression.</source>
            <target state="new">A filtered connection bundle specification includes a predicate, expressed syntactically, much like a C# lambda expression.</target>
          </trans-unit>
          <trans-unit id="198" xml:space="preserve">
            <source>The following example defines two filtered bundles:</source>
            <target state="new">The following example defines two filtered bundles:</target>
          </trans-unit>
          <trans-unit id="199" xml:space="preserve">
            <source>In the predicate for ByRow, <bpt id="2">&lt;strong&gt;</bpt>s<ept id="2">&lt;/strong&gt;</ept> is a parameter representing an index into the rectangular array of nodes of the input layer, Pixels, and <bpt id="4">&lt;strong&gt;</bpt>d<ept id="4">&lt;/strong&gt;</ept> is a parameter representing an index into the array of nodes of the hidden layer, ByRow. The type of both <bpt id="6">&lt;strong&gt;</bpt>s<ept id="6">&lt;/strong&gt;</ept> and <bpt id="8">&lt;strong&gt;</bpt>d<ept id="8">&lt;/strong&gt;</ept> is a tuple of integers of length two. Conceptually, <bpt id="10">&lt;strong&gt;</bpt>s<ept id="10">&lt;/strong&gt;</ept> ranges over all pairs of integers with _0 &lt;= s[0] &lt; 10_ and _0 &lt;= s[1] &lt; 20_, and <bpt id="12">&lt;strong&gt;</bpt>d<ept id="12">&lt;/strong&gt;</ept> ranges over all pairs of integers, with _0 &lt;= d[0] &lt; 10_ and _0 &lt;= d[1] &lt; 12_.</source>
            <target state="new">In the predicate for ByRow, <bpt id="2">&lt;strong&gt;</bpt>s<ept id="2">&lt;/strong&gt;</ept> is a parameter representing an index into the rectangular array of nodes of the input layer, Pixels, and <bpt id="4">&lt;strong&gt;</bpt>d<ept id="4">&lt;/strong&gt;</ept> is a parameter representing an index into the array of nodes of the hidden layer, ByRow. The type of both <bpt id="6">&lt;strong&gt;</bpt>s<ept id="6">&lt;/strong&gt;</ept> and <bpt id="8">&lt;strong&gt;</bpt>d<ept id="8">&lt;/strong&gt;</ept> is a tuple of integers of length two. Conceptually, <bpt id="10">&lt;strong&gt;</bpt>s<ept id="10">&lt;/strong&gt;</ept> ranges over all pairs of integers with _0 &lt;= s[0] &lt; 10_ and _0 &lt;= s[1] &lt; 20_, and <bpt id="12">&lt;strong&gt;</bpt>d<ept id="12">&lt;/strong&gt;</ept> ranges over all pairs of integers, with _0 &lt;= d[0] &lt; 10_ and _0 &lt;= d[1] &lt; 12_.</target>
          </trans-unit>
          <trans-unit id="200" xml:space="preserve">
            <source>On the right-hand side of the predicate expression, there is a condition.</source>
            <target state="new">On the right-hand side of the predicate expression, there is a condition.</target>
          </trans-unit>
          <trans-unit id="201" xml:space="preserve">
            <source>In this example, for every value of <bpt id="2">&lt;strong&gt;</bpt>s<ept id="2">&lt;/strong&gt;</ept> and <bpt id="4">&lt;strong&gt;</bpt>d<ept id="4">&lt;/strong&gt;</ept> such that the condition is True, there is an edge from the source layer node to the destination layer node.</source>
            <target state="new">In this example, for every value of <bpt id="2">&lt;strong&gt;</bpt>s<ept id="2">&lt;/strong&gt;</ept> and <bpt id="4">&lt;strong&gt;</bpt>d<ept id="4">&lt;/strong&gt;</ept> such that the condition is True, there is an edge from the source layer node to the destination layer node.</target>
          </trans-unit>
          <trans-unit id="202" xml:space="preserve">
            <source>Thus, this filter expression indicates that the bundle includes a connection from the node defined by <bpt id="2">&lt;strong&gt;</bpt>s<ept id="2">&lt;/strong&gt;</ept> to the node defined by <bpt id="4">&lt;strong&gt;</bpt>d<ept id="4">&lt;/strong&gt;</ept> in all cases where s[0] is equal to d[0].</source>
            <target state="new">Thus, this filter expression indicates that the bundle includes a connection from the node defined by <bpt id="2">&lt;strong&gt;</bpt>s<ept id="2">&lt;/strong&gt;</ept> to the node defined by <bpt id="4">&lt;strong&gt;</bpt>d<ept id="4">&lt;/strong&gt;</ept> in all cases where s[0] is equal to d[0].</target>
          </trans-unit>
          <trans-unit id="203" xml:space="preserve">
            <source>Optionally, you can specify a set of weights for a filtered bundle.</source>
            <target state="new">Optionally, you can specify a set of weights for a filtered bundle.</target>
          </trans-unit>
          <trans-unit id="204" xml:space="preserve">
            <source>The value for the <bpt id="2">&lt;strong&gt;</bpt>Weights<ept id="2">&lt;/strong&gt;</ept> attribute must be a tuple of floating point values with a length that matches the number of connections defined by the bundle.</source>
            <target state="new">The value for the <bpt id="2">&lt;strong&gt;</bpt>Weights<ept id="2">&lt;/strong&gt;</ept> attribute must be a tuple of floating point values with a length that matches the number of connections defined by the bundle.</target>
          </trans-unit>
          <trans-unit id="205" xml:space="preserve">
            <source>By default, weights are randomly generated.</source>
            <target state="new">By default, weights are randomly generated.</target>
          </trans-unit>
          <trans-unit id="206" xml:space="preserve">
            <source>Weight values are grouped by the destination node index.</source>
            <target state="new">Weight values are grouped by the destination node index.</target>
          </trans-unit>
          <trans-unit id="207" xml:space="preserve">
            <source>That is, if the first destination node is connected to K source nodes, the first <bpt id="2">&lt;em&gt;</bpt>K<ept id="2">&lt;/em&gt;</ept> elements of the <bpt id="4">&lt;strong&gt;</bpt>Weights<ept id="4">&lt;/strong&gt;</ept> tuple are the weights for the first destination node, in source index order.</source>
            <target state="new">That is, if the first destination node is connected to K source nodes, the first <bpt id="2">&lt;em&gt;</bpt>K<ept id="2">&lt;/em&gt;</ept> elements of the <bpt id="4">&lt;strong&gt;</bpt>Weights<ept id="4">&lt;/strong&gt;</ept> tuple are the weights for the first destination node, in source index order.</target>
          </trans-unit>
          <trans-unit id="208" xml:space="preserve">
            <source>The same applies for the remaining destination nodes.</source>
            <target state="new">The same applies for the remaining destination nodes.</target>
          </trans-unit>
          <trans-unit id="209" xml:space="preserve">
            <source>Convolutional bundles</source>
            <target state="new">Convolutional bundles</target>
          </trans-unit>
          <trans-unit id="210" xml:space="preserve">
            <source>When the training data has a homogeneous structure, convolutional connections are commonly used to learn high-level features of the data.</source>
            <target state="new">When the training data has a homogeneous structure, convolutional connections are commonly used to learn high-level features of the data.</target>
          </trans-unit>
          <trans-unit id="211" xml:space="preserve">
            <source>For example, in image, audio, or video data, spatial or temporal dimensionality can be fairly uniform.</source>
            <target state="new">For example, in image, audio, or video data, spatial or temporal dimensionality can be fairly uniform.</target>
          </trans-unit>
          <trans-unit id="212" xml:space="preserve">
            <source>Convolutional bundles employ rectangular <bpt id="2">&lt;strong&gt;</bpt>kernels<ept id="2">&lt;/strong&gt;</ept> that are slid through the dimensions.</source>
            <target state="new">Convolutional bundles employ rectangular <bpt id="2">&lt;strong&gt;</bpt>kernels<ept id="2">&lt;/strong&gt;</ept> that are slid through the dimensions.</target>
          </trans-unit>
          <trans-unit id="213" xml:space="preserve">
            <source>Essentially, each kernel defines a set of weights applied in local neighborhoods, referred to as <bpt id="2">&lt;strong&gt;</bpt>kernel applications<ept id="2">&lt;/strong&gt;</ept>.</source>
            <target state="new">Essentially, each kernel defines a set of weights applied in local neighborhoods, referred to as <bpt id="2">&lt;strong&gt;</bpt>kernel applications<ept id="2">&lt;/strong&gt;</ept>.</target>
          </trans-unit>
          <trans-unit id="214" xml:space="preserve">
            <source>Each kernel application corresponds to a node in the source layer, which is referred to as the <bpt id="2">&lt;strong&gt;</bpt>central node<ept id="2">&lt;/strong&gt;</ept>.</source>
            <target state="new">Each kernel application corresponds to a node in the source layer, which is referred to as the <bpt id="2">&lt;strong&gt;</bpt>central node<ept id="2">&lt;/strong&gt;</ept>.</target>
          </trans-unit>
          <trans-unit id="215" xml:space="preserve">
            <source>The weights of a kernel are shared among many connections.</source>
            <target state="new">The weights of a kernel are shared among many connections.</target>
          </trans-unit>
          <trans-unit id="216" xml:space="preserve">
            <source>In a convolutional bundle, each kernel is rectangular and all kernel applications are the same size.</source>
            <target state="new">In a convolutional bundle, each kernel is rectangular and all kernel applications are the same size.</target>
          </trans-unit>
          <trans-unit id="217" xml:space="preserve">
            <source>Convolutional bundles support the following attributes:</source>
            <target state="new">Convolutional bundles support the following attributes:</target>
          </trans-unit>
          <trans-unit id="218" xml:space="preserve">
            <source><bpt id="1">&lt;strong&gt;</bpt>InputShape<ept id="1">&lt;/strong&gt;</ept> defines the dimensionality of the source layer for the purposes of this convolutional bundle.</source>
            <target state="new"><bpt id="1">&lt;strong&gt;</bpt>InputShape<ept id="1">&lt;/strong&gt;</ept> defines the dimensionality of the source layer for the purposes of this convolutional bundle.</target>
          </trans-unit>
          <trans-unit id="219" xml:space="preserve">
            <source>The value must be a tuple of positive integers.</source>
            <target state="new">The value must be a tuple of positive integers.</target>
          </trans-unit>
          <trans-unit id="220" xml:space="preserve">
            <source>The product of the integers must equal the number of nodes in the source layer, but otherwise, it does not need to match the dimensionality declared for the source layer.</source>
            <target state="new">The product of the integers must equal the number of nodes in the source layer, but otherwise, it does not need to match the dimensionality declared for the source layer.</target>
          </trans-unit>
          <trans-unit id="221" xml:space="preserve">
            <source>The length of this tuple becomes the <bpt id="2">&lt;strong&gt;</bpt>arity<ept id="2">&lt;/strong&gt;</ept> value for the convolutional bundle.</source>
            <target state="new">The length of this tuple becomes the <bpt id="2">&lt;strong&gt;</bpt>arity<ept id="2">&lt;/strong&gt;</ept> value for the convolutional bundle.</target>
          </trans-unit>
          <trans-unit id="222" xml:space="preserve">
            <source>(Typically arity refers to the number of arguments or operands that a function can take.)</source>
            <target state="new">(Typically arity refers to the number of arguments or operands that a function can take.)</target>
          </trans-unit>
          <trans-unit id="223" xml:space="preserve">
            <source>To define the shape and locations of the kernels, use the attributes <bpt id="2">&lt;strong&gt;</bpt>KernelShape<ept id="2">&lt;/strong&gt;</ept>, <bpt id="4">&lt;strong&gt;</bpt>Stride<ept id="4">&lt;/strong&gt;</ept>, <bpt id="6">&lt;strong&gt;</bpt>Padding<ept id="6">&lt;/strong&gt;</ept>, <bpt id="8">&lt;strong&gt;</bpt>LowerPad<ept id="8">&lt;/strong&gt;</ept>, and <bpt id="10">&lt;strong&gt;</bpt>UpperPad<ept id="10">&lt;/strong&gt;</ept>:</source>
            <target state="new">To define the shape and locations of the kernels, use the attributes <bpt id="2">&lt;strong&gt;</bpt>KernelShape<ept id="2">&lt;/strong&gt;</ept>, <bpt id="4">&lt;strong&gt;</bpt>Stride<ept id="4">&lt;/strong&gt;</ept>, <bpt id="6">&lt;strong&gt;</bpt>Padding<ept id="6">&lt;/strong&gt;</ept>, <bpt id="8">&lt;strong&gt;</bpt>LowerPad<ept id="8">&lt;/strong&gt;</ept>, and <bpt id="10">&lt;strong&gt;</bpt>UpperPad<ept id="10">&lt;/strong&gt;</ept>:</target>
          </trans-unit>
          <trans-unit id="224" xml:space="preserve">
            <source><bpt id="1">&lt;strong&gt;</bpt>KernelShape<ept id="1">&lt;/strong&gt;</ept>: (required) Defines the dimensionality of each kernel for the convolutional bundle.</source>
            <target state="new"><bpt id="1">&lt;strong&gt;</bpt>KernelShape<ept id="1">&lt;/strong&gt;</ept>: (required) Defines the dimensionality of each kernel for the convolutional bundle.</target>
          </trans-unit>
          <trans-unit id="225" xml:space="preserve">
            <source>The value must be a tuple of positive integers with a length that equals the arity of the bundle.</source>
            <target state="new">The value must be a tuple of positive integers with a length that equals the arity of the bundle.</target>
          </trans-unit>
          <trans-unit id="226" xml:space="preserve">
            <source>Each component of this tuple must be no greater than the corresponding component of <bpt id="2">&lt;strong&gt;</bpt>InputShape<ept id="2">&lt;/strong&gt;</ept>.</source>
            <target state="new">Each component of this tuple must be no greater than the corresponding component of <bpt id="2">&lt;strong&gt;</bpt>InputShape<ept id="2">&lt;/strong&gt;</ept>.</target>
          </trans-unit>
          <trans-unit id="227" xml:space="preserve">
            <source><bpt id="1">&lt;strong&gt;</bpt>Stride<ept id="1">&lt;/strong&gt;</ept>: (optional) Defines the sliding step sizes of the convolution (one step size for each dimension), that is the distance between the central nodes.</source>
            <target state="new"><bpt id="1">&lt;strong&gt;</bpt>Stride<ept id="1">&lt;/strong&gt;</ept>: (optional) Defines the sliding step sizes of the convolution (one step size for each dimension), that is the distance between the central nodes.</target>
          </trans-unit>
          <trans-unit id="228" xml:space="preserve">
            <source>The value must be a tuple of positive integers with a length that is the arity of the bundle.</source>
            <target state="new">The value must be a tuple of positive integers with a length that is the arity of the bundle.</target>
          </trans-unit>
          <trans-unit id="229" xml:space="preserve">
            <source>Each component of this tuple must be no greater than the corresponding component of <bpt id="2">&lt;strong&gt;</bpt>KernelShape<ept id="2">&lt;/strong&gt;</ept>.</source>
            <target state="new">Each component of this tuple must be no greater than the corresponding component of <bpt id="2">&lt;strong&gt;</bpt>KernelShape<ept id="2">&lt;/strong&gt;</ept>.</target>
          </trans-unit>
          <trans-unit id="230" xml:space="preserve">
            <source>The default value is a tuple with all components equal to one.</source>
            <target state="new">The default value is a tuple with all components equal to one.</target>
          </trans-unit>
          <trans-unit id="231" xml:space="preserve">
            <source><bpt id="1">&lt;strong&gt;</bpt>Sharing<ept id="1">&lt;/strong&gt;</ept>: (optional) Defines the weight sharing for each dimension of the convolution.</source>
            <target state="new"><bpt id="1">&lt;strong&gt;</bpt>Sharing<ept id="1">&lt;/strong&gt;</ept>: (optional) Defines the weight sharing for each dimension of the convolution.</target>
          </trans-unit>
          <trans-unit id="232" xml:space="preserve">
            <source>The value can be a single Boolean value or a tuple of Boolean values with a length that is the arity of the bundle.</source>
            <target state="new">The value can be a single Boolean value or a tuple of Boolean values with a length that is the arity of the bundle.</target>
          </trans-unit>
          <trans-unit id="233" xml:space="preserve">
            <source>A single Boolean value is extended to be a tuple of the correct length with all components equal to the specified value.</source>
            <target state="new">A single Boolean value is extended to be a tuple of the correct length with all components equal to the specified value.</target>
          </trans-unit>
          <trans-unit id="234" xml:space="preserve">
            <source>The default value is a tuple that consists of all True values.</source>
            <target state="new">The default value is a tuple that consists of all True values.</target>
          </trans-unit>
          <trans-unit id="235" xml:space="preserve">
            <source><bpt id="1">&lt;strong&gt;</bpt>MapCount<ept id="1">&lt;/strong&gt;</ept>: (optional) Defines the number of feature maps for the convolutional bundle.</source>
            <target state="new"><bpt id="1">&lt;strong&gt;</bpt>MapCount<ept id="1">&lt;/strong&gt;</ept>: (optional) Defines the number of feature maps for the convolutional bundle.</target>
          </trans-unit>
          <trans-unit id="236" xml:space="preserve">
            <source>The value can be a single positive integer or a tuple of positive integers with a length that is the arity of the bundle.</source>
            <target state="new">The value can be a single positive integer or a tuple of positive integers with a length that is the arity of the bundle.</target>
          </trans-unit>
          <trans-unit id="237" xml:space="preserve">
            <source>A single integer value is extended to be a tuple of the correct length with the first components equal to the specified value and all the remaining components equal to one.</source>
            <target state="new">A single integer value is extended to be a tuple of the correct length with the first components equal to the specified value and all the remaining components equal to one.</target>
          </trans-unit>
          <trans-unit id="238" xml:space="preserve">
            <source>The default value is one.</source>
            <target state="new">The default value is one.</target>
          </trans-unit>
          <trans-unit id="239" xml:space="preserve">
            <source>The total number of feature maps is the product of the components of the tuple.</source>
            <target state="new">The total number of feature maps is the product of the components of the tuple.</target>
          </trans-unit>
          <trans-unit id="240" xml:space="preserve">
            <source>The factoring of this total number across the components determines how the feature map values are grouped in the destination nodes.</source>
            <target state="new">The factoring of this total number across the components determines how the feature map values are grouped in the destination nodes.</target>
          </trans-unit>
          <trans-unit id="241" xml:space="preserve">
            <source><bpt id="1">&lt;strong&gt;</bpt>Weights<ept id="1">&lt;/strong&gt;</ept>: (optional) Defines the initial weights for the bundle.</source>
            <target state="new"><bpt id="1">&lt;strong&gt;</bpt>Weights<ept id="1">&lt;/strong&gt;</ept>: (optional) Defines the initial weights for the bundle.</target>
          </trans-unit>
          <trans-unit id="242" xml:space="preserve">
            <source>The value must be a tuple of floating point values with a length that is the number of kernels times the number of weights per kernel, as defined later in this article.</source>
            <target state="new">The value must be a tuple of floating point values with a length that is the number of kernels times the number of weights per kernel, as defined later in this article.</target>
          </trans-unit>
          <trans-unit id="243" xml:space="preserve">
            <source>The default weights are randomly generated.</source>
            <target state="new">The default weights are randomly generated.</target>
          </trans-unit>
          <trans-unit id="244" xml:space="preserve">
            <source>There are two sets of properties that control padding, which are mutually exclusive:</source>
            <target state="new">There are two sets of properties that control padding, which are mutually exclusive:</target>
          </trans-unit>
          <trans-unit id="245" xml:space="preserve">
            <source><bpt id="1">&lt;strong&gt;</bpt>Padding<ept id="1">&lt;/strong&gt;</ept>: (optional) Determines whether the input should be padded by using a <bpt id="3">&lt;strong&gt;</bpt>default padding scheme<ept id="3">&lt;/strong&gt;</ept>.</source>
            <target state="new"><bpt id="1">&lt;strong&gt;</bpt>Padding<ept id="1">&lt;/strong&gt;</ept>: (optional) Determines whether the input should be padded by using a <bpt id="3">&lt;strong&gt;</bpt>default padding scheme<ept id="3">&lt;/strong&gt;</ept>.</target>
          </trans-unit>
          <trans-unit id="246" xml:space="preserve">
            <source>The value can be a single Boolean value, or it can be a tuple of Boolean values with a length that is the arity of the bundle.</source>
            <target state="new">The value can be a single Boolean value, or it can be a tuple of Boolean values with a length that is the arity of the bundle.</target>
          </trans-unit>
          <trans-unit id="247" xml:space="preserve">
            <source>A single Boolean value is extended to be a tuple of the correct length with all components equal to the specified value.</source>
            <target state="new">A single Boolean value is extended to be a tuple of the correct length with all components equal to the specified value.</target>
          </trans-unit>
          <trans-unit id="248" xml:space="preserve">
            <source>If the value for a dimension is True, the source is logically padded in that dimension with zero-valued cells to support additional kernel applications, such that the central nodes of the first and last kernels in that dimension are the first and last nodes in that dimension in the source layer.</source>
            <target state="new">If the value for a dimension is True, the source is logically padded in that dimension with zero-valued cells to support additional kernel applications, such that the central nodes of the first and last kernels in that dimension are the first and last nodes in that dimension in the source layer.</target>
          </trans-unit>
          <trans-unit id="249" xml:space="preserve">
            <source>Thus, the number of "dummy" nodes in each dimension is determined automatically, to fit exactly _(InputShape[d] - 1) / Stride[d] + 1_ kernels into the padded source layer.</source>
            <target state="new">Thus, the number of "dummy" nodes in each dimension is determined automatically, to fit exactly _(InputShape[d] - 1) / Stride[d] + 1_ kernels into the padded source layer.</target>
          </trans-unit>
          <trans-unit id="250" xml:space="preserve">
            <source>If the value for a dimension is False, the kernels are defined so that the number of nodes on each side that are left out is the same (up to a difference of 1).</source>
            <target state="new">If the value for a dimension is False, the kernels are defined so that the number of nodes on each side that are left out is the same (up to a difference of 1).</target>
          </trans-unit>
          <trans-unit id="251" xml:space="preserve">
            <source>The default value of this attribute is a tuple with all components equal to False.</source>
            <target state="new">The default value of this attribute is a tuple with all components equal to False.</target>
          </trans-unit>
          <trans-unit id="252" xml:space="preserve">
            <source><bpt id="1">&lt;strong&gt;</bpt>UpperPad<ept id="1">&lt;/strong&gt;</ept> and <bpt id="3">&lt;strong&gt;</bpt>LowerPad<ept id="3">&lt;/strong&gt;</ept>: (optional) Provide greater control over the amount of padding to use.</source>
            <target state="new"><bpt id="1">&lt;strong&gt;</bpt>UpperPad<ept id="1">&lt;/strong&gt;</ept> and <bpt id="3">&lt;strong&gt;</bpt>LowerPad<ept id="3">&lt;/strong&gt;</ept>: (optional) Provide greater control over the amount of padding to use.</target>
          </trans-unit>
          <trans-unit id="253" xml:space="preserve">
            <source><bpt id="1">&lt;strong&gt;</bpt>Important:<ept id="1">&lt;/strong&gt;</ept> These attributes can be defined if and only if the <bpt id="3">&lt;strong&gt;</bpt>Padding<ept id="3">&lt;/strong&gt;</ept> property above is <bpt id="5">&lt;strong&gt;</bpt>*not*<ept id="5">&lt;/strong&gt;</ept> defined.</source>
            <target state="new"><bpt id="1">&lt;strong&gt;</bpt>Important:<ept id="1">&lt;/strong&gt;</ept> These attributes can be defined if and only if the <bpt id="3">&lt;strong&gt;</bpt>Padding<ept id="3">&lt;/strong&gt;</ept> property above is <bpt id="5">&lt;strong&gt;</bpt>*not*<ept id="5">&lt;/strong&gt;</ept> defined.</target>
          </trans-unit>
          <trans-unit id="254" xml:space="preserve">
            <source>The values should be integer-valued tuples with lengths that are the arity of the bundle.</source>
            <target state="new">The values should be integer-valued tuples with lengths that are the arity of the bundle.</target>
          </trans-unit>
          <trans-unit id="255" xml:space="preserve">
            <source>When these attributes are specified, "dummy" nodes are added to the lower and upper ends of each dimension of the input layer.</source>
            <target state="new">When these attributes are specified, "dummy" nodes are added to the lower and upper ends of each dimension of the input layer.</target>
          </trans-unit>
          <trans-unit id="256" xml:space="preserve">
            <source>The number of nodes added to the lower and upper ends in each dimension is determined by <bpt id="2">&lt;strong&gt;</bpt>LowerPad<ept id="2">&lt;/strong&gt;</ept>[i] and <bpt id="4">&lt;strong&gt;</bpt>UpperPad<ept id="4">&lt;/strong&gt;</ept>[i] respectively.</source>
            <target state="new">The number of nodes added to the lower and upper ends in each dimension is determined by <bpt id="2">&lt;strong&gt;</bpt>LowerPad<ept id="2">&lt;/strong&gt;</ept>[i] and <bpt id="4">&lt;strong&gt;</bpt>UpperPad<ept id="4">&lt;/strong&gt;</ept>[i] respectively.</target>
          </trans-unit>
          <trans-unit id="257" xml:space="preserve">
            <source>To ensure that kernels correspond only to "real" nodes and not to "dummy" nodes, the following conditions must be met:</source>
            <target state="new">To ensure that kernels correspond only to "real" nodes and not to "dummy" nodes, the following conditions must be met:</target>
          </trans-unit>
          <trans-unit id="258" xml:space="preserve">
            <source>Each component of <bpt id="2">&lt;strong&gt;</bpt>LowerPad<ept id="2">&lt;/strong&gt;</ept> must be strictly less than KernelShape[d]/2.</source>
            <target state="new">Each component of <bpt id="2">&lt;strong&gt;</bpt>LowerPad<ept id="2">&lt;/strong&gt;</ept> must be strictly less than KernelShape[d]/2.</target>
          </trans-unit>
          <trans-unit id="259" xml:space="preserve">
            <source>Each component of <bpt id="2">&lt;strong&gt;</bpt>UpperPad<ept id="2">&lt;/strong&gt;</ept> must be no greater than KernelShape[d]/2.</source>
            <target state="new">Each component of <bpt id="2">&lt;strong&gt;</bpt>UpperPad<ept id="2">&lt;/strong&gt;</ept> must be no greater than KernelShape[d]/2.</target>
          </trans-unit>
          <trans-unit id="260" xml:space="preserve">
            <source>The default value of these attributes is a tuple with all components equal to 0.</source>
            <target state="new">The default value of these attributes is a tuple with all components equal to 0.</target>
          </trans-unit>
          <trans-unit id="261" xml:space="preserve">
            <source>The setting <bpt id="2">&lt;strong&gt;</bpt>Padding<ept id="2">&lt;/strong&gt;</ept> = true allows as much padding as is needed to keep the "center" of the kernel inside the "real" input.</source>
            <target state="new">The setting <bpt id="2">&lt;strong&gt;</bpt>Padding<ept id="2">&lt;/strong&gt;</ept> = true allows as much padding as is needed to keep the "center" of the kernel inside the "real" input.</target>
          </trans-unit>
          <trans-unit id="262" xml:space="preserve">
            <source>This changes the math a bit for computing the output size.</source>
            <target state="new">This changes the math a bit for computing the output size.</target>
          </trans-unit>
          <trans-unit id="263" xml:space="preserve">
            <source>Generally, the output size <bpt id="2">&lt;em&gt;</bpt>D<ept id="2">&lt;/em&gt;</ept> is computed as _D = (I - K) / S + 1_, where <bpt id="4">&lt;em&gt;</bpt>I<ept id="4">&lt;/em&gt;</ept> is the input size, <bpt id="6">&lt;em&gt;</bpt>K<ept id="6">&lt;/em&gt;</ept> is the kernel size, <bpt id="8">&lt;em&gt;</bpt>S<ept id="8">&lt;/em&gt;</ept> is the stride, and <bpt id="10">&lt;em&gt;</bpt>/<ept id="10">&lt;/em&gt;</ept> is integer division (round toward zero).</source>
            <target state="new">Generally, the output size <bpt id="2">&lt;em&gt;</bpt>D<ept id="2">&lt;/em&gt;</ept> is computed as _D = (I - K) / S + 1_, where <bpt id="4">&lt;em&gt;</bpt>I<ept id="4">&lt;/em&gt;</ept> is the input size, <bpt id="6">&lt;em&gt;</bpt>K<ept id="6">&lt;/em&gt;</ept> is the kernel size, <bpt id="8">&lt;em&gt;</bpt>S<ept id="8">&lt;/em&gt;</ept> is the stride, and <bpt id="10">&lt;em&gt;</bpt>/<ept id="10">&lt;/em&gt;</ept> is integer division (round toward zero).</target>
          </trans-unit>
          <trans-unit id="264" xml:space="preserve">
            <source>If you set UpperPad = [1, 1], the input size <bpt id="2">&lt;em&gt;</bpt>I<ept id="2">&lt;/em&gt;</ept> is effectively 29, and thus _D = (29 - 5) / 2 + 1 = 13_.</source>
            <target state="new">If you set UpperPad = [1, 1], the input size <bpt id="2">&lt;em&gt;</bpt>I<ept id="2">&lt;/em&gt;</ept> is effectively 29, and thus _D = (29 - 5) / 2 + 1 = 13_.</target>
          </trans-unit>
          <trans-unit id="265" xml:space="preserve">
            <source>However, when <bpt id="2">&lt;strong&gt;</bpt>Padding<ept id="2">&lt;/strong&gt;</ept> = true, essentially <bpt id="4">&lt;em&gt;</bpt>I<ept id="4">&lt;/em&gt;</ept> gets bumped up by _K - 1_; hence _D = ((28 + 4) - 5) / 2 + 1 = 27 / 2 + 1 = 13 + 1 = 14_.</source>
            <target state="new">However, when <bpt id="2">&lt;strong&gt;</bpt>Padding<ept id="2">&lt;/strong&gt;</ept> = true, essentially <bpt id="4">&lt;em&gt;</bpt>I<ept id="4">&lt;/em&gt;</ept> gets bumped up by _K - 1_; hence _D = ((28 + 4) - 5) / 2 + 1 = 27 / 2 + 1 = 13 + 1 = 14_.</target>
          </trans-unit>
          <trans-unit id="266" xml:space="preserve">
            <source>By specifying values for <bpt id="2">&lt;strong&gt;</bpt>UpperPad<ept id="2">&lt;/strong&gt;</ept> and <bpt id="4">&lt;strong&gt;</bpt>LowerPad<ept id="4">&lt;/strong&gt;</ept> you get much more control over the padding than if you just set <bpt id="6">&lt;strong&gt;</bpt>Padding<ept id="6">&lt;/strong&gt;</ept> = true.</source>
            <target state="new">By specifying values for <bpt id="2">&lt;strong&gt;</bpt>UpperPad<ept id="2">&lt;/strong&gt;</ept> and <bpt id="4">&lt;strong&gt;</bpt>LowerPad<ept id="4">&lt;/strong&gt;</ept> you get much more control over the padding than if you just set <bpt id="6">&lt;strong&gt;</bpt>Padding<ept id="6">&lt;/strong&gt;</ept> = true.</target>
          </trans-unit>
          <trans-unit id="267" xml:space="preserve">
            <source>For more information about convolutional networks and their applications, see these articles:</source>
            <target state="new">For more information about convolutional networks and their applications, see these articles:</target>
          </trans-unit>
          <trans-unit id="268" xml:space="preserve">
            <source><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>http://deeplearning.net/tutorial/lenet.html <ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept></source>
            <target state="new"><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>http://deeplearning.net/tutorial/lenet.html <ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept></target>
          </trans-unit>
          <trans-unit id="269" xml:space="preserve">
            <source><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>http://research.microsoft.com/pubs/68920/icdar03.pdf<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept></source>
            <target state="new"><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>http://research.microsoft.com/pubs/68920/icdar03.pdf<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept></target>
          </trans-unit>
          <trans-unit id="270" xml:space="preserve">
            <source><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>http://people.csail.mit.edu/jvb/papers/cnn_tutorial.pdf<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept></source>
            <target state="new"><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>http://people.csail.mit.edu/jvb/papers/cnn_tutorial.pdf<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept></target>
          </trans-unit>
          <trans-unit id="271" xml:space="preserve">
            <source>Pooling bundles</source>
            <target state="new">Pooling bundles</target>
          </trans-unit>
          <trans-unit id="272" xml:space="preserve">
            <source>A <bpt id="2">&lt;strong&gt;</bpt>pooling bundle<ept id="2">&lt;/strong&gt;</ept> applies geometry similar to convolutional connectivity, but it uses predefined functions to source node values to derive the destination node value.</source>
            <target state="new">A <bpt id="2">&lt;strong&gt;</bpt>pooling bundle<ept id="2">&lt;/strong&gt;</ept> applies geometry similar to convolutional connectivity, but it uses predefined functions to source node values to derive the destination node value.</target>
          </trans-unit>
          <trans-unit id="273" xml:space="preserve">
            <source>Hence, pooling bundles have no trainable state (weights or biases).</source>
            <target state="new">Hence, pooling bundles have no trainable state (weights or biases).</target>
          </trans-unit>
          <trans-unit id="274" xml:space="preserve">
            <source>Pooling bundles support all the convolutional attributes except <bpt id="2">&lt;strong&gt;</bpt>Sharing<ept id="2">&lt;/strong&gt;</ept>, <bpt id="4">&lt;strong&gt;</bpt>MapCount<ept id="4">&lt;/strong&gt;</ept>, and <bpt id="6">&lt;strong&gt;</bpt>Weights<ept id="6">&lt;/strong&gt;</ept>.</source>
            <target state="new">Pooling bundles support all the convolutional attributes except <bpt id="2">&lt;strong&gt;</bpt>Sharing<ept id="2">&lt;/strong&gt;</ept>, <bpt id="4">&lt;strong&gt;</bpt>MapCount<ept id="4">&lt;/strong&gt;</ept>, and <bpt id="6">&lt;strong&gt;</bpt>Weights<ept id="6">&lt;/strong&gt;</ept>.</target>
          </trans-unit>
          <trans-unit id="275" xml:space="preserve">
            <source>Typically, the kernels summarized by adjacent pooling units do not overlap.</source>
            <target state="new">Typically, the kernels summarized by adjacent pooling units do not overlap.</target>
          </trans-unit>
          <trans-unit id="276" xml:space="preserve">
            <source>If Stride[d] is equal to KernelShape[d] in each dimension, the layer obtained is the traditional local pooling layer, which is commonly employed in convolutional neural networks.</source>
            <target state="new">If Stride[d] is equal to KernelShape[d] in each dimension, the layer obtained is the traditional local pooling layer, which is commonly employed in convolutional neural networks.</target>
          </trans-unit>
          <trans-unit id="277" xml:space="preserve">
            <source>Each destination node computes the maximum or the mean of the activities of its kernel in the source layer.</source>
            <target state="new">Each destination node computes the maximum or the mean of the activities of its kernel in the source layer.</target>
          </trans-unit>
          <trans-unit id="278" xml:space="preserve">
            <source>The following example illustrates a pooling bundle:</source>
            <target state="new">The following example illustrates a pooling bundle:</target>
          </trans-unit>
          <trans-unit id="279" xml:space="preserve">
            <source>The arity of the bundle is 3 (the length of the tuples <bpt id="2">&lt;strong&gt;</bpt>InputShape<ept id="2">&lt;/strong&gt;</ept>, <bpt id="4">&lt;strong&gt;</bpt>KernelShape<ept id="4">&lt;/strong&gt;</ept>, and <bpt id="6">&lt;strong&gt;</bpt>Stride<ept id="6">&lt;/strong&gt;</ept>).</source>
            <target state="new">The arity of the bundle is 3 (the length of the tuples <bpt id="2">&lt;strong&gt;</bpt>InputShape<ept id="2">&lt;/strong&gt;</ept>, <bpt id="4">&lt;strong&gt;</bpt>KernelShape<ept id="4">&lt;/strong&gt;</ept>, and <bpt id="6">&lt;strong&gt;</bpt>Stride<ept id="6">&lt;/strong&gt;</ept>).</target>
          </trans-unit>
          <trans-unit id="280" xml:space="preserve">
            <source>The number of nodes in the source layer is _5 * 24 * 24 = 2880_.</source>
            <target state="new">The number of nodes in the source layer is _5 * 24 * 24 = 2880_.</target>
          </trans-unit>
          <trans-unit id="281" xml:space="preserve">
            <source>This is a traditional local pooling layer because <bpt id="2">&lt;strong&gt;</bpt>KernelShape<ept id="2">&lt;/strong&gt;</ept> and <bpt id="4">&lt;strong&gt;</bpt>Stride<ept id="4">&lt;/strong&gt;</ept> are equal.</source>
            <target state="new">This is a traditional local pooling layer because <bpt id="2">&lt;strong&gt;</bpt>KernelShape<ept id="2">&lt;/strong&gt;</ept> and <bpt id="4">&lt;strong&gt;</bpt>Stride<ept id="4">&lt;/strong&gt;</ept> are equal.</target>
          </trans-unit>
          <trans-unit id="282" xml:space="preserve">
            <source>The number of nodes in the destination layer is _5 * 12 * 12 = 1440_.</source>
            <target state="new">The number of nodes in the destination layer is _5 * 12 * 12 = 1440_.</target>
          </trans-unit>
          <trans-unit id="283" xml:space="preserve">
            <source>For more information about pooling layers, see these articles:</source>
            <target state="new">For more information about pooling layers, see these articles:</target>
          </trans-unit>
          <trans-unit id="284" xml:space="preserve">
            <source><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>http://www.cs.toronto.edu/~hinton/absps/imagenet.pdf<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept> (Section 3.4)</source>
            <target state="new"><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>http://www.cs.toronto.edu/~hinton/absps/imagenet.pdf<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept> (Section 3.4)</target>
          </trans-unit>
          <trans-unit id="285" xml:space="preserve">
            <source><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>http://cs.nyu.edu/~koray/publis/lecun-iscas-10.pdf<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept></source>
            <target state="new"><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>http://cs.nyu.edu/~koray/publis/lecun-iscas-10.pdf<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept></target>
          </trans-unit>
          <trans-unit id="286" xml:space="preserve">
            <source><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>http://cs.nyu.edu/~koray/publis/jarrett-iccv-09.pdf<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept></source>
            <target state="new"><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>http://cs.nyu.edu/~koray/publis/jarrett-iccv-09.pdf<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept></target>
          </trans-unit>
          <trans-unit id="287" xml:space="preserve">
            <source>Response normalization bundles</source>
            <target state="new">Response normalization bundles</target>
          </trans-unit>
          <trans-unit id="288" xml:space="preserve">
            <source><bpt id="1">&lt;strong&gt;</bpt>Response normalization<ept id="1">&lt;/strong&gt;</ept> is a local normalization scheme that was first introduced by Geoffrey Hinton, et al, in a paper titled ImageNet Classiﬁcation with Deep Convolutional Neural Networks (see section 3.3).</source>
            <target state="new"><bpt id="1">&lt;strong&gt;</bpt>Response normalization<ept id="1">&lt;/strong&gt;</ept> is a local normalization scheme that was first introduced by Geoffrey Hinton, et al, in a paper titled ImageNet Classiﬁcation with Deep Convolutional Neural Networks (see section 3.3).</target>
          </trans-unit>
          <trans-unit id="289" xml:space="preserve">
            <source>Response normalization is used to aid generalization in neural nets.</source>
            <target state="new">Response normalization is used to aid generalization in neural nets.</target>
          </trans-unit>
          <trans-unit id="290" xml:space="preserve">
            <source>When one neuron is firing at a very high activation level, a local response normalization layer suppresses the activation level of the surrounding neurons.</source>
            <target state="new">When one neuron is firing at a very high activation level, a local response normalization layer suppresses the activation level of the surrounding neurons.</target>
          </trans-unit>
          <trans-unit id="291" xml:space="preserve">
            <source>This is done by using three parameters (<bpt id="2">&lt;strong&gt;</bpt>*α*<ept id="2">&lt;/strong&gt;</ept>, <bpt id="4">&lt;strong&gt;</bpt>*β*<ept id="4">&lt;/strong&gt;</ept>, and <bpt id="6">&lt;strong&gt;</bpt>*k*<ept id="6">&lt;/strong&gt;</ept>) and a convolutional structure (or neighborhood shape).</source>
            <target state="new">This is done by using three parameters (<bpt id="2">&lt;strong&gt;</bpt>*α*<ept id="2">&lt;/strong&gt;</ept>, <bpt id="4">&lt;strong&gt;</bpt>*β*<ept id="4">&lt;/strong&gt;</ept>, and <bpt id="6">&lt;strong&gt;</bpt>*k*<ept id="6">&lt;/strong&gt;</ept>) and a convolutional structure (or neighborhood shape).</target>
          </trans-unit>
          <trans-unit id="292" xml:space="preserve">
            <source>Every neuron in the destination layer <bpt id="2">&lt;strong&gt;</bpt>*y*<ept id="2">&lt;/strong&gt;</ept> corresponds to a neuron <bpt id="4">&lt;strong&gt;</bpt>*x*<ept id="4">&lt;/strong&gt;</ept> in the source layer.</source>
            <target state="new">Every neuron in the destination layer <bpt id="2">&lt;strong&gt;</bpt>*y*<ept id="2">&lt;/strong&gt;</ept> corresponds to a neuron <bpt id="4">&lt;strong&gt;</bpt>*x*<ept id="4">&lt;/strong&gt;</ept> in the source layer.</target>
          </trans-unit>
          <trans-unit id="293" xml:space="preserve">
            <source>The activation level of <bpt id="2">&lt;strong&gt;</bpt>*y*<ept id="2">&lt;/strong&gt;</ept> is given by the following formula, where <bpt id="4">&lt;strong&gt;</bpt>*f*<ept id="4">&lt;/strong&gt;</ept> is the activation level of a neuron, and <bpt id="6">&lt;strong&gt;</bpt>*Nx*<ept id="6">&lt;/strong&gt;</ept> is the kernel (or the set that contains the neurons in the neighborhood of <bpt id="8">&lt;strong&gt;</bpt>*x*<ept id="8">&lt;/strong&gt;</ept>), as defined by the following convolutional structure:</source>
            <target state="new">The activation level of <bpt id="2">&lt;strong&gt;</bpt>*y*<ept id="2">&lt;/strong&gt;</ept> is given by the following formula, where <bpt id="4">&lt;strong&gt;</bpt>*f*<ept id="4">&lt;/strong&gt;</ept> is the activation level of a neuron, and <bpt id="6">&lt;strong&gt;</bpt>*Nx*<ept id="6">&lt;/strong&gt;</ept> is the kernel (or the set that contains the neurons in the neighborhood of <bpt id="8">&lt;strong&gt;</bpt>*x*<ept id="8">&lt;/strong&gt;</ept>), as defined by the following convolutional structure:</target>
          </trans-unit>
          <trans-unit id="294" xml:space="preserve">
            <source><bpt id="1">&lt;linkText&gt;</bpt><ept id="1">&lt;/linkText&gt;</ept></source>
            <target state="new"><bpt id="1">&lt;linkText&gt;</bpt><ept id="1">&lt;/linkText&gt;</ept></target>
          </trans-unit>
          <trans-unit id="295" xml:space="preserve">
            <source>Response normalization bundles support all the convolutional attributes except <bpt id="2">&lt;strong&gt;</bpt>Sharing<ept id="2">&lt;/strong&gt;</ept>, <bpt id="4">&lt;strong&gt;</bpt>MapCount<ept id="4">&lt;/strong&gt;</ept>, and <bpt id="6">&lt;strong&gt;</bpt>Weights<ept id="6">&lt;/strong&gt;</ept>.</source>
            <target state="new">Response normalization bundles support all the convolutional attributes except <bpt id="2">&lt;strong&gt;</bpt>Sharing<ept id="2">&lt;/strong&gt;</ept>, <bpt id="4">&lt;strong&gt;</bpt>MapCount<ept id="4">&lt;/strong&gt;</ept>, and <bpt id="6">&lt;strong&gt;</bpt>Weights<ept id="6">&lt;/strong&gt;</ept>.</target>
          </trans-unit>
          <trans-unit id="296" xml:space="preserve">
            <source>If the kernel contains neurons in the same map as <bpt id="2">&lt;strong&gt;</bpt>*x*<ept id="2">&lt;/strong&gt;</ept>, the normalization scheme is referred to as <bpt id="4">&lt;strong&gt;</bpt>same map normalization<ept id="4">&lt;/strong&gt;</ept>.</source>
            <target state="new">If the kernel contains neurons in the same map as <bpt id="2">&lt;strong&gt;</bpt>*x*<ept id="2">&lt;/strong&gt;</ept>, the normalization scheme is referred to as <bpt id="4">&lt;strong&gt;</bpt>same map normalization<ept id="4">&lt;/strong&gt;</ept>.</target>
          </trans-unit>
          <trans-unit id="297" xml:space="preserve">
            <source>To define same map normalization, the first coordinate in <bpt id="2">&lt;strong&gt;</bpt>InputShape<ept id="2">&lt;/strong&gt;</ept> must have the value 1.</source>
            <target state="new">To define same map normalization, the first coordinate in <bpt id="2">&lt;strong&gt;</bpt>InputShape<ept id="2">&lt;/strong&gt;</ept> must have the value 1.</target>
          </trans-unit>
          <trans-unit id="298" xml:space="preserve">
            <source>If the kernel contains neurons in the same spatial position as <bpt id="2">&lt;strong&gt;</bpt>*x*<ept id="2">&lt;/strong&gt;</ept>, but the neurons are in other maps, the normalization scheme is called <bpt id="4">&lt;strong&gt;</bpt>across maps normalization<ept id="4">&lt;/strong&gt;</ept>.</source>
            <target state="new">If the kernel contains neurons in the same spatial position as <bpt id="2">&lt;strong&gt;</bpt>*x*<ept id="2">&lt;/strong&gt;</ept>, but the neurons are in other maps, the normalization scheme is called <bpt id="4">&lt;strong&gt;</bpt>across maps normalization<ept id="4">&lt;/strong&gt;</ept>.</target>
          </trans-unit>
          <trans-unit id="299" xml:space="preserve">
            <source>This type of response normalization implements a form of lateral inhibition inspired by the type found in real neurons, creating competition for big activation levels amongst neuron outputs computed on different maps.</source>
            <target state="new">This type of response normalization implements a form of lateral inhibition inspired by the type found in real neurons, creating competition for big activation levels amongst neuron outputs computed on different maps.</target>
          </trans-unit>
          <trans-unit id="300" xml:space="preserve">
            <source>To define across maps normalization, the first coordinate must be an integer greater than one and no greater than the number of maps, and the rest of the coordinates must have the value 1.</source>
            <target state="new">To define across maps normalization, the first coordinate must be an integer greater than one and no greater than the number of maps, and the rest of the coordinates must have the value 1.</target>
          </trans-unit>
          <trans-unit id="301" xml:space="preserve">
            <source>Because response normalization bundles apply a predefined function to source node values to determine the destination node value, they have no trainable state (weights or biases).</source>
            <target state="new">Because response normalization bundles apply a predefined function to source node values to determine the destination node value, they have no trainable state (weights or biases).</target>
          </trans-unit>
          <trans-unit id="302" xml:space="preserve">
            <source><bpt id="1">&lt;strong&gt;</bpt>Alert<ept id="1">&lt;/strong&gt;</ept>: The nodes in the destination layer correspond to neurons that are the central nodes of the kernels.</source>
            <target state="new"><bpt id="1">&lt;strong&gt;</bpt>Alert<ept id="1">&lt;/strong&gt;</ept>: The nodes in the destination layer correspond to neurons that are the central nodes of the kernels.</target>
          </trans-unit>
          <trans-unit id="303" xml:space="preserve">
            <source>For example, if KernelShape[d] is odd, then <bpt id="2">&lt;em&gt;</bpt>KernelShape[d]/2<ept id="2">&lt;/em&gt;</ept> corresponds to the central kernel node.</source>
            <target state="new">For example, if KernelShape[d] is odd, then <bpt id="2">&lt;em&gt;</bpt>KernelShape[d]/2<ept id="2">&lt;/em&gt;</ept> corresponds to the central kernel node.</target>
          </trans-unit>
          <trans-unit id="304" xml:space="preserve">
            <source>If <bpt id="2">&lt;em&gt;</bpt>KernelShape[d]<ept id="2">&lt;/em&gt;</ept> is even, the central node is at _KernelShape[d]/2 - 1_.</source>
            <target state="new">If <bpt id="2">&lt;em&gt;</bpt>KernelShape[d]<ept id="2">&lt;/em&gt;</ept> is even, the central node is at _KernelShape[d]/2 - 1_.</target>
          </trans-unit>
          <trans-unit id="305" xml:space="preserve">
            <source>Therefore, if <bpt id="2">&lt;strong&gt;</bpt>Padding<ept id="2">&lt;/strong&gt;</ept>[d] is False, the first and the last <bpt id="4">&lt;em&gt;</bpt>KernelShape[d]/2<ept id="4">&lt;/em&gt;</ept> nodes do not have corresponding nodes in the destination layer.</source>
            <target state="new">Therefore, if <bpt id="2">&lt;strong&gt;</bpt>Padding<ept id="2">&lt;/strong&gt;</ept>[d] is False, the first and the last <bpt id="4">&lt;em&gt;</bpt>KernelShape[d]/2<ept id="4">&lt;/em&gt;</ept> nodes do not have corresponding nodes in the destination layer.</target>
          </trans-unit>
          <trans-unit id="306" xml:space="preserve">
            <source>To avoid this situation, define <bpt id="2">&lt;strong&gt;</bpt>Padding<ept id="2">&lt;/strong&gt;</ept> as [true, true, …, true].</source>
            <target state="new">To avoid this situation, define <bpt id="2">&lt;strong&gt;</bpt>Padding<ept id="2">&lt;/strong&gt;</ept> as [true, true, …, true].</target>
          </trans-unit>
          <trans-unit id="307" xml:space="preserve">
            <source>In addition to the four attributes described earlier, response normalization bundles also support the following attributes:</source>
            <target state="new">In addition to the four attributes described earlier, response normalization bundles also support the following attributes:</target>
          </trans-unit>
          <trans-unit id="308" xml:space="preserve">
            <source><bpt id="1">&lt;strong&gt;</bpt>Alpha<ept id="1">&lt;/strong&gt;</ept>: (required) Specifies a floating-point value that corresponds to <bpt id="3">&lt;strong&gt;</bpt>*α*<ept id="3">&lt;/strong&gt;</ept> in the previous formula.</source>
            <target state="new"><bpt id="1">&lt;strong&gt;</bpt>Alpha<ept id="1">&lt;/strong&gt;</ept>: (required) Specifies a floating-point value that corresponds to <bpt id="3">&lt;strong&gt;</bpt>*α*<ept id="3">&lt;/strong&gt;</ept> in the previous formula.</target>
          </trans-unit>
          <trans-unit id="309" xml:space="preserve">
            <source><bpt id="1">&lt;strong&gt;</bpt>Beta<ept id="1">&lt;/strong&gt;</ept>: (required) Specifies a floating-point value that corresponds to <bpt id="3">&lt;strong&gt;</bpt>*β*<ept id="3">&lt;/strong&gt;</ept> in the previous formula.</source>
            <target state="new"><bpt id="1">&lt;strong&gt;</bpt>Beta<ept id="1">&lt;/strong&gt;</ept>: (required) Specifies a floating-point value that corresponds to <bpt id="3">&lt;strong&gt;</bpt>*β*<ept id="3">&lt;/strong&gt;</ept> in the previous formula.</target>
          </trans-unit>
          <trans-unit id="310" xml:space="preserve">
            <source><bpt id="1">&lt;strong&gt;</bpt>Offset<ept id="1">&lt;/strong&gt;</ept>: (optional) Specifies a floating-point value that corresponds to <bpt id="3">&lt;strong&gt;</bpt>*k*<ept id="3">&lt;/strong&gt;</ept> in the previous formula.</source>
            <target state="new"><bpt id="1">&lt;strong&gt;</bpt>Offset<ept id="1">&lt;/strong&gt;</ept>: (optional) Specifies a floating-point value that corresponds to <bpt id="3">&lt;strong&gt;</bpt>*k*<ept id="3">&lt;/strong&gt;</ept> in the previous formula.</target>
          </trans-unit>
          <trans-unit id="311" xml:space="preserve">
            <source>It defaults to 1.</source>
            <target state="new">It defaults to 1.</target>
          </trans-unit>
          <trans-unit id="312" xml:space="preserve">
            <source>The following example defines a response normalization bundle using these attributes:</source>
            <target state="new">The following example defines a response normalization bundle using these attributes:</target>
          </trans-unit>
          <trans-unit id="313" xml:space="preserve">
            <source>The source layer includes five maps, each with aof dimension of 12x12, totaling in 1440 nodes.</source>
            <target state="new">The source layer includes five maps, each with aof dimension of 12x12, totaling in 1440 nodes.</target>
          </trans-unit>
          <trans-unit id="314" xml:space="preserve">
            <source>The value of <bpt id="2">&lt;strong&gt;</bpt>KernelShape<ept id="2">&lt;/strong&gt;</ept> indicates that this is a same map normalization layer, where the neighborhood is a 3x3 rectangle.</source>
            <target state="new">The value of <bpt id="2">&lt;strong&gt;</bpt>KernelShape<ept id="2">&lt;/strong&gt;</ept> indicates that this is a same map normalization layer, where the neighborhood is a 3x3 rectangle.</target>
          </trans-unit>
          <trans-unit id="315" xml:space="preserve">
            <source>The default value of <bpt id="2">&lt;strong&gt;</bpt>Padding<ept id="2">&lt;/strong&gt;</ept> is False, thus the destination layer has only 10 nodes in each dimension.</source>
            <target state="new">The default value of <bpt id="2">&lt;strong&gt;</bpt>Padding<ept id="2">&lt;/strong&gt;</ept> is False, thus the destination layer has only 10 nodes in each dimension.</target>
          </trans-unit>
          <trans-unit id="316" xml:space="preserve">
            <source>To include one node in the destination layer that corresponds to every node in the source layer, add Padding = [true, true, true]; and change the size of RN1 to [5, 12, 12].</source>
            <target state="new">To include one node in the destination layer that corresponds to every node in the source layer, add Padding = [true, true, true]; and change the size of RN1 to [5, 12, 12].</target>
          </trans-unit>
          <trans-unit id="317" xml:space="preserve">
            <source>Share declaration</source>
            <target state="new">Share declaration</target>
          </trans-unit>
          <trans-unit id="318" xml:space="preserve">
            <source>Net# optionally supports defining multiple bundles with shared weights.</source>
            <target state="new">Net# optionally supports defining multiple bundles with shared weights.</target>
          </trans-unit>
          <trans-unit id="319" xml:space="preserve">
            <source>The weights of any two bundles can be shared if their structures are the same.</source>
            <target state="new">The weights of any two bundles can be shared if their structures are the same.</target>
          </trans-unit>
          <trans-unit id="320" xml:space="preserve">
            <source>The following syntax defines bundles with shared weights:</source>
            <target state="new">The following syntax defines bundles with shared weights:</target>
          </trans-unit>
          <trans-unit id="321" xml:space="preserve">
            <source>For example, the following share-declaration specifies the layer names, indicating that both weights and biases should be shared:</source>
            <target state="new">For example, the following share-declaration specifies the layer names, indicating that both weights and biases should be shared:</target>
          </trans-unit>
          <trans-unit id="322" xml:space="preserve">
            <source>The input features are partitioned into two equal sized input layers.</source>
            <target state="new">The input features are partitioned into two equal sized input layers.</target>
          </trans-unit>
          <trans-unit id="323" xml:space="preserve">
            <source>The hidden layers then compute higher level features on the two input layers.</source>
            <target state="new">The hidden layers then compute higher level features on the two input layers.</target>
          </trans-unit>
          <trans-unit id="324" xml:space="preserve">
            <source>The share-declaration specifies that H1 and H2 must be computed in the same way from their respective inputs.</source>
            <target state="new">The share-declaration specifies that H1 and H2 must be computed in the same way from their respective inputs.</target>
          </trans-unit>
          <trans-unit id="325" xml:space="preserve">
            <source>Alternatively, this could be specified with two separate share-declarations as follows:</source>
            <target state="new">Alternatively, this could be specified with two separate share-declarations as follows:</target>
          </trans-unit>
          <trans-unit id="326" xml:space="preserve">
            <source>You can use the short form only when the layers contain a single bundle.</source>
            <target state="new">You can use the short form only when the layers contain a single bundle.</target>
          </trans-unit>
          <trans-unit id="327" xml:space="preserve">
            <source>In general, sharing is possible only when the relevant structure is identical, meaning that they have the same size, same convolutional geometry, and so forth.</source>
            <target state="new">In general, sharing is possible only when the relevant structure is identical, meaning that they have the same size, same convolutional geometry, and so forth.</target>
          </trans-unit>
          <trans-unit id="328" xml:space="preserve">
            <source>Examples of Net# usage</source>
            <target state="new">Examples of Net# usage</target>
          </trans-unit>
          <trans-unit id="329" xml:space="preserve">
            <source>This section provides some examples of how you can use Net# to add hidden layers, define the way that hidden layers interact with other layers, and build convolutional networks.</source>
            <target state="new">This section provides some examples of how you can use Net# to add hidden layers, define the way that hidden layers interact with other layers, and build convolutional networks.</target>
          </trans-unit>
          <trans-unit id="330" xml:space="preserve">
            <source>Define a simple custom neural network: "Hello World" example</source>
            <target state="new">Define a simple custom neural network: "Hello World" example</target>
          </trans-unit>
          <trans-unit id="331" xml:space="preserve">
            <source>This simple example demonstrates how to create a neural network model that has a single hidden layer.</source>
            <target state="new">This simple example demonstrates how to create a neural network model that has a single hidden layer.</target>
          </trans-unit>
          <trans-unit id="332" xml:space="preserve">
            <source>The example illustrates some basic commands as follows:</source>
            <target state="new">The example illustrates some basic commands as follows:</target>
          </trans-unit>
          <trans-unit id="333" xml:space="preserve">
            <source>The first line defines the input layer (named Data), which has 100 nodes, and each node represents a feature in the input examples.</source>
            <target state="new">The first line defines the input layer (named Data), which has 100 nodes, and each node represents a feature in the input examples.</target>
          </trans-unit>
          <trans-unit id="334" xml:space="preserve">
            <source>The second line creates the hidden layer.</source>
            <target state="new">The second line creates the hidden layer.</target>
          </trans-unit>
          <trans-unit id="335" xml:space="preserve">
            <source>The name H is assigned to the hidden layer, which has 200 nodes.</source>
            <target state="new">The name H is assigned to the hidden layer, which has 200 nodes.</target>
          </trans-unit>
          <trans-unit id="336" xml:space="preserve">
            <source>This layer is fully connected to the input layer.</source>
            <target state="new">This layer is fully connected to the input layer.</target>
          </trans-unit>
          <trans-unit id="337" xml:space="preserve">
            <source>The third line defines the output layer (named O), and it contains 10 output nodes.</source>
            <target state="new">The third line defines the output layer (named O), and it contains 10 output nodes.</target>
          </trans-unit>
          <trans-unit id="338" xml:space="preserve">
            <source>For classification neural networks, there is one output node per class.</source>
            <target state="new">For classification neural networks, there is one output node per class.</target>
          </trans-unit>
          <trans-unit id="339" xml:space="preserve">
            <source>The keyword <bpt id="2">&lt;strong&gt;</bpt>sigmoid<ept id="2">&lt;/strong&gt;</ept> indicates that the output function is applied to the output layer.</source>
            <target state="new">The keyword <bpt id="2">&lt;strong&gt;</bpt>sigmoid<ept id="2">&lt;/strong&gt;</ept> indicates that the output function is applied to the output layer.</target>
          </trans-unit>
          <trans-unit id="340" xml:space="preserve">
            <source>Define multiple hidden layers: computer vision example</source>
            <target state="new">Define multiple hidden layers: computer vision example</target>
          </trans-unit>
          <trans-unit id="341" xml:space="preserve">
            <source>The following example demonstrates how to define a slightly more complex neural network, with multiple custom hidden layers.</source>
            <target state="new">The following example demonstrates how to define a slightly more complex neural network, with multiple custom hidden layers.</target>
          </trans-unit>
          <trans-unit id="342" xml:space="preserve">
            <source>This example illustrates several features of the neural networks specification language:</source>
            <target state="new">This example illustrates several features of the neural networks specification language:</target>
          </trans-unit>
          <trans-unit id="343" xml:space="preserve">
            <source>The structure has two input layers, Pixels and MetaData.</source>
            <target state="new">The structure has two input layers, Pixels and MetaData.</target>
          </trans-unit>
          <trans-unit id="344" xml:space="preserve">
            <source>The Pixels layer is a source layer for two connection bundles, with destination layers, ByRow and ByCol.</source>
            <target state="new">The Pixels layer is a source layer for two connection bundles, with destination layers, ByRow and ByCol.</target>
          </trans-unit>
          <trans-unit id="345" xml:space="preserve">
            <source>The layers, Gather and Result, are destination layers in multiple connection bundles.</source>
            <target state="new">The layers, Gather and Result, are destination layers in multiple connection bundles.</target>
          </trans-unit>
          <trans-unit id="346" xml:space="preserve">
            <source>The output layer, Result, is a destination layer in two connection bundles; one with the second level hidden (Gather) as a destination layer, and the other with the input layer (MetaData) as a destination layer.</source>
            <target state="new">The output layer, Result, is a destination layer in two connection bundles; one with the second level hidden (Gather) as a destination layer, and the other with the input layer (MetaData) as a destination layer.</target>
          </trans-unit>
          <trans-unit id="347" xml:space="preserve">
            <source>The hidden layers, ByRow and ByCol, specify filtered connectivity by using predicate expressions.</source>
            <target state="new">The hidden layers, ByRow and ByCol, specify filtered connectivity by using predicate expressions.</target>
          </trans-unit>
          <trans-unit id="348" xml:space="preserve">
            <source>More precisely, the node in ByRow at [x, y] is connected to those nodes in Pixels by having the first index coordinate equal to the node's first coordinate, x.</source>
            <target state="new">More precisely, the node in ByRow at [x, y] is connected to those nodes in Pixels by having the first index coordinate equal to the node's first coordinate, x.</target>
          </trans-unit>
          <trans-unit id="349" xml:space="preserve">
            <source>Si</source>
            <target state="new">Si</target>
          </trans-unit>
          <trans-unit id="350" xml:space="preserve">
            <source>milarly, the node in ByCol at [x, y] is connected to those nodes in Pixels by having the second index coordinate within one of the node's second coordinate, y.</source>
            <target state="new">milarly, the node in ByCol at [x, y] is connected to those nodes in Pixels by having the second index coordinate within one of the node's second coordinate, y.</target>
          </trans-unit>
          <trans-unit id="351" xml:space="preserve">
            <source>Define a convolutional network for multiclass classification: digit recognition example</source>
            <target state="new">Define a convolutional network for multiclass classification: digit recognition example</target>
          </trans-unit>
          <trans-unit id="352" xml:space="preserve">
            <source>The definition of the following network is designed to recognize numbers, and it illustrates some advanced techniques for customizing a neural network.</source>
            <target state="new">The definition of the following network is designed to recognize numbers, and it illustrates some advanced techniques for customizing a neural network.</target>
          </trans-unit>
          <trans-unit id="353" xml:space="preserve">
            <source>The structure has a single input layer, Image.</source>
            <target state="new">The structure has a single input layer, Image.</target>
          </trans-unit>
          <trans-unit id="354" xml:space="preserve">
            <source>The keyword <bpt id="2">&lt;strong&gt;</bpt>convolve<ept id="2">&lt;/strong&gt;</ept> indicates that Conv1 and Conv2 are convolutional layers.</source>
            <target state="new">The keyword <bpt id="2">&lt;strong&gt;</bpt>convolve<ept id="2">&lt;/strong&gt;</ept> indicates that Conv1 and Conv2 are convolutional layers.</target>
          </trans-unit>
          <trans-unit id="355" xml:space="preserve">
            <source>Each of these layer declarations is followed by a list of the convolution attributes.</source>
            <target state="new">Each of these layer declarations is followed by a list of the convolution attributes.</target>
          </trans-unit>
          <trans-unit id="356" xml:space="preserve">
            <source>The net has a third hidden layer, Hid3, which is fully connected to the second hidden layer, Conv2.</source>
            <target state="new">The net has a third hidden layer, Hid3, which is fully connected to the second hidden layer, Conv2.</target>
          </trans-unit>
          <trans-unit id="357" xml:space="preserve">
            <source>The output layer, Digit, is connected only to the third hidden layer, Hid3.</source>
            <target state="new">The output layer, Digit, is connected only to the third hidden layer, Hid3.</target>
          </trans-unit>
          <trans-unit id="358" xml:space="preserve">
            <source>The keyword <bpt id="2">&lt;strong&gt;</bpt>all<ept id="2">&lt;/strong&gt;</ept> indicates that the output layer is fully connected to Hid3.</source>
            <target state="new">The keyword <bpt id="2">&lt;strong&gt;</bpt>all<ept id="2">&lt;/strong&gt;</ept> indicates that the output layer is fully connected to Hid3.</target>
          </trans-unit>
          <trans-unit id="359" xml:space="preserve">
            <source>The arity of the convolution is three (the length of the tuples <bpt id="2">&lt;strong&gt;</bpt>InputShape<ept id="2">&lt;/strong&gt;</ept>, <bpt id="4">&lt;strong&gt;</bpt>KernelShape<ept id="4">&lt;/strong&gt;</ept>, <bpt id="6">&lt;strong&gt;</bpt>Stride<ept id="6">&lt;/strong&gt;</ept>, and <bpt id="8">&lt;strong&gt;</bpt>Sharing<ept id="8">&lt;/strong&gt;</ept>).</source>
            <target state="new">The arity of the convolution is three (the length of the tuples <bpt id="2">&lt;strong&gt;</bpt>InputShape<ept id="2">&lt;/strong&gt;</ept>, <bpt id="4">&lt;strong&gt;</bpt>KernelShape<ept id="4">&lt;/strong&gt;</ept>, <bpt id="6">&lt;strong&gt;</bpt>Stride<ept id="6">&lt;/strong&gt;</ept>, and <bpt id="8">&lt;strong&gt;</bpt>Sharing<ept id="8">&lt;/strong&gt;</ept>).</target>
          </trans-unit>
          <trans-unit id="360" xml:space="preserve">
            <source>The number of weights per kernel is _1 + <bpt id="2">&lt;strong&gt;</bpt>KernelShape<ept id="2">&lt;/strong&gt;</ept>\[0] * <bpt id="4">&lt;strong&gt;</bpt>KernelShape<ept id="4">&lt;/strong&gt;</ept>\[1] * <bpt id="6">&lt;strong&gt;</bpt>KernelShape<ept id="6">&lt;/strong&gt;</ept>\[2] = 1 + 1 * 5 * 5 = 26.</source>
            <target state="new">The number of weights per kernel is _1 + <bpt id="2">&lt;strong&gt;</bpt>KernelShape<ept id="2">&lt;/strong&gt;</ept>\[0] * <bpt id="4">&lt;strong&gt;</bpt>KernelShape<ept id="4">&lt;/strong&gt;</ept>\[1] * <bpt id="6">&lt;strong&gt;</bpt>KernelShape<ept id="6">&lt;/strong&gt;</ept>\[2] = 1 + 1 * 5 * 5 = 26.</target>
          </trans-unit>
          <trans-unit id="361" xml:space="preserve">
            <source>Or 26 * 50 = 1300_.</source>
            <target state="new">Or 26 * 50 = 1300_.</target>
          </trans-unit>
          <trans-unit id="362" xml:space="preserve">
            <source>You can calculate the nodes in each hidden layer as follows:</source>
            <target state="new">You can calculate the nodes in each hidden layer as follows:</target>
          </trans-unit>
          <trans-unit id="363" xml:space="preserve">
            <source><bpt id="1">&lt;strong&gt;</bpt>NodeCount<ept id="1">&lt;/strong&gt;</ept>\[0] = (5 - 1) / 1 + 1 = 5.</source>
            <target state="new"><bpt id="1">&lt;strong&gt;</bpt>NodeCount<ept id="1">&lt;/strong&gt;</ept>\[0] = (5 - 1) / 1 + 1 = 5.</target>
          </trans-unit>
          <trans-unit id="364" xml:space="preserve">
            <source><bpt id="1">&lt;strong&gt;</bpt>NodeCount<ept id="1">&lt;/strong&gt;</ept>\[1] = (13 - 5) / 2 + 1 = 5.</source>
            <target state="new"><bpt id="1">&lt;strong&gt;</bpt>NodeCount<ept id="1">&lt;/strong&gt;</ept>\[1] = (13 - 5) / 2 + 1 = 5.</target>
          </trans-unit>
          <trans-unit id="365" xml:space="preserve">
            <source><bpt id="1">&lt;strong&gt;</bpt>NodeCount<ept id="1">&lt;/strong&gt;</ept>\[2] = (13 - 5) / 2 + 1 = 5.</source>
            <target state="new"><bpt id="1">&lt;strong&gt;</bpt>NodeCount<ept id="1">&lt;/strong&gt;</ept>\[2] = (13 - 5) / 2 + 1 = 5.</target>
          </trans-unit>
          <trans-unit id="366" xml:space="preserve">
            <source>The total number of nodes can be calculated by using the declared dimensionality of the layer, [50, 5, 5], as follows: _<bpt id="2">&lt;strong&gt;</bpt>MapCount<ept id="2">&lt;/strong&gt;</ept> * <bpt id="4">&lt;strong&gt;</bpt>NodeCount<ept id="4">&lt;/strong&gt;</ept>\[0] * <bpt id="6">&lt;strong&gt;</bpt>NodeCount<ept id="6">&lt;/strong&gt;</ept>\[1] * <bpt id="8">&lt;strong&gt;</bpt>NodeCount<ept id="8">&lt;/strong&gt;</ept>\[2] = 10 * 5 * 5 * 5_</source>
            <target state="new">The total number of nodes can be calculated by using the declared dimensionality of the layer, [50, 5, 5], as follows: _<bpt id="2">&lt;strong&gt;</bpt>MapCount<ept id="2">&lt;/strong&gt;</ept> * <bpt id="4">&lt;strong&gt;</bpt>NodeCount<ept id="4">&lt;/strong&gt;</ept>\[0] * <bpt id="6">&lt;strong&gt;</bpt>NodeCount<ept id="6">&lt;/strong&gt;</ept>\[1] * <bpt id="8">&lt;strong&gt;</bpt>NodeCount<ept id="8">&lt;/strong&gt;</ept>\[2] = 10 * 5 * 5 * 5_</target>
          </trans-unit>
          <trans-unit id="367" xml:space="preserve">
            <source>Because <bpt id="2">&lt;strong&gt;</bpt>Sharing<ept id="2">&lt;/strong&gt;</ept>[d] is False only for _d == 0_, the number of kernels is _<bpt id="4">&lt;strong&gt;</bpt>MapCount<ept id="4">&lt;/strong&gt;</ept> * <bpt id="6">&lt;strong&gt;</bpt>NodeCount<ept id="6">&lt;/strong&gt;</ept>\[0] = 10 * 5 = 50_.</source>
            <target state="new">Because <bpt id="2">&lt;strong&gt;</bpt>Sharing<ept id="2">&lt;/strong&gt;</ept>[d] is False only for _d == 0_, the number of kernels is _<bpt id="4">&lt;strong&gt;</bpt>MapCount<ept id="4">&lt;/strong&gt;</ept> * <bpt id="6">&lt;strong&gt;</bpt>NodeCount<ept id="6">&lt;/strong&gt;</ept>\[0] = 10 * 5 = 50_.</target>
          </trans-unit>
        </group>
      </group>
    </body>
  </file>
</xliff>