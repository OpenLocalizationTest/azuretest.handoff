<?xml version="1.0" encoding="utf-8"?>
<xliff version="1.2" xmlns="urn:oasis:names:tc:xliff:document:1.2" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="urn:oasis:names:tc:xliff:document:1.2 xliff-core-1.2-transitional.xsd">
  <file datatype="xml" source-language="en-us" target-language="ja-jp" original="2/20/2016 9:30:45 AM" tool-id="MarkdownTransformer" product-name="N/A" product-version="N/A" build-num="1">
    <header>
      <xliffext:olfilehash xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">2e1092ad908b147e40335291b550c65deeee309d</xliffext:olfilehash>
      <tool tool-id="MarkdownTransformer" tool-name="MarkdownToXliff" tool-version="1.0" tool-company="Microsoft" />
    </header>
    <body>
      <group extype="content">
        <group id="101">
          <trans-unit id="101" xml:space="preserve">
            <source>Migrate your data to SQL Data Warehouse | Microsoft Azure</source>
            <target state="new">Migrate your data to SQL Data Warehouse | Microsoft Azure</target>
          </trans-unit>
          <trans-unit id="102" xml:space="preserve">
            <source>Tips for migrating your data to Azure SQL Data Warehouse for developing solutions.</source>
            <target state="new">Tips for migrating your data to Azure SQL Data Warehouse for developing solutions.</target>
          </trans-unit>
          <trans-unit id="103" xml:space="preserve">
            <source>Migrate Your Data</source>
            <target state="new">Migrate Your Data</target>
          </trans-unit>
          <trans-unit id="104" xml:space="preserve">
            <source>The primary objective when migrating data is to populate your SQLDW database.</source>
            <target state="new">The primary objective when migrating data is to populate your SQLDW database.</target>
          </trans-unit>
          <trans-unit id="105" xml:space="preserve">
            <source>This process can be achieved in a number of ways.</source>
            <target state="new">This process can be achieved in a number of ways.</target>
          </trans-unit>
          <trans-unit id="106" xml:space="preserve">
            <source>ADF Copy, SSIS and bcp can all be used to achieve this goal.</source>
            <target state="new">ADF Copy, SSIS and bcp can all be used to achieve this goal.</target>
          </trans-unit>
          <trans-unit id="107" xml:space="preserve">
            <source>However, as the amount of data increases you should think about breaking down the data migration process into steps.</source>
            <target state="new">However, as the amount of data increases you should think about breaking down the data migration process into steps.</target>
          </trans-unit>
          <trans-unit id="108" xml:space="preserve">
            <source>This affords you the opportunity to optimize each step both for performance and for resilience to ensure a smooth data migration.</source>
            <target state="new">This affords you the opportunity to optimize each step both for performance and for resilience to ensure a smooth data migration.</target>
          </trans-unit>
          <trans-unit id="109" xml:space="preserve">
            <source>This article firstly discusses the simple migration scenarios of ADF Copy, SSIS and bcp.</source>
            <target state="new">This article firstly discusses the simple migration scenarios of ADF Copy, SSIS and bcp.</target>
          </trans-unit>
          <trans-unit id="110" xml:space="preserve">
            <source>It then look a little deeper into how the migration can be optimized.</source>
            <target state="new">It then look a little deeper into how the migration can be optimized.</target>
          </trans-unit>
          <trans-unit id="111" xml:space="preserve">
            <source>Azure Data Factory (ADF) copy</source>
            <target state="new">Azure Data Factory (ADF) copy</target>
          </trans-unit>
          <trans-unit id="112" xml:space="preserve">
            <source><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>ADF Copy[]<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept> is part of <bpt id="3CapsExtId1">&lt;link&gt;</bpt><bpt id="3CapsExtId2">&lt;linkText&gt;</bpt>Azure Data Factory[]<ept id="3CapsExtId2">&lt;/linkText&gt;</ept><bpt id="3CapsExtId3">&lt;title&gt;</bpt><ept id="3CapsExtId3">&lt;/title&gt;</ept><ept id="3CapsExtId1">&lt;/link&gt;</ept>.</source>
            <target state="new"><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>ADF Copy[]<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept> is part of <bpt id="3CapsExtId1">&lt;link&gt;</bpt><bpt id="3CapsExtId2">&lt;linkText&gt;</bpt>Azure Data Factory[]<ept id="3CapsExtId2">&lt;/linkText&gt;</ept><bpt id="3CapsExtId3">&lt;title&gt;</bpt><ept id="3CapsExtId3">&lt;/title&gt;</ept><ept id="3CapsExtId1">&lt;/link&gt;</ept>.</target>
          </trans-unit>
          <trans-unit id="113" xml:space="preserve">
            <source>You can use ADF Copy to export your data to flat files residing on local storage, to remote flat files held in Azure blob storage or directly into SQL Data Warehouse.</source>
            <target state="new">You can use ADF Copy to export your data to flat files residing on local storage, to remote flat files held in Azure blob storage or directly into SQL Data Warehouse.</target>
          </trans-unit>
          <trans-unit id="114" xml:space="preserve">
            <source>If your data starts in flat files then you will first need to transfer it to Azure storage blob before initiating a load it into SQL Data Warehouse.</source>
            <target state="new">If your data starts in flat files then you will first need to transfer it to Azure storage blob before initiating a load it into SQL Data Warehouse.</target>
          </trans-unit>
          <trans-unit id="115" xml:space="preserve">
            <source>Once the data is transferred into Azure blob storage you can choose to use <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>ADF Copy[]<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept> again to push the data into SQL Data Warehouse.</source>
            <target state="new">Once the data is transferred into Azure blob storage you can choose to use <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>ADF Copy[]<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept> again to push the data into SQL Data Warehouse.</target>
          </trans-unit>
          <trans-unit id="116" xml:space="preserve">
            <source>PolyBase also provides a very high performance option for loading the data.</source>
            <target state="new">PolyBase also provides a very high performance option for loading the data.</target>
          </trans-unit>
          <trans-unit id="117" xml:space="preserve">
            <source>However, that does mean using two tools instead of one.</source>
            <target state="new">However, that does mean using two tools instead of one.</target>
          </trans-unit>
          <trans-unit id="118" xml:space="preserve">
            <source>If you need the best performance then use PolyBase.</source>
            <target state="new">If you need the best performance then use PolyBase.</target>
          </trans-unit>
          <trans-unit id="119" xml:space="preserve">
            <source>If you want a single tool experience (and the data is not massive) then ADF is your answer.</source>
            <target state="new">If you want a single tool experience (and the data is not massive) then ADF is your answer.</target>
          </trans-unit>
          <trans-unit id="120" xml:space="preserve">
            <source>PolyBase requires your data files to be in UTF-8.</source>
            <target state="new">PolyBase requires your data files to be in UTF-8.</target>
          </trans-unit>
          <trans-unit id="121" xml:space="preserve">
            <source>This is ADF Copy's default encoding so there is nothing to change.</source>
            <target state="new">This is ADF Copy's default encoding so there is nothing to change.</target>
          </trans-unit>
          <trans-unit id="122" xml:space="preserve">
            <source>This is just a reminder to not change the default behavior of ADF Copy.</source>
            <target state="new">This is just a reminder to not change the default behavior of ADF Copy.</target>
          </trans-unit>
          <trans-unit id="123" xml:space="preserve">
            <source>Head over to the following article for some great [ADF Copy examples].</source>
            <target state="new">Head over to the following article for some great [ADF Copy examples].</target>
          </trans-unit>
          <trans-unit id="124" xml:space="preserve">
            <source> Integration Services</source>
            <target state="new"> Integration Services</target>
          </trans-unit>
          <trans-unit id="125" xml:space="preserve">
            <source>Integration Services (SSIS) is a powerful and flexible Extract Transform and Load (ETL) tool that supports complex workflows, data transformation, and several data loading options.</source>
            <target state="new">Integration Services (SSIS) is a powerful and flexible Extract Transform and Load (ETL) tool that supports complex workflows, data transformation, and several data loading options.</target>
          </trans-unit>
          <trans-unit id="126" xml:space="preserve">
            <source>Use SSIS to simply transfer data to Azure or as part of a broader migration.</source>
            <target state="new">Use SSIS to simply transfer data to Azure or as part of a broader migration.</target>
          </trans-unit>
          <trans-unit id="127" xml:space="preserve">
            <source>SSIS can export to UTF-8 without the byte order mark in the file.</source>
            <target state="new">SSIS can export to UTF-8 without the byte order mark in the file.</target>
          </trans-unit>
          <trans-unit id="128" xml:space="preserve">
            <source>To configure this you must first use the derived column component to convert the character data in the data flow to use the 65001 UTF-8 code page.</source>
            <target state="new">To configure this you must first use the derived column component to convert the character data in the data flow to use the 65001 UTF-8 code page.</target>
          </trans-unit>
          <trans-unit id="129" xml:space="preserve">
            <source>Once the columns have been converted, write the data to the flat file destination adapter ensuring that 65001 has also been selected as the code page for the file.</source>
            <target state="new">Once the columns have been converted, write the data to the flat file destination adapter ensuring that 65001 has also been selected as the code page for the file.</target>
          </trans-unit>
          <trans-unit id="130" xml:space="preserve">
            <source>SSIS connects to SQL Data Warehouse just as it would connect to a SQL Server deployment.</source>
            <target state="new">SSIS connects to SQL Data Warehouse just as it would connect to a SQL Server deployment.</target>
          </trans-unit>
          <trans-unit id="131" xml:space="preserve">
            <source>However, your connections will need to be using an ADO.NET connection manager.</source>
            <target state="new">However, your connections will need to be using an ADO.NET connection manager.</target>
          </trans-unit>
          <trans-unit id="132" xml:space="preserve">
            <source>You should also take care to configure the "Use bulk insert when available" setting to maximize throughput.</source>
            <target state="new">You should also take care to configure the "Use bulk insert when available" setting to maximize throughput.</target>
          </trans-unit>
          <trans-unit id="133" xml:space="preserve">
            <source>Please refer to the <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>ADO.NET destination adapter[]<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept> article to learn more about this property</source>
            <target state="new">Please refer to the <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>ADO.NET destination adapter[]<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept> article to learn more about this property</target>
          </trans-unit>
          <trans-unit id="134" xml:space="preserve">
            <source>Connecting to Azure SQL Data Warehouse by using OLEDB is not supported.</source>
            <target state="new">Connecting to Azure SQL Data Warehouse by using OLEDB is not supported.</target>
          </trans-unit>
          <trans-unit id="135" xml:space="preserve">
            <source>In addition, there is always the possibility that a package might fail due to throttling or network issues.</source>
            <target state="new">In addition, there is always the possibility that a package might fail due to throttling or network issues.</target>
          </trans-unit>
          <trans-unit id="136" xml:space="preserve">
            <source>Design packages so they can be resumed at the point of failure, without redoing work that completed before the failure.</source>
            <target state="new">Design packages so they can be resumed at the point of failure, without redoing work that completed before the failure.</target>
          </trans-unit>
          <trans-unit id="137" xml:space="preserve">
            <source>For more information consult the <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>SSIS documentation[]<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept>.</source>
            <target state="new">For more information consult the <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>SSIS documentation[]<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept>.</target>
          </trans-unit>
          <trans-unit id="138" xml:space="preserve">
            <source>bcp</source>
            <target state="new">bcp</target>
          </trans-unit>
          <trans-unit id="139" xml:space="preserve">
            <source>bcp is a command-line utility that is designed for flat file data import and export.</source>
            <target state="new">bcp is a command-line utility that is designed for flat file data import and export.</target>
          </trans-unit>
          <trans-unit id="140" xml:space="preserve">
            <source>Some transformation can can take place during data export.</source>
            <target state="new">Some transformation can can take place during data export.</target>
          </trans-unit>
          <trans-unit id="141" xml:space="preserve">
            <source>To perform simple transformations use a query to select and transform the data.</source>
            <target state="new">To perform simple transformations use a query to select and transform the data.</target>
          </trans-unit>
          <trans-unit id="142" xml:space="preserve">
            <source>Once exported, the flat files can then be loaded directly into the target the SQL Data Warehouse database.</source>
            <target state="new">Once exported, the flat files can then be loaded directly into the target the SQL Data Warehouse database.</target>
          </trans-unit>
          <trans-unit id="143" xml:space="preserve">
            <source>It is often a good idea to encapsulate the transformations used during data export in a view on the source system.</source>
            <target state="new">It is often a good idea to encapsulate the transformations used during data export in a view on the source system.</target>
          </trans-unit>
          <trans-unit id="144" xml:space="preserve">
            <source>This ensures that the logic is retained and the process is repeatable.</source>
            <target state="new">This ensures that the logic is retained and the process is repeatable.</target>
          </trans-unit>
          <trans-unit id="145" xml:space="preserve">
            <source>Advantages of bcp are:</source>
            <target state="new">Advantages of bcp are:</target>
          </trans-unit>
          <trans-unit id="146" xml:space="preserve">
            <source>Simplicity.</source>
            <target state="new">Simplicity.</target>
          </trans-unit>
          <trans-unit id="147" xml:space="preserve">
            <source>bcp commands are simple to build and execute</source>
            <target state="new">bcp commands are simple to build and execute</target>
          </trans-unit>
          <trans-unit id="148" xml:space="preserve">
            <source>Re-startable load process.</source>
            <target state="new">Re-startable load process.</target>
          </trans-unit>
          <trans-unit id="149" xml:space="preserve">
            <source>Once exported the load can be executed any number of times</source>
            <target state="new">Once exported the load can be executed any number of times</target>
          </trans-unit>
          <trans-unit id="150" xml:space="preserve">
            <source>Limitations of bcp are:</source>
            <target state="new">Limitations of bcp are:</target>
          </trans-unit>
          <trans-unit id="151" xml:space="preserve">
            <source>bcp works with tabulated flat files only.</source>
            <target state="new">bcp works with tabulated flat files only.</target>
          </trans-unit>
          <trans-unit id="152" xml:space="preserve">
            <source>It does not work with files such as xml or JSON</source>
            <target state="new">It does not work with files such as xml or JSON</target>
          </trans-unit>
          <trans-unit id="153" xml:space="preserve">
            <source>bcp does not support exporting to UTF-8.</source>
            <target state="new">bcp does not support exporting to UTF-8.</target>
          </trans-unit>
          <trans-unit id="154" xml:space="preserve">
            <source>This may prevent using PolyBase on bcp exported data</source>
            <target state="new">This may prevent using PolyBase on bcp exported data</target>
          </trans-unit>
          <trans-unit id="155" xml:space="preserve">
            <source>Data transformation capabilities are limited to the export stage only and are simple in nature</source>
            <target state="new">Data transformation capabilities are limited to the export stage only and are simple in nature</target>
          </trans-unit>
          <trans-unit id="156" xml:space="preserve">
            <source>bcp has not been adapted to be robust when loading data over the internet.</source>
            <target state="new">bcp has not been adapted to be robust when loading data over the internet.</target>
          </trans-unit>
          <trans-unit id="157" xml:space="preserve">
            <source>Any network instability may cause a load error.</source>
            <target state="new">Any network instability may cause a load error.</target>
          </trans-unit>
          <trans-unit id="158" xml:space="preserve">
            <source>bcp relies on the schema being present in the target database prior to the load</source>
            <target state="new">bcp relies on the schema being present in the target database prior to the load</target>
          </trans-unit>
          <trans-unit id="159" xml:space="preserve">
            <source>For more information, see <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>Use bcp to load data into SQL Data Warehouse[]<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept>.</source>
            <target state="new">For more information, see <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>Use bcp to load data into SQL Data Warehouse[]<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept>.</target>
          </trans-unit>
          <trans-unit id="160" xml:space="preserve">
            <source>Optimizing data migration</source>
            <target state="new">Optimizing data migration</target>
          </trans-unit>
          <trans-unit id="161" xml:space="preserve">
            <source>A SQLDW data migration process can be effectively broken down into three discrete steps:</source>
            <target state="new">A SQLDW data migration process can be effectively broken down into three discrete steps:</target>
          </trans-unit>
          <trans-unit id="162" xml:space="preserve">
            <source>Export of source data</source>
            <target state="new">Export of source data</target>
          </trans-unit>
          <trans-unit id="163" xml:space="preserve">
            <source>Transfer of data to Azure</source>
            <target state="new">Transfer of data to Azure</target>
          </trans-unit>
          <trans-unit id="164" xml:space="preserve">
            <source>Load into the target SQLDW database</source>
            <target state="new">Load into the target SQLDW database</target>
          </trans-unit>
          <trans-unit id="165" xml:space="preserve">
            <source>Each step can be individually optimized to create a robust, re-startable and resilient migration process that maximises performance at each step.</source>
            <target state="new">Each step can be individually optimized to create a robust, re-startable and resilient migration process that maximises performance at each step.</target>
          </trans-unit>
          <trans-unit id="166" xml:space="preserve">
            <source>Optimizing data load</source>
            <target state="new">Optimizing data load</target>
          </trans-unit>
          <trans-unit id="167" xml:space="preserve">
            <source>Looking at these in reverse order for a moment; the fastest way to load data is via PolyBase.</source>
            <target state="new">Looking at these in reverse order for a moment; the fastest way to load data is via PolyBase.</target>
          </trans-unit>
          <trans-unit id="168" xml:space="preserve">
            <source>Optimizing for a PolyBase load process places pre-quisites on the preceding steps so it's best to understand this upfront.</source>
            <target state="new">Optimizing for a PolyBase load process places pre-quisites on the preceding steps so it's best to understand this upfront.</target>
          </trans-unit>
          <trans-unit id="169" xml:space="preserve">
            <source>They are:</source>
            <target state="new">They are:</target>
          </trans-unit>
          <trans-unit id="170" xml:space="preserve">
            <source>Encoding of data files</source>
            <target state="new">Encoding of data files</target>
          </trans-unit>
          <trans-unit id="171" xml:space="preserve">
            <source>Format of data files</source>
            <target state="new">Format of data files</target>
          </trans-unit>
          <trans-unit id="172" xml:space="preserve">
            <source>Location of data files</source>
            <target state="new">Location of data files</target>
          </trans-unit>
          <trans-unit id="173" xml:space="preserve">
            <source>Encoding</source>
            <target state="new">Encoding</target>
          </trans-unit>
          <trans-unit id="174" xml:space="preserve">
            <source>PolyBase requires data files to be UTF-8 encoded.</source>
            <target state="new">PolyBase requires data files to be UTF-8 encoded.</target>
          </trans-unit>
          <trans-unit id="175" xml:space="preserve">
            <source>This means that when you export your data it must conform to this requirement.</source>
            <target state="new">This means that when you export your data it must conform to this requirement.</target>
          </trans-unit>
          <trans-unit id="176" xml:space="preserve">
            <source>If your data only contains basic ASCII characters (not extended ASCII) then these map directly to the UTF-8 standard and you don't have to worry too much about the encoding.</source>
            <target state="new">If your data only contains basic ASCII characters (not extended ASCII) then these map directly to the UTF-8 standard and you don't have to worry too much about the encoding.</target>
          </trans-unit>
          <trans-unit id="177" xml:space="preserve">
            <source>However, if your data contains any special characters such as umlauts, accents or symbols or your data supports non-latin languages then you will have to ensure that your export files are properly UTF-8 encoded.</source>
            <target state="new">However, if your data contains any special characters such as umlauts, accents or symbols or your data supports non-latin languages then you will have to ensure that your export files are properly UTF-8 encoded.</target>
          </trans-unit>
          <trans-unit id="178" xml:space="preserve">
            <source>bcp does not support exporting data to UTF-8.</source>
            <target state="new">bcp does not support exporting data to UTF-8.</target>
          </trans-unit>
          <trans-unit id="179" xml:space="preserve">
            <source>Therefore your best option is to use either Integration Services or ADF Copy for the data export.</source>
            <target state="new">Therefore your best option is to use either Integration Services or ADF Copy for the data export.</target>
          </trans-unit>
          <trans-unit id="180" xml:space="preserve">
            <source>It is worth pointing out that the UTF-8 byte order mark (BOM) is not required in the data file.</source>
            <target state="new">It is worth pointing out that the UTF-8 byte order mark (BOM) is not required in the data file.</target>
          </trans-unit>
          <trans-unit id="181" xml:space="preserve">
            <source>Any files encoded using UTF-16 will need to be re-written <bpt id="2">&lt;strong&gt;</bpt>*prior*<ept id="2">&lt;/strong&gt;</ept> to the data transfer.</source>
            <target state="new">Any files encoded using UTF-16 will need to be re-written <bpt id="2">&lt;strong&gt;</bpt>*prior*<ept id="2">&lt;/strong&gt;</ept> to the data transfer.</target>
          </trans-unit>
          <trans-unit id="182" xml:space="preserve">
            <source>Format of data files</source>
            <target state="new">Format of data files</target>
          </trans-unit>
          <trans-unit id="183" xml:space="preserve">
            <source>PolyBase mandates a fixed row terminator of \n or newline.</source>
            <target state="new">PolyBase mandates a fixed row terminator of \n or newline.</target>
          </trans-unit>
          <trans-unit id="184" xml:space="preserve">
            <source>Your data files must conform to this standard.</source>
            <target state="new">Your data files must conform to this standard.</target>
          </trans-unit>
          <trans-unit id="185" xml:space="preserve">
            <source>There aren't any restrictions on string or column terminators.</source>
            <target state="new">There aren't any restrictions on string or column terminators.</target>
          </trans-unit>
          <trans-unit id="186" xml:space="preserve">
            <source>You will have to define every column in the file as part of your external table in PolyBase.</source>
            <target state="new">You will have to define every column in the file as part of your external table in PolyBase.</target>
          </trans-unit>
          <trans-unit id="187" xml:space="preserve">
            <source>Make sure that all exported columns are required and that the types conform to the required standards.</source>
            <target state="new">Make sure that all exported columns are required and that the types conform to the required standards.</target>
          </trans-unit>
          <trans-unit id="188" xml:space="preserve">
            <source>Please refer back to the [migrate your schema] article for detail on supported data types.</source>
            <target state="new">Please refer back to the [migrate your schema] article for detail on supported data types.</target>
          </trans-unit>
          <trans-unit id="189" xml:space="preserve">
            <source>Location of data files</source>
            <target state="new">Location of data files</target>
          </trans-unit>
          <trans-unit id="190" xml:space="preserve">
            <source>SQL Data Warehouse uses PolyBase to load data from Azure Blob Storage exclusively.</source>
            <target state="new">SQL Data Warehouse uses PolyBase to load data from Azure Blob Storage exclusively.</target>
          </trans-unit>
          <trans-unit id="191" xml:space="preserve">
            <source>Consequently, the data must have been first transferred into blob storage.</source>
            <target state="new">Consequently, the data must have been first transferred into blob storage.</target>
          </trans-unit>
          <trans-unit id="192" xml:space="preserve">
            <source>Optimizing data transfer</source>
            <target state="new">Optimizing data transfer</target>
          </trans-unit>
          <trans-unit id="193" xml:space="preserve">
            <source>One of the slowest parts of data migration is the transfer of the data to Azure.</source>
            <target state="new">One of the slowest parts of data migration is the transfer of the data to Azure.</target>
          </trans-unit>
          <trans-unit id="194" xml:space="preserve">
            <source>Not only can network bandwidth be an issue but also network reliability can seriously hamper progress.</source>
            <target state="new">Not only can network bandwidth be an issue but also network reliability can seriously hamper progress.</target>
          </trans-unit>
          <trans-unit id="195" xml:space="preserve">
            <source>By default migrating data to Azure is over the internet so the chances of transfer errors occurring are reasonably likely.</source>
            <target state="new">By default migrating data to Azure is over the internet so the chances of transfer errors occurring are reasonably likely.</target>
          </trans-unit>
          <trans-unit id="196" xml:space="preserve">
            <source>However, these errors may require data to be re-sent either in whole or in part.</source>
            <target state="new">However, these errors may require data to be re-sent either in whole or in part.</target>
          </trans-unit>
          <trans-unit id="197" xml:space="preserve">
            <source>Fortunately you have several options to improve the speed and resilience of this process:</source>
            <target state="new">Fortunately you have several options to improve the speed and resilience of this process:</target>
          </trans-unit>
          <trans-unit id="198" xml:space="preserve">
            <source><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>ExpressRoute[]<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept></source>
            <target state="new"><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>ExpressRoute[]<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept></target>
          </trans-unit>
          <trans-unit id="199" xml:space="preserve">
            <source>You may want to consider using <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>ExpressRoute[]<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept> to speed up the transfer.</source>
            <target state="new">You may want to consider using <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>ExpressRoute[]<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept> to speed up the transfer.</target>
          </trans-unit>
          <trans-unit id="200" xml:space="preserve">
            <source><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>ExpressRoute[]<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept>provides you with an established private connection to Azure so the connection does not go over the public internet.</source>
            <target state="new"><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>ExpressRoute[]<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept>provides you with an established private connection to Azure so the connection does not go over the public internet.</target>
          </trans-unit>
          <trans-unit id="201" xml:space="preserve">
            <source>This is by no means a mandatory step.</source>
            <target state="new">This is by no means a mandatory step.</target>
          </trans-unit>
          <trans-unit id="202" xml:space="preserve">
            <source>However, it will improve throughput when pushing data to Azure from an on-premises or co-location facility.</source>
            <target state="new">However, it will improve throughput when pushing data to Azure from an on-premises or co-location facility.</target>
          </trans-unit>
          <trans-unit id="203" xml:space="preserve">
            <source>The benefits of using <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>ExpressRoute[]<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept> are:</source>
            <target state="new">The benefits of using <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>ExpressRoute[]<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept> are:</target>
          </trans-unit>
          <trans-unit id="204" xml:space="preserve">
            <source>Increased reliability</source>
            <target state="new">Increased reliability</target>
          </trans-unit>
          <trans-unit id="205" xml:space="preserve">
            <source>Faster network speed</source>
            <target state="new">Faster network speed</target>
          </trans-unit>
          <trans-unit id="206" xml:space="preserve">
            <source>Lower network latency</source>
            <target state="new">Lower network latency</target>
          </trans-unit>
          <trans-unit id="207" xml:space="preserve">
            <source>higher network security</source>
            <target state="new">higher network security</target>
          </trans-unit>
          <trans-unit id="208" xml:space="preserve">
            <source><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>ExpressRoute[]<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept> is beneficial for a number of scenarios; not just the migration.</source>
            <target state="new"><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>ExpressRoute[]<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept> is beneficial for a number of scenarios; not just the migration.</target>
          </trans-unit>
          <trans-unit id="209" xml:space="preserve">
            <source>Interested?</source>
            <target state="new">Interested?</target>
          </trans-unit>
          <trans-unit id="210" xml:space="preserve">
            <source>For more information and pricing please visit the <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>ExpressRoute documentation[]<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept>.</source>
            <target state="new">For more information and pricing please visit the <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>ExpressRoute documentation[]<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept>.</target>
          </trans-unit>
          <trans-unit id="211" xml:space="preserve">
            <source>Azure Import and Export Service</source>
            <target state="new">Azure Import and Export Service</target>
          </trans-unit>
          <trans-unit id="212" xml:space="preserve">
            <source>The Azure Import and Export Service is a data transfer process designed for large (GB++) to massive (TB++) transfers of data into Azure.</source>
            <target state="new">The Azure Import and Export Service is a data transfer process designed for large (GB++) to massive (TB++) transfers of data into Azure.</target>
          </trans-unit>
          <trans-unit id="213" xml:space="preserve">
            <source>It involves writing your data to disks and shipping them to an Azure data center.</source>
            <target state="new">It involves writing your data to disks and shipping them to an Azure data center.</target>
          </trans-unit>
          <trans-unit id="214" xml:space="preserve">
            <source>The disk contents will then be loaded into Azure Storage Blobss on your behalf.</source>
            <target state="new">The disk contents will then be loaded into Azure Storage Blobss on your behalf.</target>
          </trans-unit>
          <trans-unit id="215" xml:space="preserve">
            <source>A high level view of the import export process is as follows:</source>
            <target state="new">A high level view of the import export process is as follows:</target>
          </trans-unit>
          <trans-unit id="216" xml:space="preserve">
            <source>Configure an Azure Blob Storage container to receive the data</source>
            <target state="new">Configure an Azure Blob Storage container to receive the data</target>
          </trans-unit>
          <trans-unit id="217" xml:space="preserve">
            <source>Export your data to local storage</source>
            <target state="new">Export your data to local storage</target>
          </trans-unit>
          <trans-unit id="218" xml:space="preserve">
            <source>Copy the data to 3.5 inch SATA II/III hard disk drives using the [Azure Import/Export Tool]</source>
            <target state="new">Copy the data to 3.5 inch SATA II/III hard disk drives using the [Azure Import/Export Tool]</target>
          </trans-unit>
          <trans-unit id="219" xml:space="preserve">
            <source>Create an Import Job using the Azure Import and Export Service providing the journal files produced by the [Azure Import/Export Tool]</source>
            <target state="new">Create an Import Job using the Azure Import and Export Service providing the journal files produced by the [Azure Import/Export Tool]</target>
          </trans-unit>
          <trans-unit id="220" xml:space="preserve">
            <source>Ship the disks your nominated Azure data center</source>
            <target state="new">Ship the disks your nominated Azure data center</target>
          </trans-unit>
          <trans-unit id="221" xml:space="preserve">
            <source>Your data is transferred to your Azure Blob Storage container</source>
            <target state="new">Your data is transferred to your Azure Blob Storage container</target>
          </trans-unit>
          <trans-unit id="222" xml:space="preserve">
            <source>Load the data into SQLDW using PolyBase</source>
            <target state="new">Load the data into SQLDW using PolyBase</target>
          </trans-unit>
          <trans-unit id="223" xml:space="preserve">
            <source><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>AZCopy[]<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept> utility</source>
            <target state="new"><bpt id="1CapsExtId1">&lt;link&gt;</bpt><bpt id="1CapsExtId2">&lt;linkText&gt;</bpt>AZCopy[]<ept id="1CapsExtId2">&lt;/linkText&gt;</ept><bpt id="1CapsExtId3">&lt;title&gt;</bpt><ept id="1CapsExtId3">&lt;/title&gt;</ept><ept id="1CapsExtId1">&lt;/link&gt;</ept> utility</target>
          </trans-unit>
          <trans-unit id="224" xml:space="preserve">
            <source>The <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>AZCopy[]<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept> utility is a great tool for getting your data transferred into Azure Storage Blobs.</source>
            <target state="new">The <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>AZCopy[]<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept> utility is a great tool for getting your data transferred into Azure Storage Blobs.</target>
          </trans-unit>
          <trans-unit id="225" xml:space="preserve">
            <source>It is designed for small (MB++) to very large (GB++) data transfers.</source>
            <target state="new">It is designed for small (MB++) to very large (GB++) data transfers.</target>
          </trans-unit>
          <trans-unit id="226" xml:space="preserve">
            <source>[AZCopy] has also been designed to provide good resilient throughput when transferring data to Azure and so is a great choice for the data transfer step.</source>
            <target state="new">[AZCopy] has also been designed to provide good resilient throughput when transferring data to Azure and so is a great choice for the data transfer step.</target>
          </trans-unit>
          <trans-unit id="227" xml:space="preserve">
            <source>Once transferred you can load the data using PolyBase into SQL Data Warehouse.</source>
            <target state="new">Once transferred you can load the data using PolyBase into SQL Data Warehouse.</target>
          </trans-unit>
          <trans-unit id="228" xml:space="preserve">
            <source>You can also incorporate AZCopy into your SSIS packages using an "Execute Process" task.</source>
            <target state="new">You can also incorporate AZCopy into your SSIS packages using an "Execute Process" task.</target>
          </trans-unit>
          <trans-unit id="229" xml:space="preserve">
            <source>To use AZCopy you will first need to download and install it.</source>
            <target state="new">To use AZCopy you will first need to download and install it.</target>
          </trans-unit>
          <trans-unit id="230" xml:space="preserve">
            <source>There is a <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>production version[]<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept> and a <bpt id="4CapsExtId1">&lt;link&gt;</bpt><bpt id="4CapsExtId2">&lt;linkText&gt;</bpt>preview version[]<ept id="4CapsExtId2">&lt;/linkText&gt;</ept><bpt id="4CapsExtId3">&lt;title&gt;</bpt><ept id="4CapsExtId3">&lt;/title&gt;</ept><ept id="4CapsExtId1">&lt;/link&gt;</ept> available.</source>
            <target state="new">There is a <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>production version[]<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept> and a <bpt id="4CapsExtId1">&lt;link&gt;</bpt><bpt id="4CapsExtId2">&lt;linkText&gt;</bpt>preview version[]<ept id="4CapsExtId2">&lt;/linkText&gt;</ept><bpt id="4CapsExtId3">&lt;title&gt;</bpt><ept id="4CapsExtId3">&lt;/title&gt;</ept><ept id="4CapsExtId1">&lt;/link&gt;</ept> available.</target>
          </trans-unit>
          <trans-unit id="231" xml:space="preserve">
            <source>To upload a file from your file system you will need a command like the one below:</source>
            <target state="new">To upload a file from your file system you will need a command like the one below:</target>
          </trans-unit>
          <trans-unit id="232" xml:space="preserve">
            <source>A high level process summary could be:</source>
            <target state="new">A high level process summary could be:</target>
          </trans-unit>
          <trans-unit id="233" xml:space="preserve">
            <source>Configure an Azure storage blob container to receive the data</source>
            <target state="new">Configure an Azure storage blob container to receive the data</target>
          </trans-unit>
          <trans-unit id="234" xml:space="preserve">
            <source>Export your data to local storage</source>
            <target state="new">Export your data to local storage</target>
          </trans-unit>
          <trans-unit id="235" xml:space="preserve">
            <source>AZCopy your data in the Azure Blob Storage container</source>
            <target state="new">AZCopy your data in the Azure Blob Storage container</target>
          </trans-unit>
          <trans-unit id="236" xml:space="preserve">
            <source>Load the data into SQL Data Warehouse using PolyBase</source>
            <target state="new">Load the data into SQL Data Warehouse using PolyBase</target>
          </trans-unit>
          <trans-unit id="237" xml:space="preserve">
            <source>Full documentation available: <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>AZCopy[]<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept>.</source>
            <target state="new">Full documentation available: <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>AZCopy[]<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept>.</target>
          </trans-unit>
          <trans-unit id="238" xml:space="preserve">
            <source>Optimizing data export</source>
            <target state="new">Optimizing data export</target>
          </trans-unit>
          <trans-unit id="239" xml:space="preserve">
            <source>In addition to ensuring that the export conforms to the requirements laid out by PolyBase you can also seek to optimize the export of the data to improve the process further.</source>
            <target state="new">In addition to ensuring that the export conforms to the requirements laid out by PolyBase you can also seek to optimize the export of the data to improve the process further.</target>
          </trans-unit>
          <trans-unit id="240" xml:space="preserve">
            <source>As PolyBase requires the data to be in UTF-8 format it is unlikely you will use bcp to perform the data export.</source>
            <target state="new">As PolyBase requires the data to be in UTF-8 format it is unlikely you will use bcp to perform the data export.</target>
          </trans-unit>
          <trans-unit id="241" xml:space="preserve">
            <source>bcp does not support outputting data files to  UTF-8.</source>
            <target state="new">bcp does not support outputting data files to  UTF-8.</target>
          </trans-unit>
          <trans-unit id="242" xml:space="preserve">
            <source>SSIS or ADF Copy are much better suited to performing this kind of data export.</source>
            <target state="new">SSIS or ADF Copy are much better suited to performing this kind of data export.</target>
          </trans-unit>
          <trans-unit id="243" xml:space="preserve">
            <source>Data compression</source>
            <target state="new">Data compression</target>
          </trans-unit>
          <trans-unit id="244" xml:space="preserve">
            <source>PolyBase can read gzip compressed data.</source>
            <target state="new">PolyBase can read gzip compressed data.</target>
          </trans-unit>
          <trans-unit id="245" xml:space="preserve">
            <source>If you are able to compress your data to gzip files then you will minimize the amount of data being pushed over the network.</source>
            <target state="new">If you are able to compress your data to gzip files then you will minimize the amount of data being pushed over the network.</target>
          </trans-unit>
          <trans-unit id="246" xml:space="preserve">
            <source>Multiple files</source>
            <target state="new">Multiple files</target>
          </trans-unit>
          <trans-unit id="247" xml:space="preserve">
            <source>Breaking up large tables into several files not only helps to improve export speed, it also helps with transfer re-startability, and the overall manageability of the data once in the Azure blob storage.</source>
            <target state="new">Breaking up large tables into several files not only helps to improve export speed, it also helps with transfer re-startability, and the overall manageability of the data once in the Azure blob storage.</target>
          </trans-unit>
          <trans-unit id="248" xml:space="preserve">
            <source>One of the many nice features of PolyBase is that it will read all the files inside a folder and treat it as one table.</source>
            <target state="new">One of the many nice features of PolyBase is that it will read all the files inside a folder and treat it as one table.</target>
          </trans-unit>
          <trans-unit id="249" xml:space="preserve">
            <source>It is therefore a good idea to isolate the files for each table into its own folder.</source>
            <target state="new">It is therefore a good idea to isolate the files for each table into its own folder.</target>
          </trans-unit>
          <trans-unit id="250" xml:space="preserve">
            <source>PolyBase also supports a feature known as "recursive folder traversal".</source>
            <target state="new">PolyBase also supports a feature known as "recursive folder traversal".</target>
          </trans-unit>
          <trans-unit id="251" xml:space="preserve">
            <source>You can use this feature to further enhance the organization of your exported data to improve your data management.</source>
            <target state="new">You can use this feature to further enhance the organization of your exported data to improve your data management.</target>
          </trans-unit>
          <trans-unit id="252" xml:space="preserve">
            <source>To learn more about loading data with PolyBase, see <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>Use PolyBase to load data into SQL Data Warehouse[]<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept>.</source>
            <target state="new">To learn more about loading data with PolyBase, see <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>Use PolyBase to load data into SQL Data Warehouse[]<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept>.</target>
          </trans-unit>
          <trans-unit id="253" xml:space="preserve">
            <source>Next steps</source>
            <target state="new">Next steps</target>
          </trans-unit>
          <trans-unit id="254" xml:space="preserve">
            <source>For more about migration, see <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>Migrate your solution to SQL Data Warehouse[]<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept>.</source>
            <target state="new">For more about migration, see <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>Migrate your solution to SQL Data Warehouse[]<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept>.</target>
          </trans-unit>
          <trans-unit id="255" xml:space="preserve">
            <source>For more development tips, see <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>development overview[]<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept>.</source>
            <target state="new">For more development tips, see <bpt id="2CapsExtId1">&lt;link&gt;</bpt><bpt id="2CapsExtId2">&lt;linkText&gt;</bpt>development overview[]<ept id="2CapsExtId2">&lt;/linkText&gt;</ept><bpt id="2CapsExtId3">&lt;title&gt;</bpt><ept id="2CapsExtId3">&lt;/title&gt;</ept><ept id="2CapsExtId1">&lt;/link&gt;</ept>.</target>
          </trans-unit>
        </group>
      </group>
    </body>
  </file>
</xliff>